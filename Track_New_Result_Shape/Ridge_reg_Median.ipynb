{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122fb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phate\n",
    "import scprep\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from scipy.stats import stats\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from skimage.morphology import dilation, erosion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from skimage import measure\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D Plotting\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from skimage.morphology import dilation, erosion\n",
    "from skimage import measure\n",
    "from scipy.ndimage import center_of_mass\n",
    "from glob import glob\n",
    "import random\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from tifffile import imread\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d71c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "130 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "92 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "38 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [        nan         nan -0.05271933         nan -0.03905195         nan\n",
      "         nan -0.05362407 -0.03333291 -0.04322811 -0.04085406 -0.05271933\n",
      "         nan         nan -0.03905195 -0.0664473  -0.03333291         nan\n",
      "         nan         nan -0.04866403 -0.05226614 -0.03071592 -0.03270265\n",
      " -0.02119995 -0.03509986 -0.0476655  -0.03001526 -0.02983955 -0.04329441\n",
      " -0.01886564 -0.05198149 -0.05155039 -0.03966763 -0.06259848 -0.04246036\n",
      " -0.03325552 -0.02977846 -0.04096161 -0.05182733 -0.04393922 -0.03464484\n",
      " -0.06176984 -0.04237161 -0.03001526 -0.06343518 -0.04226612 -0.03411719\n",
      " -0.02966003         nan -0.03071592         nan         nan -0.03001526\n",
      " -0.02870393 -0.0393907  -0.04394981 -0.05226614         nan         nan\n",
      " -0.04096161         nan -0.02977846 -0.02678024 -0.03411719 -0.03071592\n",
      " -0.03509986         nan         nan -0.04246036 -0.03411719 -0.03407298\n",
      "         nan         nan         nan -0.0447278          nan -0.02536663\n",
      " -0.04405578 -0.05037512 -0.03338227         nan -0.03528981 -0.052282\n",
      "         nan -0.03333291 -0.04592774 -0.03235426 -0.04329441         nan\n",
      " -0.04486496 -0.02966003         nan -0.04469102 -0.04469102 -0.03569722\n",
      " -0.04399974 -0.04548506 -0.0393907  -0.03622307]\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2941417/3565055210.py:130: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metrics_df_all.sort_values(\"R2_Test\", ascending=False), x=\"R2_Test\", y=\"Model\", palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuned Model: XGBoost_Tuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2941417/3565055210.py:156: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=feat_df, x=\"Importance\", y=\"Feature\", palette=\"crest\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R¬≤ Score: 0.0591\n",
      "Test Mean Squared Error: 29.1183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median\"\n",
    "plot_dir = f\"{output_root}/Reg_Results\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "df_clean = df[collinear_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA (Train/Test Only) ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_collinear, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # === SCALING ONLY (NO PCA) ===\n",
    "# scaler = StandardScaler()\n",
    "# X_train_final = scaler.fit_transform(X_train)\n",
    "# X_test_final = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "X_train_final = pca.fit_transform(X_train_scaled)\n",
    "X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# === TUNE MODELS USING RANDOMIZED SEARCH ===\n",
    "tuned_models = {}\n",
    "param_logs = []\n",
    "\n",
    "ridge_param_grid = {\"alpha\": np.logspace(-3, 3, 100)}\n",
    "ridge_search = RandomizedSearchCV(Ridge(), param_distributions=ridge_param_grid, n_iter=100, cv=5, scoring='r2', random_state=42)\n",
    "ridge_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Ridge_Tuned\"] = ridge_search.best_estimator_\n",
    "param_logs.append((\"Ridge_Tuned\", ridge_search.best_params_))\n",
    "\n",
    "lasso_param_grid = {\"alpha\": np.logspace(-3, 3, 100), \"max_iter\": [10000]}\n",
    "lasso_search = RandomizedSearchCV(Lasso(random_state=42), param_distributions=lasso_param_grid, n_iter=100, cv=5, scoring='r2', random_state=42)\n",
    "lasso_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Lasso_Tuned\"] = lasso_search.best_estimator_\n",
    "param_logs.append((\"Lasso_Tuned\", lasso_search.best_params_))\n",
    "\n",
    "enet_param_grid = {\n",
    "    \"alpha\": np.logspace(-3, 3, 100),\n",
    "    \"l1_ratio\": np.linspace(0.1, 0.9, 20),\n",
    "    \"max_iter\": [10000]\n",
    "}\n",
    "enet_search = RandomizedSearchCV(ElasticNet(random_state=42), param_distributions=enet_param_grid, n_iter=30, cv=3, scoring='r2', random_state=42)\n",
    "enet_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"ElasticNet_Tuned\"] = enet_search.best_estimator_\n",
    "param_logs.append((\"ElasticNet_Tuned\", enet_search.best_params_))\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 10, 15],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\", None]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "                               param_distributions=rf_param_grid,\n",
    "                               n_iter=100, cv=5, scoring='r2', random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"RandomForest_Tuned\"] = rf_search.best_estimator_\n",
    "param_logs.append((\"RandomForest_Tuned\", rf_search.best_params_))\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 6, 10],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"subsample\": [0.7, 0.9, 0.5],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(XGBRegressor(random_state=42, verbosity=0, n_jobs=-1),\n",
    "                                param_distributions=xgb_param_grid,\n",
    "                                n_iter=100, cv=5, scoring='r2', random_state=42, n_jobs=-1)\n",
    "xgb_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"XGBoost_Tuned\"] = xgb_search.best_estimator_\n",
    "param_logs.append((\"XGBoost_Tuned\", xgb_search.best_params_))\n",
    "\n",
    "tuned_models[\"LinearRegression_Tuned\"] = LinearRegression()\n",
    "param_logs.append((\"LinearRegression_Tuned\", \"No tuning required\"))\n",
    "\n",
    "# === LOG BEST PARAMS ===\n",
    "param_df = pd.DataFrame(param_logs, columns=[\"Model\", \"Best_Params\"])\n",
    "param_df.to_csv(f\"{plot_dir}/best_model_hyperparameters.csv\", index=False)\n",
    "\n",
    "# === EVALUATE TUNED MODELS ON TEST ===\n",
    "results = {}\n",
    "all_metrics = []\n",
    "for name, model in tuned_models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_test_pred = model.predict(X_test_final)\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "        mse = mean_squared_error(y_test, y_test_pred)\n",
    "        results[name] = r2\n",
    "        all_metrics.append({\"Model\": name, \"R2_Test\": r2, \"MSE_Test\": mse})\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "metrics_df_all = pd.DataFrame(all_metrics)\n",
    "metrics_df_all.to_csv(f\"{plot_dir}/all_model_test_metrics.csv\", index=False)\n",
    "\n",
    "# === COMPARISON PLOT ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=metrics_df_all.sort_values(\"R2_Test\", ascending=False), x=\"R2_Test\", y=\"Model\", palette=\"viridis\")\n",
    "plt.title(\"Test R¬≤ Score by Model\")\n",
    "plt.xlabel(\"R¬≤ Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/model_comparison_r2_test.png\")\n",
    "plt.close()\n",
    "\n",
    "# === SELECT AND RETRAIN BEST MODEL FOR TEST SET ===\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = tuned_models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "print(f\"Best Tuned Model: {best_model_name}\")\n",
    "\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = best_model.feature_importances_\n",
    "    feat_labels = [f\"PC{i+1}\" for i in range(len(importances))]\n",
    "    feat_df = pd.DataFrame({\"Feature\": feat_labels, \"Importance\": importances})\n",
    "    feat_df = feat_df.sort_values(by=\"Importance\", ascending=False)\n",
    "    feat_df.to_csv(f\"{plot_dir}/best_model_feature_importances.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feat_df, x=\"Importance\", y=\"Feature\", palette=\"crest\")\n",
    "    plt.title(\"Feature Importances - Best Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_dir}/best_model_feature_importance_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === PRINT FINAL TEST METRICS ===\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"True\": y_test,\n",
    "    \"Predicted\": y_test_pred\n",
    "})\n",
    "pred_df.to_csv(f\"{plot_dir}/best_model_predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712b629",
   "metadata": {},
   "source": [
    "# Final Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfdec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: XGBoost\n",
      "        Split  R-squared        MSE\n",
      "0       Train   0.213656  34.094028\n",
      "1  Validation   0.046176  33.371527\n",
      "2        Test   0.021721  33.211349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2941417/414109741.py:261: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median\"\n",
    "plot_dir = f\"{output_root}/Ridge_Plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA'] #, #'X_Centroid_Distance_MA', #'Y_Centroid_Distance_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "\n",
    "df_clean = df[collinear_features + spatial_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "X_spatial = df_clean[spatial_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA ===\n",
    "X_col_train, X_col_temp, X_spatial_train, X_spatial_temp, y_train, y_temp = train_test_split(\n",
    "    X_collinear, X_spatial, y, test_size=0.4, random_state=42)\n",
    "X_col_val, X_col_test, X_spatial_val, X_spatial_test, y_val, y_test = train_test_split(\n",
    "    X_col_temp, X_spatial_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_col_train_scaled = scaler.fit_transform(X_col_train)\n",
    "X_col_val_scaled = scaler.transform(X_col_val)\n",
    "X_col_test_scaled = scaler.transform(X_col_test)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca_train = pca.fit_transform(X_col_train_scaled)\n",
    "X_pca_val = pca.transform(X_col_val_scaled)\n",
    "X_pca_test = pca.transform(X_col_test_scaled)\n",
    "\n",
    "X_train_final = np.hstack([X_pca_train, X_spatial_train])\n",
    "X_val_final = np.hstack([X_pca_val, X_spatial_val])\n",
    "X_test_final = np.hstack([X_pca_test, X_spatial_test])\n",
    "\n",
    "# === DEFINE MODELS ===\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5),\n",
    "    \"Lasso\": LassoCV(cv=5, random_state=42, max_iter=10000),\n",
    "    \"ElasticNet\": ElasticNetCV(cv=5, random_state=42, max_iter=10000),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.01, random_state=42, n_jobs=-1, verbosity=0)\n",
    "}\n",
    "\n",
    "# === EVALUATE MODELS ON VALIDATION SET ===\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_val_pred = model.predict(X_val_final)\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        results[name] = r2\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# === SAVE COEFFICIENTS AND PERFORMANCE ===\n",
    "coeff_labels = [f'PC{i+1}' for i in range(3)] + spatial_features\n",
    "coef_df = pd.DataFrame({'Feature': coeff_labels, 'Coefficient': best_model.coef_}) if hasattr(best_model, 'coef_') else pd.DataFrame()\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['R-squared', 'MSE'],\n",
    "    'Value': [r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred)]\n",
    "})\n",
    "coef_df.to_csv(f\"{plot_dir}/best_model_coefficients.csv\", index=False)\n",
    "metrics_df.to_csv(f\"{plot_dir}/best_model_metrics.csv\", index=False)\n",
    "\n",
    "# === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === REPORT ON SPLITS ===\n",
    "y_train_pred = best_model.predict(X_train_final)\n",
    "y_val_pred = best_model.predict(X_val_final)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"R-squared\": [\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_val, y_val_pred),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ],\n",
    "    \"MSE\": [\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_val, y_val_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)\n",
    "    ]\n",
    "})\n",
    "\n",
    "performance.to_csv(f\"{plot_dir}/model_performance_by_split.csv\", index=False)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(performance)\n",
    "\n",
    "\n",
    "# Generate the code block for PCA + Clustering using validation data and applying to test data\n",
    "\n",
    "\n",
    "# === PCA + KMeans CLUSTERING (Fit on Validation, Apply to Test) ===\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "output_directory = plot_dir\n",
    "window_size = 5\n",
    "\n",
    "# --- Fit PCA on Validation Set ---\n",
    "X_scaled_val = X_col_val_scaled\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled_val)\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# --- Plot PCA Cumulative Variance (Elbow Plot) ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='s', linestyle='-', color='red', label=\"Cumulative Variance\")\n",
    "plt.axvline(x=optimal_components, color='green', linestyle='--', label=f\"Optimal Components ({optimal_components})\")\n",
    "plt.scatter(optimal_components, cumulative_variance[optimal_components-1], color='black', s=50, label=\"Chosen Point\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Elbow Plot with Cumulative Variance\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_Elbow_Plot_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Fit 2D PCA on Validation ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_val_2d = pca_2d.fit_transform(X_scaled_val)\n",
    "\n",
    "# --- KMeans on 2D PCA space (Validation) ---\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 15)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "    score = silhouette_score(pca_val_2d, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "optimal_score = max(silhouette_scores)\n",
    "\n",
    "# --- Plot Silhouette Scores ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='-', color='purple', label=\"Silhouette Score\")\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', label=f\"Optimal k = {optimal_k}\")\n",
    "plt.scatter(optimal_k, optimal_score, color='red', s=100, zorder=5)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs Number of Clusters (Validation)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Silhouette_Score_vs_K_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Apply to Test Data ---\n",
    "X_scaled_test = X_col_test_scaled\n",
    "pca_test_2d = pca_2d.transform(X_scaled_test)\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "test_clusters = kmeans_final.predict(pca_test_2d)\n",
    "\n",
    "# --- Create Test DataFrame with PCA + Clusters ---\n",
    "pca_test_df = pd.DataFrame(pca_test_2d, columns=[\"PCA1\", \"PCA2\"])\n",
    "pca_test_df[\"X_Centroid_Velocity_MA\"] = X_col_test[:, 0]  # Area_MA is first collinear feature\n",
    "pca_test_df[\"Cluster\"] = test_clusters\n",
    "\n",
    "# --- Clustered 2D PCA (Colored by Area) ---\n",
    "vmin = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 5)\n",
    "vmax = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 95)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sc = plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"X_Centroid_Velocity_MA\"], cmap=\"plasma\", alpha=0.8, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(sc, label=\"X_Centroid Velocity (MA)\")\n",
    "plt.title(\"2D PCA Test Data Colored by X_Centroid Velocity(MA)\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Cluster Plot with Centers ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"Cluster\"], cmap=\"tab10\", alpha=0.8)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, marker='o', label='Centers')\n",
    "for idx, (x, y) in enumerate(centers):\n",
    "    plt.text(x, y, str(idx), fontsize=12, fontweight='bold', ha='center', va='center', color='white',\n",
    "             bbox=dict(facecolor='black', boxstyle='circle,pad=0.2'))\n",
    "plt.title(f\"Test Data PCA with KMeans Clusters (k={optimal_k})\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_Clusters_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Boxplot: Area by Cluster (Test Data) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n",
    "sns.stripplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", color='gray', alpha=0.5, jitter=0.2, size=3)\n",
    "plt.title(\"X_Centroid Velocity (MA) by Cluster (Test Data)\", fontsize=14)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"X_Centroid Velocity (MA)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Boxplot_X_Centroid_Velocity_MA_By_Cluster_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Save clustered PCA test data and summary ---\n",
    "pca_test_df.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Results_Test_{window_size}.csv\"), index=False)\n",
    "\n",
    "summary = pca_test_df.groupby(\"Cluster\").agg({\n",
    "    \"PCA1\": [\"mean\", \"std\"],\n",
    "    \"PCA2\": [\"mean\", \"std\"],\n",
    "    \"X_Centroid_Velocity_MA\": [\"mean\", \"std\"]\n",
    "}).reset_index()\n",
    "summary.columns = [\"_\".join(col).strip(\"_\") for col in summary.columns.values]\n",
    "summary.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Summary_Test_{window_size}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae6cb0",
   "metadata": {},
   "source": [
    "# Create the updated FNN pipeline script content with PCA + spatial features + grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d207188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=45.9729\n",
      "Epoch 20: Loss=45.6892\n",
      "Epoch 30: Loss=44.0364\n",
      "Epoch 40: Loss=43.8994\n",
      "Epoch 50: Loss=43.9254\n",
      "Epoch 60: Loss=43.6998\n",
      "Epoch 70: Loss=44.0124\n",
      "Epoch 80: Loss=43.7315\n",
      "Epoch 90: Loss=43.3511\n",
      "Epoch 100: Loss=43.3002\n",
      "‚úÖ MSE: 34.4711 | R¬≤: -0.0154\n",
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=45.6286\n",
      "Epoch 20: Loss=44.6924\n",
      "Epoch 30: Loss=44.8121\n",
      "Epoch 40: Loss=46.1905\n",
      "Epoch 50: Loss=44.2942\n",
      "Epoch 60: Loss=43.3662\n",
      "Epoch 70: Loss=43.2290\n",
      "Epoch 80: Loss=43.2994\n",
      "Epoch 90: Loss=43.0904\n",
      "Epoch 100: Loss=43.2748\n",
      "Epoch 110: Loss=43.6389\n",
      "Epoch 120: Loss=43.0306\n",
      "Epoch 130: Loss=43.5091\n",
      "Epoch 140: Loss=43.6350\n",
      "Epoch 150: Loss=42.9947\n",
      "Epoch 160: Loss=43.2403\n",
      "Epoch 170: Loss=42.8865\n",
      "Epoch 180: Loss=43.1496\n",
      "Epoch 190: Loss=43.5016\n",
      "Epoch 200: Loss=43.0925\n",
      "‚úÖ MSE: 34.1326 | R¬≤: -0.0054\n",
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=45.6187\n",
      "Epoch 20: Loss=46.1215\n",
      "Epoch 30: Loss=43.8297\n",
      "Epoch 40: Loss=44.4499\n",
      "Epoch 50: Loss=43.8783\n",
      "Epoch 60: Loss=45.7036\n",
      "Epoch 70: Loss=43.5433\n",
      "Epoch 80: Loss=43.8813\n",
      "Epoch 90: Loss=45.0536\n",
      "Epoch 100: Loss=44.7627\n",
      "‚úÖ MSE: 36.2233 | R¬≤: -0.0670\n",
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=46.5508\n",
      "Epoch 20: Loss=43.7955\n",
      "Epoch 30: Loss=43.8532\n",
      "Epoch 40: Loss=43.3174\n",
      "Epoch 50: Loss=43.1217\n",
      "Epoch 60: Loss=43.3210\n",
      "Epoch 70: Loss=43.2566\n",
      "Epoch 80: Loss=43.2014\n",
      "Epoch 90: Loss=43.4065\n",
      "Epoch 100: Loss=43.0813\n",
      "Epoch 110: Loss=43.1672\n",
      "Epoch 120: Loss=43.0850\n",
      "Epoch 130: Loss=42.9952\n",
      "Epoch 140: Loss=43.2082\n",
      "Epoch 150: Loss=43.2401\n",
      "Epoch 160: Loss=43.0509\n",
      "Epoch 170: Loss=43.0206\n",
      "Epoch 180: Loss=43.0317\n",
      "Epoch 190: Loss=43.3329\n",
      "Epoch 200: Loss=43.0402\n",
      "‚úÖ MSE: 34.4257 | R¬≤: -0.0140\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=51.8586\n",
      "Epoch 20: Loss=44.1731\n",
      "Epoch 30: Loss=46.1982\n",
      "Epoch 40: Loss=47.6981\n",
      "Epoch 50: Loss=45.9763\n",
      "Epoch 60: Loss=47.5713\n",
      "Epoch 70: Loss=43.3622\n",
      "Epoch 80: Loss=43.2345\n",
      "Epoch 90: Loss=43.1807\n",
      "Epoch 100: Loss=43.0105\n",
      "‚úÖ MSE: 33.7994 | R¬≤: 0.0044\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=45.7166\n",
      "Epoch 20: Loss=44.3545\n",
      "Epoch 30: Loss=45.2156\n",
      "Epoch 40: Loss=48.4584\n",
      "Epoch 50: Loss=44.9608\n",
      "Epoch 60: Loss=44.1297\n",
      "Epoch 70: Loss=43.6005\n",
      "Epoch 80: Loss=47.4771\n",
      "Epoch 90: Loss=44.8432\n",
      "Epoch 100: Loss=43.3924\n",
      "Epoch 110: Loss=44.1190\n",
      "Epoch 120: Loss=44.4842\n",
      "Epoch 130: Loss=44.3240\n",
      "Epoch 140: Loss=42.9194\n",
      "Epoch 150: Loss=43.2401\n",
      "Epoch 160: Loss=43.2123\n",
      "Epoch 170: Loss=43.2581\n",
      "Epoch 180: Loss=43.3013\n",
      "Epoch 190: Loss=42.9880\n",
      "Epoch 200: Loss=42.7455\n",
      "‚úÖ MSE: 33.9361 | R¬≤: 0.0004\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=43.9807\n",
      "Epoch 20: Loss=43.8071\n",
      "Epoch 30: Loss=43.4207\n",
      "Epoch 40: Loss=43.3466\n",
      "Epoch 50: Loss=44.4237\n",
      "Epoch 60: Loss=44.9567\n",
      "Epoch 70: Loss=43.7597\n",
      "Epoch 80: Loss=42.9208\n",
      "Epoch 90: Loss=43.7226\n",
      "Epoch 100: Loss=43.0001\n",
      "‚úÖ MSE: 34.1462 | R¬≤: -0.0058\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=44.1386\n",
      "Epoch 20: Loss=43.6501\n",
      "Epoch 30: Loss=43.3552\n",
      "Epoch 40: Loss=43.7961\n",
      "Epoch 50: Loss=42.9773\n",
      "Epoch 60: Loss=42.7534\n",
      "Epoch 70: Loss=43.2680\n",
      "Epoch 80: Loss=43.0861\n",
      "Epoch 90: Loss=43.0190\n",
      "Epoch 100: Loss=42.9421\n",
      "Epoch 110: Loss=42.9789\n",
      "Epoch 120: Loss=43.1571\n",
      "Epoch 130: Loss=42.8642\n",
      "Epoch 140: Loss=42.8617\n",
      "Epoch 150: Loss=42.9685\n",
      "Epoch 160: Loss=42.9160\n",
      "Epoch 170: Loss=42.8987\n",
      "Epoch 180: Loss=43.0551\n",
      "Epoch 190: Loss=42.9863\n",
      "Epoch 200: Loss=42.7876\n",
      "‚úÖ MSE: 33.9284 | R¬≤: 0.0006\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=44.8505\n",
      "Epoch 20: Loss=46.7343\n",
      "Epoch 30: Loss=44.9552\n",
      "Epoch 40: Loss=44.8709\n",
      "Epoch 50: Loss=44.2888\n",
      "Epoch 60: Loss=45.2342\n",
      "Epoch 70: Loss=45.5337\n",
      "Epoch 80: Loss=43.2293\n",
      "Epoch 90: Loss=43.6447\n",
      "Epoch 100: Loss=44.5631\n",
      "‚úÖ MSE: 36.2868 | R¬≤: -0.0689\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=46.4337\n",
      "Epoch 20: Loss=47.9306\n",
      "Epoch 30: Loss=46.0407\n",
      "Epoch 40: Loss=46.0089\n",
      "Epoch 50: Loss=44.9405\n",
      "Epoch 60: Loss=43.7409\n",
      "Epoch 70: Loss=43.4432\n",
      "Epoch 80: Loss=44.2830\n",
      "Epoch 90: Loss=43.6395\n",
      "Epoch 100: Loss=44.0465\n",
      "Epoch 110: Loss=44.0669\n",
      "Epoch 120: Loss=43.9362\n",
      "Epoch 130: Loss=43.3121\n",
      "Epoch 140: Loss=43.2747\n",
      "Epoch 150: Loss=43.0500\n",
      "Epoch 160: Loss=43.1868\n",
      "Epoch 170: Loss=43.4139\n",
      "Epoch 180: Loss=43.3007\n",
      "Epoch 190: Loss=43.1111\n",
      "Epoch 200: Loss=43.5451\n",
      "‚úÖ MSE: 34.7462 | R¬≤: -0.0235\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=45.6493\n",
      "Epoch 20: Loss=46.3677\n",
      "Epoch 30: Loss=43.7552\n",
      "Epoch 40: Loss=44.6531\n",
      "Epoch 50: Loss=46.6366\n",
      "Epoch 60: Loss=44.4001\n",
      "Epoch 70: Loss=44.0314\n",
      "Epoch 80: Loss=46.5309\n",
      "Epoch 90: Loss=44.1993\n",
      "Epoch 100: Loss=44.1761\n",
      "‚úÖ MSE: 35.1681 | R¬≤: -0.0359\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=44.4039\n",
      "Epoch 20: Loss=44.7303\n",
      "Epoch 30: Loss=44.2336\n",
      "Epoch 40: Loss=43.8749\n",
      "Epoch 50: Loss=45.8082\n",
      "Epoch 60: Loss=45.3680\n",
      "Epoch 70: Loss=46.8214\n",
      "Epoch 80: Loss=43.9975\n",
      "Epoch 90: Loss=44.3477\n",
      "Epoch 100: Loss=43.4159\n",
      "Epoch 110: Loss=43.4550\n",
      "Epoch 120: Loss=43.2680\n",
      "Epoch 130: Loss=43.2214\n",
      "Epoch 140: Loss=43.0399\n",
      "Epoch 150: Loss=43.3643\n",
      "Epoch 160: Loss=42.9570\n",
      "Epoch 170: Loss=43.0007\n",
      "Epoch 180: Loss=43.2162\n",
      "Epoch 190: Loss=43.0684\n",
      "Epoch 200: Loss=43.0019\n",
      "‚úÖ MSE: 34.3559 | R¬≤: -0.0120\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=45.3218\n",
      "Epoch 20: Loss=49.9164\n",
      "Epoch 30: Loss=49.8476\n",
      "Epoch 40: Loss=45.6765\n",
      "Epoch 50: Loss=43.8623\n",
      "Epoch 60: Loss=44.0201\n",
      "Epoch 70: Loss=44.2774\n",
      "Epoch 80: Loss=45.3059\n",
      "Epoch 90: Loss=43.5174\n",
      "Epoch 100: Loss=43.6393\n",
      "‚úÖ MSE: 34.3753 | R¬≤: -0.0126\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=46.8796\n",
      "Epoch 20: Loss=45.9936\n",
      "Epoch 30: Loss=45.9123\n",
      "Epoch 40: Loss=43.3734\n",
      "Epoch 50: Loss=43.9228\n",
      "Epoch 60: Loss=50.2138\n",
      "Epoch 70: Loss=49.0087\n",
      "Epoch 80: Loss=45.7096\n",
      "Epoch 90: Loss=43.3379\n",
      "Epoch 100: Loss=44.2629\n",
      "Epoch 110: Loss=43.1701\n",
      "Epoch 120: Loss=45.0113\n",
      "Epoch 130: Loss=43.8190\n",
      "Epoch 140: Loss=43.3309\n",
      "Epoch 150: Loss=43.5335\n",
      "Epoch 160: Loss=44.0003\n",
      "Epoch 170: Loss=43.3657\n",
      "Epoch 180: Loss=43.6820\n",
      "Epoch 190: Loss=43.2793\n",
      "Epoch 200: Loss=43.3167\n",
      "‚úÖ MSE: 34.4119 | R¬≤: -0.0136\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=44.7670\n",
      "Epoch 20: Loss=43.9467\n",
      "Epoch 30: Loss=45.7111\n",
      "Epoch 40: Loss=43.1025\n",
      "Epoch 50: Loss=43.8279\n",
      "Epoch 60: Loss=44.0546\n",
      "Epoch 70: Loss=44.1661\n",
      "Epoch 80: Loss=43.2971\n",
      "Epoch 90: Loss=43.7545\n",
      "Epoch 100: Loss=45.9253\n",
      "‚úÖ MSE: 34.7696 | R¬≤: -0.0242\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=44.8517\n",
      "Epoch 20: Loss=43.6964\n",
      "Epoch 30: Loss=43.6640\n",
      "Epoch 40: Loss=44.2508\n",
      "Epoch 50: Loss=44.0611\n",
      "Epoch 60: Loss=43.8294\n",
      "Epoch 70: Loss=44.0770\n",
      "Epoch 80: Loss=43.1311\n",
      "Epoch 90: Loss=43.5280\n",
      "Epoch 100: Loss=44.6712\n",
      "Epoch 110: Loss=43.7315\n",
      "Epoch 120: Loss=44.8780\n",
      "Epoch 130: Loss=43.2087\n",
      "Epoch 140: Loss=43.6349\n",
      "Epoch 150: Loss=43.2883\n",
      "Epoch 160: Loss=43.7948\n",
      "Epoch 170: Loss=43.4309\n",
      "Epoch 180: Loss=43.0930\n",
      "Epoch 190: Loss=47.3610\n",
      "Epoch 200: Loss=43.7841\n",
      "‚úÖ MSE: 34.5990 | R¬≤: -0.0192\n",
      "üèÜ Best config: {'lr': 0.001, 'batch_size': 32, 'hidden_sizes': (128, 64, 32), 'epochs': 100}, R¬≤=0.0044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIG ---\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Reg_Analysis_FNN_PCA\"\n",
    "os.makedirs(f\"{output_root}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/plots\", exist_ok=True)\n",
    "\n",
    "# --- FNN Model ---\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"‚úÖ MSE: {mse:.4f} | R¬≤: {r2:.4f}\")\n",
    "    return mse, r2, y_true, y_pred\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_results(y_true, y_pred, train_losses=None):\n",
    "    residuals = y_true - y_pred\n",
    "    if train_losses:\n",
    "        plt.figure(); plt.plot(train_losses); plt.title(\"Training Loss\"); plt.tight_layout()\n",
    "        plt.savefig(f\"{output_root}/plots/loss_curve.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_pred, y=residuals); plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(\"Residuals vs Predicted\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_vs_predicted.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=y_pred); plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(\"Predicted vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/predicted_vs_actual.png\"); plt.close()\n",
    "    plt.figure(); sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Residuals\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_distribution.png\"); plt.close()\n",
    "    plt.figure(); plt.plot(np.sort(np.abs(residuals)), np.linspace(0, 1, len(residuals)))\n",
    "    plt.title(\"Cumulative Absolute Errors\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/cumulative_absolute_errors.png\"); plt.close()\n",
    "    plt.figure(); stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/qq_plot_residuals.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=np.abs(residuals))\n",
    "    plt.title(\"Absolute Error vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/absolute_error_vs_actual.png\"); plt.close()\n",
    "\n",
    "# --- Run FNN ---\n",
    "def run_fnn(X_train, X_val, X_test, y_train, y_val, y_test, input_size, hidden_sizes, lr, batch_size, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FNN(input_size=input_size, hidden_sizes=hidden_sizes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
    "    X_val_tensor = torch.tensor(X_val).float().to(device)\n",
    "    y_val_tensor = torch.tensor(y_val).float().to(device)\n",
    "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, X_test_tensor, y_test_tensor\n",
    "\n",
    "# --- Grid Search ---\n",
    "def grid_search_fnn(csv_path, param_grid):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    shape_features = ['Area_MA', 'Perimeter_MA', 'Extent_MA', 'Solidity_MA', 'Compactness_MA', 'Elongation_MA', 'Circularity_MA', 'Convexity_MA']\n",
    "    # spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "    target = 'X_Centroid_Velocity_MA'\n",
    "    df = df.dropna(subset=shape_features + spatial_features + [target])\n",
    "\n",
    "    X_shape = df[shape_features].values\n",
    "    X_spatial = df[spatial_features].values\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    X_shape_scaled = StandardScaler().fit_transform(X_shape)\n",
    "    X_train_s, X_temp_s, X_train_spatial, X_temp_spatial, y_train, y_temp = train_test_split(\n",
    "        X_shape_scaled, X_spatial, y, test_size=0.4, random_state=42)\n",
    "    X_val_s, X_test_s, X_val_spatial, X_test_spatial, y_val, y_test = train_test_split(\n",
    "        X_temp_s, X_temp_spatial, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_s)\n",
    "    X_val_pca = pca.transform(X_val_s)\n",
    "    X_test_pca = pca.transform(X_test_s)\n",
    "\n",
    "    # Combine PCA + spatial\n",
    "    X_train_final = np.hstack([X_train_pca, X_train_spatial])\n",
    "    X_val_final = np.hstack([X_val_pca, X_val_spatial])\n",
    "    X_test_final = np.hstack([X_test_pca, X_test_spatial])\n",
    "\n",
    "    results = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_losses = None\n",
    "    best_test_tensors = None\n",
    "\n",
    "    for lr, batch_size, hidden_sizes, epochs in product(param_grid[\"lr\"], param_grid[\"batch_size\"],\n",
    "                                                        param_grid[\"hidden_sizes\"], param_grid[\"epochs\"]):\n",
    "        print(f\"‚ñ∂ Training: lr={lr}, batch={batch_size}, hidden={hidden_sizes}, epochs={epochs}\")\n",
    "        model, train_losses, X_test_tensor, y_test_tensor = run_fnn(\n",
    "            X_train_final, X_val_final, X_test_final,\n",
    "            y_train, y_val, y_test,\n",
    "            input_size=X_train_final.shape[1],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "        results.append({\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"hidden_sizes\": str(hidden_sizes),\n",
    "            \"epochs\": epochs, \"mse\": mse, \"r2\": r2\n",
    "        })\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_config = {\n",
    "                \"lr\": lr, \"batch_size\": batch_size,\n",
    "                \"hidden_sizes\": hidden_sizes, \"epochs\": epochs\n",
    "            }\n",
    "            best_losses = train_losses\n",
    "            best_test_tensors = (X_test_tensor, y_test_tensor, y_true, y_pred)\n",
    "\n",
    "    # Save best model\n",
    "    model_path = f\"{output_root}/models/fnn_best_model.pt\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(f\"{output_root}/results/grid_search_results.csv\", index=False)\n",
    "    print(f\"üèÜ Best config: {best_config}, R¬≤={best_r2:.4f}\")\n",
    "    return best_model, best_config, best_losses, best_test_tensors\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        \"lr\": [0.001, 0.0005],\n",
    "        \"batch_size\": [16, 32],\n",
    "        \"hidden_sizes\": [(128, 64, 32), (64, 32, 16)],\n",
    "        \"epochs\": [100, 200]\n",
    "    }\n",
    "    csv_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "    best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = grid_search_fnn(csv_path, param_grid)\n",
    "    plot_results(y_true, y_pred, train_losses=best_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5eb0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=42.4394\n",
      "Epoch 20: Loss=41.6210\n",
      "Epoch 30: Loss=41.6410\n",
      "Epoch 40: Loss=40.8219\n",
      "Epoch 50: Loss=39.6963\n",
      "Epoch 60: Loss=38.7996\n",
      "Epoch 70: Loss=38.6891\n",
      "Epoch 80: Loss=37.3509\n",
      "Epoch 90: Loss=36.4081\n",
      "Epoch 100: Loss=36.0461\n",
      "‚úÖ MSE: 33.9742 | R¬≤: -0.0007\n",
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=42.5917\n",
      "Epoch 20: Loss=41.9256\n",
      "Epoch 30: Loss=41.4579\n",
      "Epoch 40: Loss=40.7395\n",
      "Epoch 50: Loss=40.2562\n",
      "Epoch 60: Loss=40.0617\n",
      "Epoch 70: Loss=39.4408\n",
      "Epoch 80: Loss=38.9423\n",
      "Epoch 90: Loss=38.3509\n",
      "Epoch 100: Loss=38.1911\n",
      "Epoch 110: Loss=36.8088\n",
      "Epoch 120: Loss=36.6766\n",
      "Epoch 130: Loss=35.7722\n",
      "Epoch 140: Loss=34.8743\n",
      "Epoch 150: Loss=34.6152\n",
      "Epoch 160: Loss=34.5277\n",
      "Epoch 170: Loss=34.3201\n",
      "Epoch 180: Loss=32.9198\n",
      "Epoch 190: Loss=32.5978\n",
      "Epoch 200: Loss=32.4688\n",
      "‚úÖ MSE: 34.4698 | R¬≤: -0.0153\n",
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=42.5790\n",
      "Epoch 20: Loss=42.3638\n",
      "Epoch 30: Loss=41.8741\n",
      "Epoch 40: Loss=41.1392\n",
      "Epoch 50: Loss=41.1129\n",
      "Epoch 60: Loss=40.8186\n",
      "Epoch 70: Loss=40.0965\n",
      "Epoch 80: Loss=39.9494\n",
      "Epoch 90: Loss=39.8819\n",
      "Epoch 100: Loss=39.4899\n",
      "‚úÖ MSE: 33.1968 | R¬≤: 0.0221\n",
      "‚ñ∂ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=42.9020\n",
      "Epoch 20: Loss=42.2069\n",
      "Epoch 30: Loss=41.5769\n",
      "Epoch 40: Loss=41.3955\n",
      "Epoch 50: Loss=41.0039\n",
      "Epoch 60: Loss=40.5029\n",
      "Epoch 70: Loss=39.9231\n",
      "Epoch 80: Loss=39.6038\n",
      "Epoch 90: Loss=39.4361\n",
      "Epoch 100: Loss=38.7832\n",
      "Epoch 110: Loss=38.3196\n",
      "Epoch 120: Loss=38.0273\n",
      "Epoch 130: Loss=38.1130\n",
      "Epoch 140: Loss=37.2264\n",
      "Epoch 150: Loss=36.8519\n",
      "Epoch 160: Loss=36.6128\n",
      "Epoch 170: Loss=35.9113\n",
      "Epoch 180: Loss=35.4968\n",
      "Epoch 190: Loss=35.1278\n",
      "Epoch 200: Loss=34.7515\n",
      "‚úÖ MSE: 32.5047 | R¬≤: 0.0425\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=42.4840\n",
      "Epoch 20: Loss=42.0720\n",
      "Epoch 30: Loss=41.4667\n",
      "Epoch 40: Loss=40.9076\n",
      "Epoch 50: Loss=40.5034\n",
      "Epoch 60: Loss=40.0539\n",
      "Epoch 70: Loss=39.4525\n",
      "Epoch 80: Loss=38.9433\n",
      "Epoch 90: Loss=38.3169\n",
      "Epoch 100: Loss=38.0322\n",
      "‚úÖ MSE: 33.3007 | R¬≤: 0.0191\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=42.7112\n",
      "Epoch 20: Loss=42.3981\n",
      "Epoch 30: Loss=41.8665\n",
      "Epoch 40: Loss=41.2735\n",
      "Epoch 50: Loss=40.6598\n",
      "Epoch 60: Loss=40.3764\n",
      "Epoch 70: Loss=39.6893\n",
      "Epoch 80: Loss=38.6512\n",
      "Epoch 90: Loss=38.0109\n",
      "Epoch 100: Loss=38.1751\n",
      "Epoch 110: Loss=37.0612\n",
      "Epoch 120: Loss=36.2293\n",
      "Epoch 130: Loss=35.9471\n",
      "Epoch 140: Loss=35.1337\n",
      "Epoch 150: Loss=34.7112\n",
      "Epoch 160: Loss=33.8958\n",
      "Epoch 170: Loss=34.3544\n",
      "Epoch 180: Loss=32.7693\n",
      "Epoch 190: Loss=32.2679\n",
      "Epoch 200: Loss=31.9817\n",
      "‚úÖ MSE: 35.6426 | R¬≤: -0.0499\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=42.8045\n",
      "Epoch 20: Loss=42.4723\n",
      "Epoch 30: Loss=42.1363\n",
      "Epoch 40: Loss=41.7744\n",
      "Epoch 50: Loss=41.4973\n",
      "Epoch 60: Loss=41.2029\n",
      "Epoch 70: Loss=40.8061\n",
      "Epoch 80: Loss=40.5698\n",
      "Epoch 90: Loss=40.3899\n",
      "Epoch 100: Loss=39.8345\n",
      "‚úÖ MSE: 33.8728 | R¬≤: 0.0022\n",
      "‚ñ∂ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=42.6213\n",
      "Epoch 20: Loss=42.3717\n",
      "Epoch 30: Loss=42.0344\n",
      "Epoch 40: Loss=41.5346\n",
      "Epoch 50: Loss=41.2191\n",
      "Epoch 60: Loss=40.8801\n",
      "Epoch 70: Loss=40.4527\n",
      "Epoch 80: Loss=40.2651\n",
      "Epoch 90: Loss=40.0279\n",
      "Epoch 100: Loss=39.7971\n",
      "Epoch 110: Loss=39.4296\n",
      "Epoch 120: Loss=39.2110\n",
      "Epoch 130: Loss=39.0463\n",
      "Epoch 140: Loss=38.5464\n",
      "Epoch 150: Loss=38.4895\n",
      "Epoch 160: Loss=37.9651\n",
      "Epoch 170: Loss=37.9276\n",
      "Epoch 180: Loss=37.3336\n",
      "Epoch 190: Loss=37.6304\n",
      "Epoch 200: Loss=36.9029\n",
      "‚úÖ MSE: 33.3297 | R¬≤: 0.0182\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=42.7863\n",
      "Epoch 20: Loss=42.3622\n",
      "Epoch 30: Loss=41.8989\n",
      "Epoch 40: Loss=41.6599\n",
      "Epoch 50: Loss=40.9185\n",
      "Epoch 60: Loss=40.4778\n",
      "Epoch 70: Loss=39.8058\n",
      "Epoch 80: Loss=39.5500\n",
      "Epoch 90: Loss=39.0638\n",
      "Epoch 100: Loss=38.5681\n",
      "‚úÖ MSE: 33.7754 | R¬≤: 0.0051\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=42.5532\n",
      "Epoch 20: Loss=42.2728\n",
      "Epoch 30: Loss=41.6719\n",
      "Epoch 40: Loss=41.1553\n",
      "Epoch 50: Loss=40.8427\n",
      "Epoch 60: Loss=40.3397\n",
      "Epoch 70: Loss=40.0542\n",
      "Epoch 80: Loss=39.4086\n",
      "Epoch 90: Loss=39.0609\n",
      "Epoch 100: Loss=38.8555\n",
      "Epoch 110: Loss=38.3941\n",
      "Epoch 120: Loss=38.4060\n",
      "Epoch 130: Loss=37.5580\n",
      "Epoch 140: Loss=37.5485\n",
      "Epoch 150: Loss=36.8812\n",
      "Epoch 160: Loss=37.3471\n",
      "Epoch 170: Loss=36.2796\n",
      "Epoch 180: Loss=35.9768\n",
      "Epoch 190: Loss=35.6961\n",
      "Epoch 200: Loss=35.2183\n",
      "‚úÖ MSE: 32.4407 | R¬≤: 0.0444\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=42.8741\n",
      "Epoch 20: Loss=42.4932\n",
      "Epoch 30: Loss=42.1452\n",
      "Epoch 40: Loss=41.7652\n",
      "Epoch 50: Loss=41.4654\n",
      "Epoch 60: Loss=41.2422\n",
      "Epoch 70: Loss=40.8970\n",
      "Epoch 80: Loss=40.7358\n",
      "Epoch 90: Loss=40.6540\n",
      "Epoch 100: Loss=40.3869\n",
      "‚úÖ MSE: 33.5609 | R¬≤: 0.0114\n",
      "‚ñ∂ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=42.9279\n",
      "Epoch 20: Loss=42.5772\n",
      "Epoch 30: Loss=42.3054\n",
      "Epoch 40: Loss=42.0584\n",
      "Epoch 50: Loss=41.8221\n",
      "Epoch 60: Loss=41.6144\n",
      "Epoch 70: Loss=41.2260\n",
      "Epoch 80: Loss=40.9361\n",
      "Epoch 90: Loss=40.8032\n",
      "Epoch 100: Loss=40.4363\n",
      "Epoch 110: Loss=40.3930\n",
      "Epoch 120: Loss=40.2349\n",
      "Epoch 130: Loss=39.9092\n",
      "Epoch 140: Loss=39.7295\n",
      "Epoch 150: Loss=39.5310\n",
      "Epoch 160: Loss=39.5072\n",
      "Epoch 170: Loss=39.2904\n",
      "Epoch 180: Loss=39.2252\n",
      "Epoch 190: Loss=38.9527\n",
      "Epoch 200: Loss=39.0298\n",
      "‚úÖ MSE: 33.7200 | R¬≤: 0.0067\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=42.7695\n",
      "Epoch 20: Loss=42.4860\n",
      "Epoch 30: Loss=42.0725\n",
      "Epoch 40: Loss=41.7016\n",
      "Epoch 50: Loss=41.3044\n",
      "Epoch 60: Loss=40.9399\n",
      "Epoch 70: Loss=40.6758\n",
      "Epoch 80: Loss=40.2985\n",
      "Epoch 90: Loss=40.1649\n",
      "Epoch 100: Loss=39.9021\n",
      "‚úÖ MSE: 33.5821 | R¬≤: 0.0108\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=42.7921\n",
      "Epoch 20: Loss=42.4230\n",
      "Epoch 30: Loss=42.1034\n",
      "Epoch 40: Loss=41.7567\n",
      "Epoch 50: Loss=41.4474\n",
      "Epoch 60: Loss=41.1290\n",
      "Epoch 70: Loss=40.7576\n",
      "Epoch 80: Loss=40.3640\n",
      "Epoch 90: Loss=40.0874\n",
      "Epoch 100: Loss=39.7193\n",
      "Epoch 110: Loss=39.2073\n",
      "Epoch 120: Loss=38.9233\n",
      "Epoch 130: Loss=38.3552\n",
      "Epoch 140: Loss=38.0866\n",
      "Epoch 150: Loss=37.8840\n",
      "Epoch 160: Loss=37.5581\n",
      "Epoch 170: Loss=37.0764\n",
      "Epoch 180: Loss=36.6154\n",
      "Epoch 190: Loss=36.4631\n",
      "Epoch 200: Loss=35.9010\n",
      "‚úÖ MSE: 33.3273 | R¬≤: 0.0183\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=43.5476\n",
      "Epoch 20: Loss=42.7869\n",
      "Epoch 30: Loss=42.5257\n",
      "Epoch 40: Loss=42.2983\n",
      "Epoch 50: Loss=42.0415\n",
      "Epoch 60: Loss=41.7626\n",
      "Epoch 70: Loss=41.5171\n",
      "Epoch 80: Loss=41.2108\n",
      "Epoch 90: Loss=40.9571\n",
      "Epoch 100: Loss=40.7766\n",
      "‚úÖ MSE: 33.6687 | R¬≤: 0.0083\n",
      "‚ñ∂ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=42.9550\n",
      "Epoch 20: Loss=42.6219\n",
      "Epoch 30: Loss=42.4024\n",
      "Epoch 40: Loss=42.1738\n",
      "Epoch 50: Loss=41.9778\n",
      "Epoch 60: Loss=41.7721\n",
      "Epoch 70: Loss=41.5334\n",
      "Epoch 80: Loss=41.2924\n",
      "Epoch 90: Loss=41.1172\n",
      "Epoch 100: Loss=40.8904\n",
      "Epoch 110: Loss=40.6415\n",
      "Epoch 120: Loss=40.4100\n",
      "Epoch 130: Loss=40.2621\n",
      "Epoch 140: Loss=40.0499\n",
      "Epoch 150: Loss=39.9525\n",
      "Epoch 160: Loss=39.7295\n",
      "Epoch 170: Loss=39.6983\n",
      "Epoch 180: Loss=39.5195\n",
      "Epoch 190: Loss=39.4571\n",
      "Epoch 200: Loss=39.2390\n",
      "‚úÖ MSE: 33.1679 | R¬≤: 0.0230\n",
      "üèÜ Best config: {'lr': 0.0005, 'batch_size': 16, 'hidden_sizes': (128, 64, 32), 'epochs': 200}, R¬≤=0.0444\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIG ---\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Reg_Analysis_FNN_PCA\"\n",
    "os.makedirs(f\"{output_root}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/plots\", exist_ok=True)\n",
    "\n",
    "# --- FNN Model ---\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\u2705 MSE: {mse:.4f} | R¬≤: {r2:.4f}\")\n",
    "    return mse, r2, y_true, y_pred\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_results(y_true, y_pred, train_losses=None):\n",
    "    residuals = y_true - y_pred\n",
    "    if train_losses:\n",
    "        plt.figure(); plt.plot(train_losses); plt.title(\"Training Loss\"); plt.tight_layout()\n",
    "        plt.savefig(f\"{output_root}/plots/loss_curve.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_pred, y=residuals); plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(\"Residuals vs Predicted\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_vs_predicted.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=y_pred); plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(\"Predicted vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/predicted_vs_actual.png\"); plt.close()\n",
    "    plt.figure(); sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Residuals\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_distribution.png\"); plt.close()\n",
    "    plt.figure(); plt.plot(np.sort(np.abs(residuals)), np.linspace(0, 1, len(residuals)))\n",
    "    plt.title(\"Cumulative Absolute Errors\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/cumulative_absolute_errors.png\"); plt.close()\n",
    "    plt.figure(); stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/qq_plot_residuals.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=np.abs(residuals))\n",
    "    plt.title(\"Absolute Error vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/absolute_error_vs_actual.png\"); plt.close()\n",
    "\n",
    "# --- Run FNN ---\n",
    "def run_fnn(X_train, X_val, X_test, y_train, y_val, y_test, input_size, hidden_sizes, lr, batch_size, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FNN(input_size=input_size, hidden_sizes=hidden_sizes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
    "    X_val_tensor = torch.tensor(X_val).float().to(device)\n",
    "    y_val_tensor = torch.tensor(y_val).float().to(device)\n",
    "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, X_test_tensor, y_test_tensor\n",
    "\n",
    "# --- Grid Search ---\n",
    "def grid_search_fnn(csv_path, param_grid):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    shape_features = ['Area_MA', 'Perimeter_MA', 'Extent_MA', 'Solidity_MA', 'Compactness_MA', 'Elongation_MA', 'Circularity_MA', 'Convexity_MA']\n",
    "    target = 'X_Centroid_Velocity_MA'\n",
    "    df = df.dropna(subset=shape_features + [target])\n",
    "\n",
    "    X_shape = df[shape_features].values\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    X_shape_scaled = StandardScaler().fit_transform(X_shape)\n",
    "    X_train_s, X_temp_s, y_train, y_temp = train_test_split(X_shape_scaled, y, test_size=0.4, random_state=42)\n",
    "    X_val_s, X_test_s, y_val, y_test = train_test_split(X_temp_s, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_s)\n",
    "    X_val_pca = pca.transform(X_val_s)\n",
    "    X_test_pca = pca.transform(X_test_s)\n",
    "\n",
    "    X_train_final = X_train_pca\n",
    "    X_val_final = X_val_pca\n",
    "    X_test_final = X_test_pca\n",
    "\n",
    "    results = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_losses = None\n",
    "    best_test_tensors = None\n",
    "\n",
    "    for lr, batch_size, hidden_sizes, epochs in product(param_grid[\"lr\"], param_grid[\"batch_size\"],\n",
    "                                                        param_grid[\"hidden_sizes\"], param_grid[\"epochs\"]):\n",
    "        print(f\"‚ñ∂ Training: lr={lr}, batch={batch_size}, hidden={hidden_sizes}, epochs={epochs}\")\n",
    "        model, train_losses, X_test_tensor, y_test_tensor = run_fnn(\n",
    "            X_train_final, X_val_final, X_test_final,\n",
    "            y_train, y_val, y_test,\n",
    "            input_size=X_train_final.shape[1],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "        results.append({\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"hidden_sizes\": str(hidden_sizes),\n",
    "            \"epochs\": epochs, \"mse\": mse, \"r2\": r2\n",
    "        })\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_config = {\n",
    "                \"lr\": lr, \"batch_size\": batch_size,\n",
    "                \"hidden_sizes\": hidden_sizes, \"epochs\": epochs\n",
    "            }\n",
    "            best_losses = train_losses\n",
    "            best_test_tensors = (X_test_tensor, y_test_tensor, y_true, y_pred)\n",
    "\n",
    "    # Save best model\n",
    "    model_path = f\"{output_root}/models/fnn_best_model.pt\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(f\"{output_root}/results/grid_search_results.csv\", index=False)\n",
    "    print(f\"üèÜ Best config: {best_config}, R¬≤={best_r2:.4f}\")\n",
    "    return best_model, best_config, best_losses, best_test_tensors\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        \"lr\": [0.001, 0.0005],\n",
    "        \"batch_size\": [16, 32],\n",
    "        \"hidden_sizes\": [(128, 64, 32), (64, 32, 16)],\n",
    "        \"epochs\": [100, 200]\n",
    "    }\n",
    "    csv_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "    best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = grid_search_fnn(csv_path, param_grid)\n",
    "    plot_results(y_true, y_pred, train_losses=best_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shp_chc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
