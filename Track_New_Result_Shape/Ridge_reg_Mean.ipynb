{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122fb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phate\n",
    "import scprep\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from scipy.stats import stats\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from skimage.morphology import dilation, erosion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from skimage import measure\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D Plotting\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from skimage.morphology import dilation, erosion\n",
    "from skimage import measure\n",
    "from scipy.ndimage import center_of_mass\n",
    "from glob import glob\n",
    "import random\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from tifffile import imread\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c254e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "150 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "95 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "55 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [        nan         nan -0.02110293 -0.02696683 -0.01334337         nan\n",
      " -0.01319745 -0.03313118         nan -0.03544353 -0.01997771 -0.01319745\n",
      " -0.0338839          nan         nan -0.01635734 -0.01319745         nan\n",
      " -0.02698728 -0.01717417 -0.02650412         nan -0.04081447 -0.02892868\n",
      "         nan         nan         nan -0.01202919         nan -0.01635734\n",
      " -0.04842356 -0.04484863         nan -0.02650412         nan -0.02506858\n",
      "         nan -0.03350361 -0.03278321 -0.02651106 -0.01997771 -0.04542712\n",
      " -0.03627857         nan -0.04347344 -0.02001658         nan -0.02006646\n",
      "         nan -0.03286915 -0.02651106 -0.01635734 -0.02506858         nan\n",
      " -0.02037908         nan         nan         nan -0.03352141 -0.03358151\n",
      " -0.01717417         nan -0.0398639  -0.04212318 -0.01992329 -0.0327061\n",
      " -0.02360814 -0.02506858 -0.03735048         nan -0.03571254 -0.04484863\n",
      " -0.01202919 -0.01937834 -0.03932244 -0.0398639  -0.01334337         nan\n",
      " -0.01937834 -0.02423621 -0.03932244 -0.03558349 -0.02006646         nan\n",
      " -0.02269031 -0.04542712 -0.02389058         nan -0.02042756 -0.02037908\n",
      " -0.02360814         nan -0.02110293         nan -0.01997771         nan\n",
      " -0.03627857 -0.03232683 -0.01202919 -0.01992329]\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1786382/597750344.py:133: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metrics_df_all.sort_values(\"R2_Test\", ascending=False), x=\"R2_Test\", y=\"Model\", palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuned Model: RandomForest_Tuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1786382/597750344.py:159: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=feat_df, x=\"Importance\", y=\"Feature\", palette=\"crest\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R² Score: 0.0117\n",
      "Test Mean Squared Error: 57.0700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# input_path = \"/home/MinaHossain/EmbedTrack/Track_EMBD_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "# output_root = \"/home/MinaHossain/EmbedTrack/Track_EMBD_Result_Shape/Median\"\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean\"\n",
    "\n",
    "plot_dir = f\"{output_root}/Reg_Results\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "df_clean = df[collinear_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA (Train/Test Only) ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_collinear, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # === SCALING ONLY (NO PCA) ===\n",
    "# scaler = StandardScaler()\n",
    "# X_train_final = scaler.fit_transform(X_train)\n",
    "# X_test_final = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_train_final = pca.fit_transform(X_train_scaled)\n",
    "X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# === TUNE MODELS USING RANDOMIZED SEARCH ===\n",
    "tuned_models = {}\n",
    "param_logs = []\n",
    "\n",
    "ridge_param_grid = {\"alpha\": np.logspace(-3, 3, 100)}\n",
    "ridge_search = RandomizedSearchCV(Ridge(), param_distributions=ridge_param_grid, n_iter=100, cv=5, scoring='r2', random_state=42)\n",
    "ridge_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Ridge_Tuned\"] = ridge_search.best_estimator_\n",
    "param_logs.append((\"Ridge_Tuned\", ridge_search.best_params_))\n",
    "\n",
    "lasso_param_grid = {\"alpha\": np.logspace(-3, 3, 100), \"max_iter\": [10000]}\n",
    "lasso_search = RandomizedSearchCV(Lasso(random_state=42), param_distributions=lasso_param_grid, n_iter=100, cv=5, scoring='r2', random_state=42)\n",
    "lasso_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Lasso_Tuned\"] = lasso_search.best_estimator_\n",
    "param_logs.append((\"Lasso_Tuned\", lasso_search.best_params_))\n",
    "\n",
    "enet_param_grid = {\n",
    "    \"alpha\": np.logspace(-3, 3, 100),\n",
    "    \"l1_ratio\": np.linspace(0.1, 0.9, 20),\n",
    "    \"max_iter\": [10000]\n",
    "}\n",
    "enet_search = RandomizedSearchCV(ElasticNet(random_state=42), param_distributions=enet_param_grid, n_iter=30, cv=3, scoring='r2', random_state=42)\n",
    "enet_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"ElasticNet_Tuned\"] = enet_search.best_estimator_\n",
    "param_logs.append((\"ElasticNet_Tuned\", enet_search.best_params_))\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 10, 15],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "                               param_distributions=rf_param_grid,\n",
    "                               n_iter=100, cv=5, scoring='r2', random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"RandomForest_Tuned\"] = rf_search.best_estimator_\n",
    "param_logs.append((\"RandomForest_Tuned\", rf_search.best_params_))\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 6, 10],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"subsample\": [0.7, 0.9, 0.5],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(XGBRegressor(random_state=42, verbosity=0, n_jobs=-1),\n",
    "                                param_distributions=xgb_param_grid,\n",
    "                                n_iter=100, cv=5, scoring='r2', random_state=42, n_jobs=-1)\n",
    "xgb_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"XGBoost_Tuned\"] = xgb_search.best_estimator_\n",
    "param_logs.append((\"XGBoost_Tuned\", xgb_search.best_params_))\n",
    "\n",
    "tuned_models[\"LinearRegression_Tuned\"] = LinearRegression()\n",
    "param_logs.append((\"LinearRegression_Tuned\", \"No tuning required\"))\n",
    "\n",
    "# === LOG BEST PARAMS ===\n",
    "param_df = pd.DataFrame(param_logs, columns=[\"Model\", \"Best_Params\"])\n",
    "param_df.to_csv(f\"{plot_dir}/best_model_hyperparameters.csv\", index=False)\n",
    "\n",
    "# === EVALUATE TUNED MODELS ON TEST ===\n",
    "results = {}\n",
    "all_metrics = []\n",
    "for name, model in tuned_models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_test_pred = model.predict(X_test_final)\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "        mse = mean_squared_error(y_test, y_test_pred)\n",
    "        results[name] = r2\n",
    "        all_metrics.append({\"Model\": name, \"R2_Test\": r2, \"MSE_Test\": mse})\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "metrics_df_all = pd.DataFrame(all_metrics)\n",
    "metrics_df_all.to_csv(f\"{plot_dir}/all_model_test_metrics.csv\", index=False)\n",
    "\n",
    "# === COMPARISON PLOT ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=metrics_df_all.sort_values(\"R2_Test\", ascending=False), x=\"R2_Test\", y=\"Model\", palette=\"viridis\")\n",
    "plt.title(\"Test R² Score by Model\")\n",
    "plt.xlabel(\"R² Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/model_comparison_r2_test.png\")\n",
    "plt.close()\n",
    "\n",
    "# === SELECT AND RETRAIN BEST MODEL FOR TEST SET ===\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = tuned_models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "print(f\"Best Tuned Model: {best_model_name}\")\n",
    "\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = best_model.feature_importances_\n",
    "    feat_labels = [f\"PC{i+1}\" for i in range(len(importances))]\n",
    "    feat_df = pd.DataFrame({\"Feature\": feat_labels, \"Importance\": importances})\n",
    "    feat_df = feat_df.sort_values(by=\"Importance\", ascending=False)\n",
    "    feat_df.to_csv(f\"{plot_dir}/best_model_feature_importances.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feat_df, x=\"Importance\", y=\"Feature\", palette=\"crest\")\n",
    "    plt.title(\"Feature Importances - Best Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_dir}/best_model_feature_importance_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === PRINT FINAL TEST METRICS ===\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test R² Score: {r2:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"True\": y_test,\n",
    "    \"Predicted\": y_test_pred\n",
    "})\n",
    "pred_df.to_csv(f\"{plot_dir}/best_model_predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8545910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # === CONFIGURATION ===\n",
    "# # === CONFIGURATION ===\n",
    "# input_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "# # output_root = \"./model_outputs\"\n",
    "# output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape\"\n",
    "# os.makedirs(f\"{output_root}/Ridge_plots\", exist_ok=True)\n",
    "\n",
    "# # === LOAD AND CLEAN DATA ===\n",
    "# df = pd.read_csv(input_path)\n",
    "# collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "#                       'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "# spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "# target_col = 'X_Centroid_Velocity_MA'\n",
    "\n",
    "# relevant_columns = collinear_features + spatial_features + [target_col]\n",
    "# df_clean = df[relevant_columns].dropna()\n",
    "\n",
    "# X_collinear = df_clean[collinear_features].values\n",
    "# X_spatial = df_clean[spatial_features].values\n",
    "# y = df_clean[target_col].values\n",
    "\n",
    "# # === SPLIT DATA ===\n",
    "# X_col_train, X_col_temp, X_spatial_train, X_spatial_temp, y_train, y_temp = train_test_split(\n",
    "#     X_collinear, X_spatial, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# X_col_val, X_col_test, X_spatial_val, X_spatial_test, y_val, y_test = train_test_split(\n",
    "#     X_col_temp, X_spatial_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# # === PCA ON TRAIN ===\n",
    "# scaler = StandardScaler()\n",
    "# X_col_train_scaled = scaler.fit_transform(X_col_train)\n",
    "# X_col_val_scaled = scaler.transform(X_col_val)\n",
    "# X_col_test_scaled = scaler.transform(X_col_test)\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# X_pca_train = pca.fit_transform(X_col_train_scaled)\n",
    "# X_pca_val = pca.transform(X_col_val_scaled)\n",
    "# X_pca_test = pca.transform(X_col_test_scaled)\n",
    "\n",
    "# # === FINAL COMBINED DATASETS ===\n",
    "# X_train_final = np.hstack([X_pca_train, X_spatial_train])\n",
    "# X_val_final = np.hstack([X_pca_val, X_spatial_val])\n",
    "# X_test_final = np.hstack([X_pca_test, X_spatial_test])\n",
    "\n",
    "# # === DEFINE MODELS ===\n",
    "# models = {\n",
    "#     \"Ridge\": RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5),\n",
    "#     \"Lasso\": LassoCV(cv=5, random_state=42, max_iter=10000),\n",
    "#     \"ElasticNet\": ElasticNetCV(cv=5, random_state=42, max_iter=10000),\n",
    "#     \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n",
    "#     \"XGBoost\": XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42, n_jobs=-1, verbosity=0)\n",
    "# }\n",
    "\n",
    "# # === EVALUATE MODELS ON VALIDATION SET ===\n",
    "# results = {}\n",
    "# for name, model in models.items():\n",
    "#     try:\n",
    "#         model.fit(X_train_final, y_train)\n",
    "#         y_val_pred = model.predict(X_val_final)\n",
    "#         r2 = r2_score(y_val, y_val_pred)\n",
    "#         results[name] = r2\n",
    "#     except Exception as e:\n",
    "#         results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "# # === TRAIN BEST MODEL AND EVALUATE ON TEST ===\n",
    "# best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "# best_model = models[best_model_name]\n",
    "# best_model.fit(X_train_final, y_train)\n",
    "# y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# # === SAVE COEFFICIENTS AND PERFORMANCE ===\n",
    "# coeff_labels = [f'PC{i+1}' for i in range(3)] + spatial_features\n",
    "# coef_df = pd.DataFrame({'Feature': coeff_labels, 'Coefficient': best_model.coef_}) if hasattr(best_model, 'coef_') else pd.DataFrame()\n",
    "# metrics_df = pd.DataFrame({\n",
    "#     'Metric': ['R-squared', 'MSE'],\n",
    "#     'Value': [r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred)]\n",
    "# })\n",
    "# coef_df.to_csv(f\"{output_root}/Ridge_plots/best_model_coefficients.csv\", index=False)\n",
    "# metrics_df.to_csv(f\"{output_root}/Ridge_plots/best_model_metrics.csv\", index=False)\n",
    "\n",
    "# # === PLOTTING ===\n",
    "# def plot_results(y_true, y_pred):\n",
    "#     residuals = y_true - y_pred\n",
    "#     absolute_errors = np.abs(residuals)\n",
    "\n",
    "#     def save_plot(fig, name):\n",
    "#         fig.tight_layout()\n",
    "#         fig.savefig(f\"{output_root}/Ridge_plots/{name}.png\")\n",
    "#         plt.close(fig)\n",
    "\n",
    "#     # 1. Residuals vs Predicted\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "#     ax.axhline(0, linestyle='--', color='red')\n",
    "#     ax.set_title(\"Residuals vs Predicted\")\n",
    "#     ax.set_xlabel(\"Predicted\")\n",
    "#     ax.set_ylabel(\"Residuals\")\n",
    "#     save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "#     # 2. Predicted vs Actual\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "#     ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "#     ax.set_title(\"Predicted vs Actual\")\n",
    "#     ax.set_xlabel(\"Actual\")\n",
    "#     ax.set_ylabel(\"Predicted\")\n",
    "#     save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "#     # 3. Residual Histogram\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "#     ax.set_title(\"Distribution of Residuals\")\n",
    "#     ax.set_xlabel(\"Residuals\")\n",
    "#     save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "#     # 4. Cumulative Absolute Errors\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sorted_errors = np.sort(absolute_errors)\n",
    "#     cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "#     ax.plot(sorted_errors, cum_dist)\n",
    "#     ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "#     ax.set_xlabel(\"Absolute Error\")\n",
    "#     ax.set_ylabel(\"Cumulative Proportion\")\n",
    "#     save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "#     # 5. Q-Q Plot\n",
    "#     fig = plt.figure(figsize=(6, 6))\n",
    "#     stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "#     plt.title(\"Q-Q Plot of Residuals\")\n",
    "#     save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "#     # 6. Absolute Error vs Actual\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "#     ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "#     ax.set_xlabel(\"Actual Values\")\n",
    "#     ax.set_ylabel(\"Absolute Error\")\n",
    "#     save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "# plot_results(y_test, y_test_pred)\n",
    "# print(\"Best model:\", best_model_name)\n",
    "# print(\"Test R²:\", r2_score(y_test, y_test_pred))\n",
    "\n",
    "# # Predict\n",
    "# y_train_pred = best_model.predict(X_train_final)\n",
    "# y_val_pred = best_model.predict(X_val_final)\n",
    "# y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# # Evaluate\n",
    "# performance = pd.DataFrame({\n",
    "#     \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "#     \"R-squared\": [\n",
    "#         r2_score(y_train, y_train_pred),\n",
    "#         r2_score(y_val, y_val_pred),\n",
    "#         r2_score(y_test, y_test_pred)\n",
    "#     ],\n",
    "#     \"MSE\": [\n",
    "#         mean_squared_error(y_train, y_train_pred),\n",
    "#         mean_squared_error(y_val, y_val_pred),\n",
    "#         mean_squared_error(y_test, y_test_pred)\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# print(\"\\\\n=== Model Performance on Each Split ===\")\n",
    "# print(performance.to_string(index=False))\n",
    "# performance.to_csv(f\"{output_root}/Ridge_plots/model_performance_by_split.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712b629",
   "metadata": {},
   "source": [
    "# Final Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92dc75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # === CONFIGURATION ===\n",
    "# input_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "# output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape\"\n",
    "# plot_dir = f\"{output_root}/Ridge_plots\"\n",
    "# os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# # === LOAD AND CLEAN DATA ===\n",
    "# df = pd.read_csv(input_path)\n",
    "# collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "#                       'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "# spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "# target_col = 'X_Centroid_Velocity_MA'\n",
    "\n",
    "# df_clean = df[collinear_features + spatial_features + [target_col]].dropna()\n",
    "# X_collinear = df_clean[collinear_features].values\n",
    "# X_spatial = df_clean[spatial_features].values\n",
    "# y = df_clean[target_col].values\n",
    "\n",
    "# # === SPLIT DATA ===\n",
    "# X_col_train, X_col_temp, X_spatial_train, X_spatial_temp, y_train, y_temp = train_test_split(\n",
    "#     X_collinear, X_spatial, y, test_size=0.4, random_state=42)\n",
    "# X_col_val, X_col_test, X_spatial_val, X_spatial_test, y_val, y_test = train_test_split(\n",
    "#     X_col_temp, X_spatial_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# # === PCA ON TRAIN ===\n",
    "# scaler = StandardScaler()\n",
    "# X_col_train_scaled = scaler.fit_transform(X_col_train)\n",
    "# X_col_val_scaled = scaler.transform(X_col_val)\n",
    "# X_col_test_scaled = scaler.transform(X_col_test)\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "# X_pca_train = pca.fit_transform(X_col_train_scaled)\n",
    "# X_pca_val = pca.transform(X_col_val_scaled)\n",
    "# X_pca_test = pca.transform(X_col_test_scaled)\n",
    "\n",
    "# X_train_final = np.hstack([X_pca_train, X_spatial_train])\n",
    "# X_val_final = np.hstack([X_pca_val, X_spatial_val])\n",
    "# X_test_final = np.hstack([X_pca_test, X_spatial_test])\n",
    "\n",
    "# # === DEFINE MODELS ===\n",
    "# models = {\n",
    "#     \"LinearRegression\": LinearRegression(),\n",
    "#     \"Ridge\": RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5),\n",
    "#     \"Lasso\": LassoCV(cv=5, random_state=42, max_iter=10000),\n",
    "#     \"ElasticNet\": ElasticNetCV(cv=5, random_state=42, max_iter=10000),\n",
    "#     \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n",
    "#     \"XGBoost\": XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42, n_jobs=-1, verbosity=0)\n",
    "# }\n",
    "\n",
    "# # === EVALUATE MODELS ON VALIDATION SET ===\n",
    "# results = {}\n",
    "# for name, model in models.items():\n",
    "#     try:\n",
    "#         model.fit(X_train_final, y_train)\n",
    "#         y_val_pred = model.predict(X_val_final)\n",
    "#         r2 = r2_score(y_val, y_val_pred)\n",
    "#         results[name] = r2\n",
    "#     except Exception as e:\n",
    "#         results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "# best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "# best_model = models[best_model_name]\n",
    "# best_model.fit(X_train_final, y_train)\n",
    "# y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# # === SAVE COEFFICIENTS AND PERFORMANCE ===\n",
    "# coeff_labels = [f'PC{i+1}' for i in range(3)] + spatial_features\n",
    "# coef_df = pd.DataFrame({'Feature': coeff_labels, 'Coefficient': best_model.coef_}) if hasattr(best_model, 'coef_') else pd.DataFrame()\n",
    "# metrics_df = pd.DataFrame({\n",
    "#     'Metric': ['R-squared', 'MSE'],\n",
    "#     'Value': [r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred)]\n",
    "# })\n",
    "# coef_df.to_csv(f\"{plot_dir}/best_model_coefficients.csv\", index=False)\n",
    "# metrics_df.to_csv(f\"{plot_dir}/best_model_metrics.csv\", index=False)\n",
    "\n",
    "# # === PLOTTING ===\n",
    "# def plot_results(y_true, y_pred):\n",
    "#     residuals = y_true - y_pred\n",
    "#     absolute_errors = np.abs(residuals)\n",
    "\n",
    "#     def save_plot(fig, name):\n",
    "#         fig.tight_layout()\n",
    "#         fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "#         plt.close(fig)\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "#     ax.axhline(0, linestyle='--', color='red')\n",
    "#     ax.set_title(\"Residuals vs Predicted\")\n",
    "#     ax.set_xlabel(\"Predicted\")\n",
    "#     ax.set_ylabel(\"Residuals\")\n",
    "#     save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "#     ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "#     ax.set_title(\"Predicted vs Actual\")\n",
    "#     ax.set_xlabel(\"Actual\")\n",
    "#     ax.set_ylabel(\"Predicted\")\n",
    "#     save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "#     ax.set_title(\"Distribution of Residuals\")\n",
    "#     ax.set_xlabel(\"Residuals\")\n",
    "#     save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sorted_errors = np.sort(absolute_errors)\n",
    "#     cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "#     ax.plot(sorted_errors, cum_dist)\n",
    "#     ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "#     ax.set_xlabel(\"Absolute Error\")\n",
    "#     ax.set_ylabel(\"Cumulative Proportion\")\n",
    "#     save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "#     fig = plt.figure(figsize=(6, 6))\n",
    "#     stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "#     plt.title(\"Q-Q Plot of Residuals\")\n",
    "#     save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "#     ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "#     ax.set_xlabel(\"Actual Values\")\n",
    "#     ax.set_ylabel(\"Absolute Error\")\n",
    "#     save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "# plot_results(y_test, y_test_pred)\n",
    "\n",
    "# # === REPORT ON SPLITS ===\n",
    "# y_train_pred = best_model.predict(X_train_final)\n",
    "# y_val_pred = best_model.predict(X_val_final)\n",
    "# y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# performance = pd.DataFrame({\n",
    "#     \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "#     \"R-squared\": [\n",
    "#         r2_score(y_train, y_train_pred),\n",
    "#         r2_score(y_val, y_val_pred),\n",
    "#         r2_score(y_test, y_test_pred)\n",
    "#     ],\n",
    "#     \"MSE\": [\n",
    "#         mean_squared_error(y_train, y_train_pred),\n",
    "#         mean_squared_error(y_val, y_val_pred),\n",
    "#         mean_squared_error(y_test, y_test_pred)\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# performance.to_csv(f\"{plot_dir}/model_performance_by_split.csv\", index=False)\n",
    "# print(f\"Best model: {best_model_name}\")\n",
    "# print(performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffe1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "65 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "33 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "32 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.01983512 0.01355537 0.00228698        nan 0.02094479 0.02743009\n",
      " 0.01687243 0.02392573 0.01778721 0.00561879 0.01900548 0.01946466\n",
      " 0.02408297        nan 0.01355537 0.02807229 0.02700277 0.0058303\n",
      "        nan 0.02325862 0.02925416        nan 0.02861526 0.01750121\n",
      "        nan        nan 0.01547083 0.03343378        nan 0.02930158\n",
      "        nan 0.01221059 0.00565235 0.01543116 0.03343378 0.01561561\n",
      " 0.01119879        nan 0.02925416 0.02094479        nan 0.01679288\n",
      " 0.01377852 0.01320227        nan        nan 0.02305525 0.0119332\n",
      "        nan 0.02116807]\n",
      "  warnings.warn(\n",
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "70 fits failed out of a total of 250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1170, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/xgboost/training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/xgboost/core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/xgboost/core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value 2 for Parameter subsample exceed bound [0,1]\n",
      "subsample: Row subsample ratio of training instance.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [ 0.0097203   0.00183458         nan -0.00333224 -0.00325205  0.00882253\n",
      "         nan -0.00405552 -0.00088505         nan -0.00233367         nan\n",
      "         nan         nan -0.00206696         nan         nan  0.00608976\n",
      " -0.00490585 -0.00059007 -0.00175922  0.00257716 -0.00420085 -0.00305985\n",
      "         nan         nan -0.00061203 -0.00495954  0.00383431         nan\n",
      " -0.00270451 -0.00538477  0.0106033  -0.00501879  0.00087571 -0.0011808\n",
      "  0.00886828  0.00067932         nan -0.00254905 -0.00451675 -0.00280932\n",
      " -0.00224954  0.00559052 -0.00042768 -0.00488687         nan         nan\n",
      "  0.00407139 -0.00071818]\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1246088/1869951977.py:131: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=metrics_df_all.sort_values(\"R2_Test\", ascending=False), x=\"R2_Test\", y=\"Model\", palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuned Model: RandomForest_Tuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1246088/1869951977.py:157: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=feat_df, x=\"Importance\", y=\"Feature\", palette=\"crest\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R² Score: 0.0282\n",
      "Test Mean Squared Error: 60.7428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean\"\n",
    "\n",
    "plot_dir = f\"{output_root}/Reg_Results\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "df_clean = df[collinear_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA (Train/Test Only) ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_collinear, y, test_size=0.2, random_state=99)\n",
    "\n",
    "# # === SCALING ONLY (NO PCA) ===\n",
    "# scaler = StandardScaler()\n",
    "# X_train_final = scaler.fit_transform(X_train)\n",
    "# X_test_final = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_train_final = pca.fit_transform(X_train_scaled)\n",
    "X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# === TUNE MODELS USING RANDOMIZED SEARCH ===\n",
    "tuned_models = {}\n",
    "param_logs = []\n",
    "\n",
    "ridge_param_grid = {\"alpha\": np.logspace(-3, 3, 100)}\n",
    "ridge_search = RandomizedSearchCV(Ridge(), param_distributions=ridge_param_grid, n_iter=100, cv=5, scoring='r2', random_state=42)\n",
    "ridge_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Ridge_Tuned\"] = ridge_search.best_estimator_\n",
    "param_logs.append((\"Ridge_Tuned\", ridge_search.best_params_))\n",
    "\n",
    "lasso_param_grid = {\"alpha\": np.logspace(-3, 3, 100), \"max_iter\": [10000]}\n",
    "lasso_search = RandomizedSearchCV(Lasso(random_state=42), param_distributions=lasso_param_grid, n_iter=100, cv=5, scoring='r2', random_state=42)\n",
    "lasso_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Lasso_Tuned\"] = lasso_search.best_estimator_\n",
    "param_logs.append((\"Lasso_Tuned\", lasso_search.best_params_))\n",
    "\n",
    "enet_param_grid = {\n",
    "    \"alpha\": np.logspace(-3, 3, 100),\n",
    "    \"l1_ratio\": np.linspace(0.1, 0.99, 9),\n",
    "    \"max_iter\": [10000]\n",
    "}\n",
    "enet_search = RandomizedSearchCV(ElasticNet(random_state=42), param_distributions=enet_param_grid, n_iter=100, cv=5, scoring='r2', random_state=42)\n",
    "enet_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"ElasticNet_Tuned\"] = enet_search.best_estimator_\n",
    "param_logs.append((\"ElasticNet_Tuned\", enet_search.best_params_))\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300,500],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "                               param_distributions=rf_param_grid,\n",
    "                               n_iter=50, cv=5, scoring='r2', random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"RandomForest_Tuned\"] = rf_search.best_estimator_\n",
    "param_logs.append((\"RandomForest_Tuned\", rf_search.best_params_))\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 6, 10],\n",
    "    \"learning_rate\": [0.001, 0.0005, 0.0001],\n",
    "    \"subsample\": [0.7, 0.9, 2.0],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(XGBRegressor(random_state=42, verbosity=0, n_jobs=-1),\n",
    "                                param_distributions=xgb_param_grid,\n",
    "                                n_iter=50, cv=5, scoring='r2', random_state=42, n_jobs=-1)\n",
    "xgb_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"XGBoost_Tuned\"] = xgb_search.best_estimator_\n",
    "param_logs.append((\"XGBoost_Tuned\", xgb_search.best_params_))\n",
    "\n",
    "tuned_models[\"LinearRegression_Tuned\"] = LinearRegression()\n",
    "param_logs.append((\"LinearRegression_Tuned\", \"No tuning required\"))\n",
    "\n",
    "# === LOG BEST PARAMS ===\n",
    "param_df = pd.DataFrame(param_logs, columns=[\"Model\", \"Best_Params\"])\n",
    "param_df.to_csv(f\"{plot_dir}/best_model_hyperparameters.csv\", index=False)\n",
    "\n",
    "# === EVALUATE TUNED MODELS ON TEST ===\n",
    "results = {}\n",
    "all_metrics = []\n",
    "for name, model in tuned_models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_test_pred = model.predict(X_test_final)\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "        mse = mean_squared_error(y_test, y_test_pred)\n",
    "        results[name] = r2\n",
    "        all_metrics.append({\"Model\": name, \"R2_Test\": r2, \"MSE_Test\": mse})\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "metrics_df_all = pd.DataFrame(all_metrics)\n",
    "metrics_df_all.to_csv(f\"{plot_dir}/all_model_test_metrics.csv\", index=False)\n",
    "\n",
    "# === COMPARISON PLOT ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=metrics_df_all.sort_values(\"R2_Test\", ascending=False), x=\"R2_Test\", y=\"Model\", palette=\"viridis\")\n",
    "plt.title(\"Test R² Score by Model\")\n",
    "plt.xlabel(\"R² Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/model_comparison_r2_test.png\")\n",
    "plt.close()\n",
    "\n",
    "# === SELECT AND RETRAIN BEST MODEL FOR TEST SET ===\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = tuned_models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "print(f\"Best Tuned Model: {best_model_name}\")\n",
    "\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = best_model.feature_importances_\n",
    "    feat_labels = [f\"PC{i+1}\" for i in range(len(importances))]\n",
    "    feat_df = pd.DataFrame({\"Feature\": feat_labels, \"Importance\": importances})\n",
    "    feat_df = feat_df.sort_values(by=\"Importance\", ascending=False)\n",
    "    feat_df.to_csv(f\"{plot_dir}/best_model_feature_importances.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feat_df, x=\"Importance\", y=\"Feature\", palette=\"crest\")\n",
    "    plt.title(\"Feature Importances - Best Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_dir}/best_model_feature_importance_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === PRINT FINAL TEST METRICS ===\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test R² Score: {r2:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"True\": y_test,\n",
    "    \"Predicted\": y_test_pred\n",
    "})\n",
    "pred_df.to_csv(f\"{plot_dir}/best_model_predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfdec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: RandomForest\n",
      "        Split  R-squared       MSE\n",
      "0       Train   0.989451  0.506358\n",
      "1  Validation   0.895908  6.055242\n",
      "2        Test   0.872992  5.755399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3302722/2445501966.py:261: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean\"\n",
    "plot_dir = f\"{output_root}/Ridge_Plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "\n",
    "df_clean = df[collinear_features + spatial_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "X_spatial = df_clean[spatial_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA ===\n",
    "X_col_train, X_col_temp, X_spatial_train, X_spatial_temp, y_train, y_temp = train_test_split(\n",
    "    X_collinear, X_spatial, y, test_size=0.4, random_state=42)\n",
    "X_col_val, X_col_test, X_spatial_val, X_spatial_test, y_val, y_test = train_test_split(\n",
    "    X_col_temp, X_spatial_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_col_train_scaled = scaler.fit_transform(X_col_train)\n",
    "X_col_val_scaled = scaler.transform(X_col_val)\n",
    "X_col_test_scaled = scaler.transform(X_col_test)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca_train = pca.fit_transform(X_col_train_scaled)\n",
    "X_pca_val = pca.transform(X_col_val_scaled)\n",
    "X_pca_test = pca.transform(X_col_test_scaled)\n",
    "\n",
    "X_train_final = np.hstack([X_pca_train, X_spatial_train])\n",
    "X_val_final = np.hstack([X_pca_val, X_spatial_val])\n",
    "X_test_final = np.hstack([X_pca_test, X_spatial_test])\n",
    "\n",
    "# === DEFINE MODELS ===\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5),\n",
    "    \"Lasso\": LassoCV(cv=5, random_state=42, max_iter=10000),\n",
    "    \"ElasticNet\": ElasticNetCV(cv=5, random_state=42, max_iter=10000),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.01, random_state=42, n_jobs=-1, verbosity=0)\n",
    "}\n",
    "\n",
    "# === EVALUATE MODELS ON VALIDATION SET ===\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_val_pred = model.predict(X_val_final)\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        results[name] = r2\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# === SAVE COEFFICIENTS AND PERFORMANCE ===\n",
    "coeff_labels = [f'PC{i+1}' for i in range(3)] + spatial_features\n",
    "coef_df = pd.DataFrame({'Feature': coeff_labels, 'Coefficient': best_model.coef_}) if hasattr(best_model, 'coef_') else pd.DataFrame()\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['R-squared', 'MSE'],\n",
    "    'Value': [r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred)]\n",
    "})\n",
    "coef_df.to_csv(f\"{plot_dir}/best_model_coefficients.csv\", index=False)\n",
    "metrics_df.to_csv(f\"{plot_dir}/best_model_metrics.csv\", index=False)\n",
    "\n",
    "# === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === REPORT ON SPLITS ===\n",
    "y_train_pred = best_model.predict(X_train_final)\n",
    "y_val_pred = best_model.predict(X_val_final)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"R-squared\": [\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_val, y_val_pred),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ],\n",
    "    \"MSE\": [\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_val, y_val_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)\n",
    "    ]\n",
    "})\n",
    "\n",
    "performance.to_csv(f\"{plot_dir}/model_performance_by_split.csv\", index=False)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(performance)\n",
    "\n",
    "\n",
    "# Generate the code block for PCA + Clustering using validation data and applying to test data\n",
    "\n",
    "\n",
    "# === PCA + KMeans CLUSTERING (Fit on Validation, Apply to Test) ===\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "output_directory = plot_dir\n",
    "window_size = 5\n",
    "\n",
    "# --- Fit PCA on Validation Set ---\n",
    "X_scaled_val = X_col_val_scaled\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled_val)\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# --- Plot PCA Cumulative Variance (Elbow Plot) ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='s', linestyle='-', color='red', label=\"Cumulative Variance\")\n",
    "plt.axvline(x=optimal_components, color='green', linestyle='--', label=f\"Optimal Components ({optimal_components})\")\n",
    "plt.scatter(optimal_components, cumulative_variance[optimal_components-1], color='black', s=50, label=\"Chosen Point\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Elbow Plot with Cumulative Variance\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_Elbow_Plot_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Fit 2D PCA on Validation ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_val_2d = pca_2d.fit_transform(X_scaled_val)\n",
    "\n",
    "# --- KMeans on 2D PCA space (Validation) ---\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 15)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "    score = silhouette_score(pca_val_2d, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "optimal_score = max(silhouette_scores)\n",
    "\n",
    "# --- Plot Silhouette Scores ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='-', color='purple', label=\"Silhouette Score\")\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', label=f\"Optimal k = {optimal_k}\")\n",
    "plt.scatter(optimal_k, optimal_score, color='red', s=100, zorder=5)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs Number of Clusters (Validation)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Silhouette_Score_vs_K_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Apply to Test Data ---\n",
    "X_scaled_test = X_col_test_scaled\n",
    "pca_test_2d = pca_2d.transform(X_scaled_test)\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "test_clusters = kmeans_final.predict(pca_test_2d)\n",
    "\n",
    "# --- Create Test DataFrame with PCA + Clusters ---\n",
    "pca_test_df = pd.DataFrame(pca_test_2d, columns=[\"PCA1\", \"PCA2\"])\n",
    "pca_test_df[\"X_Centroid_Velocity_MA\"] = X_col_test[:, 0]  # Area_MA is first collinear feature\n",
    "pca_test_df[\"Cluster\"] = test_clusters\n",
    "\n",
    "# --- Clustered 2D PCA (Colored by Area) ---\n",
    "vmin = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 5)\n",
    "vmax = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 95)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sc = plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"X_Centroid_Velocity_MA\"], cmap=\"plasma\", alpha=0.8, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(sc, label=\"X_Centroid Velocity (MA)\")\n",
    "plt.title(\"2D PCA Test Data Colored by X_Centroid Velocity(MA)\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Cluster Plot with Centers ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"Cluster\"], cmap=\"tab10\", alpha=0.8)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, marker='o', label='Centers')\n",
    "for idx, (x, y) in enumerate(centers):\n",
    "    plt.text(x, y, str(idx), fontsize=12, fontweight='bold', ha='center', va='center', color='white',\n",
    "             bbox=dict(facecolor='black', boxstyle='circle,pad=0.2'))\n",
    "plt.title(f\"Test Data PCA with KMeans Clusters (k={optimal_k})\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_Clusters_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Boxplot: Area by Cluster (Test Data) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n",
    "sns.stripplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", color='gray', alpha=0.5, jitter=0.2, size=3)\n",
    "plt.title(\"X_Centroid Velocity (MA) by Cluster (Test Data)\", fontsize=14)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"X_Centroid Velocity (MA)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Boxplot_X_Centroid_Velocity_MA_By_Cluster_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Save clustered PCA test data and summary ---\n",
    "pca_test_df.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Results_Test_{window_size}.csv\"), index=False)\n",
    "\n",
    "summary = pca_test_df.groupby(\"Cluster\").agg({\n",
    "    \"PCA1\": [\"mean\", \"std\"],\n",
    "    \"PCA2\": [\"mean\", \"std\"],\n",
    "    \"X_Centroid_Velocity_MA\": [\"mean\", \"std\"]\n",
    "}).reset_index()\n",
    "summary.columns = [\"_\".join(col).strip(\"_\") for col in summary.columns.values]\n",
    "summary.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Summary_Test_{window_size}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae6cb0",
   "metadata": {},
   "source": [
    "# Create the updated FNN pipeline script content with PCA + spatial features + grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d207188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=52.1162\n",
      "Epoch 20: Loss=16.9313\n",
      "Epoch 30: Loss=3.6308\n",
      "Epoch 40: Loss=2.0905\n",
      "Epoch 50: Loss=2.4759\n",
      "Epoch 60: Loss=2.7854\n",
      "Epoch 70: Loss=1.7099\n",
      "Epoch 80: Loss=1.6702\n",
      "Epoch 90: Loss=3.2997\n",
      "Epoch 100: Loss=1.1207\n",
      "✅ MSE: 6.4259 | R²: 0.8582\n",
      "▶ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=44.0612\n",
      "Epoch 20: Loss=25.5412\n",
      "Epoch 30: Loss=5.0369\n",
      "Epoch 40: Loss=9.0516\n",
      "Epoch 50: Loss=2.0109\n",
      "Epoch 60: Loss=2.2338\n",
      "Epoch 70: Loss=1.6553\n",
      "Epoch 80: Loss=1.4908\n",
      "Epoch 90: Loss=1.9928\n",
      "Epoch 100: Loss=1.2453\n",
      "Epoch 110: Loss=1.4414\n",
      "Epoch 120: Loss=1.2631\n",
      "Epoch 130: Loss=1.2409\n",
      "Epoch 140: Loss=1.4140\n",
      "Epoch 150: Loss=2.0771\n",
      "Epoch 160: Loss=3.7896\n",
      "Epoch 170: Loss=0.9716\n",
      "Epoch 180: Loss=1.5261\n",
      "Epoch 190: Loss=1.0281\n",
      "Epoch 200: Loss=0.9659\n",
      "✅ MSE: 5.9163 | R²: 0.8694\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=45.5627\n",
      "Epoch 20: Loss=42.3353\n",
      "Epoch 30: Loss=12.9980\n",
      "Epoch 40: Loss=4.0565\n",
      "Epoch 50: Loss=2.3249\n",
      "Epoch 60: Loss=2.0444\n",
      "Epoch 70: Loss=1.8111\n",
      "Epoch 80: Loss=2.3445\n",
      "Epoch 90: Loss=2.0945\n",
      "Epoch 100: Loss=1.6284\n",
      "✅ MSE: 6.1154 | R²: 0.8650\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=45.7736\n",
      "Epoch 20: Loss=36.8649\n",
      "Epoch 30: Loss=13.5252\n",
      "Epoch 40: Loss=4.3540\n",
      "Epoch 50: Loss=3.0131\n",
      "Epoch 60: Loss=2.8095\n",
      "Epoch 70: Loss=2.7447\n",
      "Epoch 80: Loss=2.3129\n",
      "Epoch 90: Loss=2.1928\n",
      "Epoch 100: Loss=2.0644\n",
      "Epoch 110: Loss=2.5505\n",
      "Epoch 120: Loss=2.8917\n",
      "Epoch 130: Loss=4.4316\n",
      "Epoch 140: Loss=2.3530\n",
      "Epoch 150: Loss=2.4281\n",
      "Epoch 160: Loss=1.9631\n",
      "Epoch 170: Loss=1.6274\n",
      "Epoch 180: Loss=1.1657\n",
      "Epoch 190: Loss=1.0390\n",
      "Epoch 200: Loss=0.8935\n",
      "✅ MSE: 5.9963 | R²: 0.8677\n",
      "▶ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=46.2022\n",
      "Epoch 20: Loss=39.3739\n",
      "Epoch 30: Loss=19.0798\n",
      "Epoch 40: Loss=5.4680\n",
      "Epoch 50: Loss=3.5768\n",
      "Epoch 60: Loss=2.4817\n",
      "Epoch 70: Loss=2.0792\n",
      "Epoch 80: Loss=2.5394\n",
      "Epoch 90: Loss=1.8659\n",
      "Epoch 100: Loss=2.6436\n",
      "✅ MSE: 5.7057 | R²: 0.8741\n",
      "▶ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=49.0573\n",
      "Epoch 20: Loss=37.4543\n",
      "Epoch 30: Loss=20.2382\n",
      "Epoch 40: Loss=9.6865\n",
      "Epoch 50: Loss=2.1415\n",
      "Epoch 60: Loss=1.9754\n",
      "Epoch 70: Loss=1.7747\n",
      "Epoch 80: Loss=1.5517\n",
      "Epoch 90: Loss=1.6954\n",
      "Epoch 100: Loss=1.6788\n",
      "Epoch 110: Loss=1.5406\n",
      "Epoch 120: Loss=1.9494\n",
      "Epoch 130: Loss=1.3233\n",
      "Epoch 140: Loss=1.0911\n",
      "Epoch 150: Loss=1.3113\n",
      "Epoch 160: Loss=1.0693\n",
      "Epoch 170: Loss=2.0441\n",
      "Epoch 180: Loss=1.0040\n",
      "Epoch 190: Loss=1.0846\n",
      "Epoch 200: Loss=1.7198\n",
      "✅ MSE: 5.9050 | R²: 0.8697\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=47.1510\n",
      "Epoch 20: Loss=48.5934\n",
      "Epoch 30: Loss=37.2439\n",
      "Epoch 40: Loss=27.3430\n",
      "Epoch 50: Loss=25.1004\n",
      "Epoch 60: Loss=6.1760\n",
      "Epoch 70: Loss=3.0716\n",
      "Epoch 80: Loss=1.6924\n",
      "Epoch 90: Loss=1.4708\n",
      "Epoch 100: Loss=1.4732\n",
      "✅ MSE: 5.8028 | R²: 0.8719\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=45.6813\n",
      "Epoch 20: Loss=39.4380\n",
      "Epoch 30: Loss=32.8151\n",
      "Epoch 40: Loss=21.2598\n",
      "Epoch 50: Loss=6.7351\n",
      "Epoch 60: Loss=6.3589\n",
      "Epoch 70: Loss=3.8232\n",
      "Epoch 80: Loss=4.6236\n",
      "Epoch 90: Loss=2.9001\n",
      "Epoch 100: Loss=2.7453\n",
      "Epoch 110: Loss=3.7230\n",
      "Epoch 120: Loss=2.4786\n",
      "Epoch 130: Loss=2.3575\n",
      "Epoch 140: Loss=2.5504\n",
      "Epoch 150: Loss=2.2163\n",
      "Epoch 160: Loss=1.9087\n",
      "Epoch 170: Loss=2.1808\n",
      "Epoch 180: Loss=1.6886\n",
      "Epoch 190: Loss=2.5447\n",
      "Epoch 200: Loss=1.8261\n",
      "✅ MSE: 6.4423 | R²: 0.8578\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=46.3828\n",
      "Epoch 20: Loss=41.3540\n",
      "Epoch 30: Loss=28.3776\n",
      "Epoch 40: Loss=22.8418\n",
      "Epoch 50: Loss=6.3732\n",
      "Epoch 60: Loss=3.4637\n",
      "Epoch 70: Loss=2.9952\n",
      "Epoch 80: Loss=2.2693\n",
      "Epoch 90: Loss=5.3749\n",
      "Epoch 100: Loss=1.9605\n",
      "✅ MSE: 5.9809 | R²: 0.8680\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=45.1468\n",
      "Epoch 20: Loss=38.9560\n",
      "Epoch 30: Loss=33.2122\n",
      "Epoch 40: Loss=13.2864\n",
      "Epoch 50: Loss=2.8958\n",
      "Epoch 60: Loss=2.3107\n",
      "Epoch 70: Loss=1.9202\n",
      "Epoch 80: Loss=2.1681\n",
      "Epoch 90: Loss=2.8151\n",
      "Epoch 100: Loss=1.7196\n",
      "Epoch 110: Loss=1.4288\n",
      "Epoch 120: Loss=3.1473\n",
      "Epoch 130: Loss=1.3149\n",
      "Epoch 140: Loss=1.3549\n",
      "Epoch 150: Loss=1.0913\n",
      "Epoch 160: Loss=1.4235\n",
      "Epoch 170: Loss=1.1290\n",
      "Epoch 180: Loss=1.2345\n",
      "Epoch 190: Loss=1.6973\n",
      "Epoch 200: Loss=2.3299\n",
      "✅ MSE: 5.1187 | R²: 0.8870\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=50.8608\n",
      "Epoch 20: Loss=50.1388\n",
      "Epoch 30: Loss=51.3908\n",
      "Epoch 40: Loss=51.3258\n",
      "Epoch 50: Loss=52.6734\n",
      "Epoch 60: Loss=48.2608\n",
      "Epoch 70: Loss=49.1878\n",
      "Epoch 80: Loss=49.5040\n",
      "Epoch 90: Loss=47.5723\n",
      "Epoch 100: Loss=47.4533\n",
      "✅ MSE: 45.3211 | R²: -0.0001\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=44.5514\n",
      "Epoch 20: Loss=38.0112\n",
      "Epoch 30: Loss=26.6996\n",
      "Epoch 40: Loss=10.4376\n",
      "Epoch 50: Loss=3.9044\n",
      "Epoch 60: Loss=2.5107\n",
      "Epoch 70: Loss=3.1548\n",
      "Epoch 80: Loss=3.1284\n",
      "Epoch 90: Loss=1.9330\n",
      "Epoch 100: Loss=1.8853\n",
      "Epoch 110: Loss=1.7280\n",
      "Epoch 120: Loss=1.8504\n",
      "Epoch 130: Loss=2.7892\n",
      "Epoch 140: Loss=1.7813\n",
      "Epoch 150: Loss=2.1610\n",
      "Epoch 160: Loss=1.8085\n",
      "Epoch 170: Loss=1.5632\n",
      "Epoch 180: Loss=1.8779\n",
      "Epoch 190: Loss=2.4203\n",
      "Epoch 200: Loss=1.2854\n",
      "✅ MSE: 4.9408 | R²: 0.8910\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=49.9954\n",
      "Epoch 20: Loss=44.7101\n",
      "Epoch 30: Loss=42.5655\n",
      "Epoch 40: Loss=39.7526\n",
      "Epoch 50: Loss=33.1410\n",
      "Epoch 60: Loss=27.3470\n",
      "Epoch 70: Loss=21.5745\n",
      "Epoch 80: Loss=11.8788\n",
      "Epoch 90: Loss=3.8998\n",
      "Epoch 100: Loss=2.7048\n",
      "✅ MSE: 5.7469 | R²: 0.8732\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=49.3708\n",
      "Epoch 20: Loss=52.4787\n",
      "Epoch 30: Loss=42.5091\n",
      "Epoch 40: Loss=29.4656\n",
      "Epoch 50: Loss=19.9080\n",
      "Epoch 60: Loss=10.8046\n",
      "Epoch 70: Loss=9.4993\n",
      "Epoch 80: Loss=3.5258\n",
      "Epoch 90: Loss=2.3739\n",
      "Epoch 100: Loss=2.0944\n",
      "Epoch 110: Loss=2.3561\n",
      "Epoch 120: Loss=2.1215\n",
      "Epoch 130: Loss=1.9457\n",
      "Epoch 140: Loss=2.1734\n",
      "Epoch 150: Loss=1.7101\n",
      "Epoch 160: Loss=2.1746\n",
      "Epoch 170: Loss=1.5372\n",
      "Epoch 180: Loss=1.7989\n",
      "Epoch 190: Loss=1.3746\n",
      "Epoch 200: Loss=1.2256\n",
      "✅ MSE: 5.6408 | R²: 0.8755\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=49.7601\n",
      "Epoch 20: Loss=44.3790\n",
      "Epoch 30: Loss=47.7057\n",
      "Epoch 40: Loss=40.1788\n",
      "Epoch 50: Loss=39.9010\n",
      "Epoch 60: Loss=26.5331\n",
      "Epoch 70: Loss=16.1157\n",
      "Epoch 80: Loss=11.3490\n",
      "Epoch 90: Loss=4.9874\n",
      "Epoch 100: Loss=3.0317\n",
      "✅ MSE: 6.9126 | R²: 0.8475\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=49.5182\n",
      "Epoch 20: Loss=44.5851\n",
      "Epoch 30: Loss=39.9723\n",
      "Epoch 40: Loss=37.4735\n",
      "Epoch 50: Loss=31.5905\n",
      "Epoch 60: Loss=21.5721\n",
      "Epoch 70: Loss=11.7656\n",
      "Epoch 80: Loss=6.6148\n",
      "Epoch 90: Loss=3.9602\n",
      "Epoch 100: Loss=3.0184\n",
      "Epoch 110: Loss=3.8001\n",
      "Epoch 120: Loss=2.6448\n",
      "Epoch 130: Loss=2.4269\n",
      "Epoch 140: Loss=2.5392\n",
      "Epoch 150: Loss=2.1907\n",
      "Epoch 160: Loss=2.1756\n",
      "Epoch 170: Loss=3.4952\n",
      "Epoch 180: Loss=2.1513\n",
      "Epoch 190: Loss=2.1264\n",
      "Epoch 200: Loss=2.4485\n",
      "✅ MSE: 5.6708 | R²: 0.8749\n",
      "🏆 Best config: {'lr': 0.0005, 'batch_size': 16, 'hidden_sizes': (64, 32, 16), 'epochs': 200}, R²=0.8910\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIG ---\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean/Reg_Analysis_FNN_PCA\"\n",
    "os.makedirs(f\"{output_root}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/plots\", exist_ok=True)\n",
    "\n",
    "# --- FNN Model ---\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"✅ MSE: {mse:.4f} | R²: {r2:.4f}\")\n",
    "    return mse, r2, y_true, y_pred\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_results(y_true, y_pred, train_losses=None):\n",
    "    residuals = y_true - y_pred\n",
    "    if train_losses:\n",
    "        plt.figure(); plt.plot(train_losses); plt.title(\"Training Loss\"); plt.tight_layout()\n",
    "        plt.savefig(f\"{output_root}/plots/loss_curve.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_pred, y=residuals); plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(\"Residuals vs Predicted\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_vs_predicted.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=y_pred); plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(\"Predicted vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/predicted_vs_actual.png\"); plt.close()\n",
    "    plt.figure(); sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Residuals\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_distribution.png\"); plt.close()\n",
    "    plt.figure(); plt.plot(np.sort(np.abs(residuals)), np.linspace(0, 1, len(residuals)))\n",
    "    plt.title(\"Cumulative Absolute Errors\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/cumulative_absolute_errors.png\"); plt.close()\n",
    "    plt.figure(); stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/qq_plot_residuals.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=np.abs(residuals))\n",
    "    plt.title(\"Absolute Error vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/absolute_error_vs_actual.png\"); plt.close()\n",
    "\n",
    "# --- Run FNN ---\n",
    "def run_fnn(X_train, X_val, X_test, y_train, y_val, y_test, input_size, hidden_sizes, lr, batch_size, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FNN(input_size=input_size, hidden_sizes=hidden_sizes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
    "    X_val_tensor = torch.tensor(X_val).float().to(device)\n",
    "    y_val_tensor = torch.tensor(y_val).float().to(device)\n",
    "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, X_test_tensor, y_test_tensor\n",
    "\n",
    "# --- Grid Search ---\n",
    "def grid_search_fnn(csv_path, param_grid):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    shape_features = ['Area_MA', 'Perimeter_MA', 'Extent_MA', 'Solidity_MA', 'Compactness_MA', 'Elongation_MA', 'Circularity_MA', 'Convexity_MA']\n",
    "    spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "    target = 'X_Centroid_Velocity_MA'\n",
    "    df = df.dropna(subset=shape_features + spatial_features + [target])\n",
    "\n",
    "    X_shape = df[shape_features].values\n",
    "    X_spatial = df[spatial_features].values\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    X_shape_scaled = StandardScaler().fit_transform(X_shape)\n",
    "    X_train_s, X_temp_s, X_train_spatial, X_temp_spatial, y_train, y_temp = train_test_split(\n",
    "        X_shape_scaled, X_spatial, y, test_size=0.4, random_state=42)\n",
    "    X_val_s, X_test_s, X_val_spatial, X_test_spatial, y_val, y_test = train_test_split(\n",
    "        X_temp_s, X_temp_spatial, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_s)\n",
    "    X_val_pca = pca.transform(X_val_s)\n",
    "    X_test_pca = pca.transform(X_test_s)\n",
    "\n",
    "    # Combine PCA + spatial\n",
    "    X_train_final = np.hstack([X_train_pca, X_train_spatial])\n",
    "    X_val_final = np.hstack([X_val_pca, X_val_spatial])\n",
    "    X_test_final = np.hstack([X_test_pca, X_test_spatial])\n",
    "\n",
    "    results = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_losses = None\n",
    "    best_test_tensors = None\n",
    "\n",
    "    for lr, batch_size, hidden_sizes, epochs in product(param_grid[\"lr\"], param_grid[\"batch_size\"],\n",
    "                                                        param_grid[\"hidden_sizes\"], param_grid[\"epochs\"]):\n",
    "        print(f\"▶ Training: lr={lr}, batch={batch_size}, hidden={hidden_sizes}, epochs={epochs}\")\n",
    "        model, train_losses, X_test_tensor, y_test_tensor = run_fnn(\n",
    "            X_train_final, X_val_final, X_test_final,\n",
    "            y_train, y_val, y_test,\n",
    "            input_size=X_train_final.shape[1],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "        results.append({\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"hidden_sizes\": str(hidden_sizes),\n",
    "            \"epochs\": epochs, \"mse\": mse, \"r2\": r2\n",
    "        })\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_config = {\n",
    "                \"lr\": lr, \"batch_size\": batch_size,\n",
    "                \"hidden_sizes\": hidden_sizes, \"epochs\": epochs\n",
    "            }\n",
    "            best_losses = train_losses\n",
    "            best_test_tensors = (X_test_tensor, y_test_tensor, y_true, y_pred)\n",
    "\n",
    "    # Save best model\n",
    "    model_path = f\"{output_root}/models/fnn_best_model.pt\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(f\"{output_root}/results/grid_search_results.csv\", index=False)\n",
    "    print(f\"🏆 Best config: {best_config}, R²={best_r2:.4f}\")\n",
    "    return best_model, best_config, best_losses, best_test_tensors\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        \"lr\": [0.001, 0.0005],\n",
    "        \"batch_size\": [16, 32],\n",
    "        \"hidden_sizes\": [(128, 64, 32), (64, 32, 16)],\n",
    "        \"epochs\": [100, 200]\n",
    "    }\n",
    "    csv_path = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "    best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = grid_search_fnn(csv_path, param_grid)\n",
    "    plot_results(y_true, y_pred, train_losses=best_losses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shp_chc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
