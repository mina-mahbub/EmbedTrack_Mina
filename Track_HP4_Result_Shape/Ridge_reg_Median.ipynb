{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "122fb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phate\n",
    "import scprep\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from scipy.stats import stats\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from skimage.morphology import dilation, erosion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from skimage import measure\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D Plotting\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from skimage.morphology import dilation, erosion\n",
    "from skimage import measure\n",
    "from scipy.ndimage import center_of_mass\n",
    "from glob import glob\n",
    "import random\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from tifffile import imread\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712b629",
   "metadata": {},
   "source": [
    "# Final Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12573f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/externals/loky/backend/popen_loky_posix.py\", line 155, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/__init__.py\", line 114, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/__init__.py\", line 114, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/__init__.py\", line 114, in <module>\n",
      "    parser = argparse.ArgumentParser(\"Command line parser\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/argparse.py\", line 1807, in __init__\n",
      "    from .memory import Memory    \n",
      "from .memory import Memory    \n",
      "from .memory import Memory\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/memory.py\", line 29, in <module>\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/memory.py\", line 12, in <module>\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/memory.py\", line 28, in <module>\n",
      "    self.add_argument(\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/argparse.py\", line 1486, in add_argument\n",
      "    self._get_formatter()._format_args(action, None)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/argparse.py\", line 2613, in _get_formatter\n",
      "    import asyncio\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/asyncio/__init__.py\", line 8, in <module>\n",
      "    from ._store_backends import CacheWarning  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^\n",
      "from . import hashing\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/_store_backends.py\", line 17, in <module>\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/hashing.py\", line 10, in <module>\n",
      "    return self.formatter_class(prog=self.prog)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/argparse.py\", line 189, in __init__\n",
      "    from .backports import concurrency_safe_rename\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/backports.py\", line 112, in <module>\n",
      "    from .base_events import *\n",
      "    self._whitespace_matcher = _re.compile(r'\\s+', _re.ASCII)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/asyncio/base_events.py\", line 18, in <module>\n",
      "                               ^    ^import concurrent.futures^\n",
      "^^^^^^^^^^  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/concurrent/futures/__init__.py\", line 8, in <module>\n",
      "^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/re/__init__.py\", line 228, in compile\n",
      "    return _compile(pattern, flags)\n",
      "    import pickle\n",
      "           ^^^^^^^^^  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/pickle.py\", line 37, in <module>\n",
      "^^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/re/__init__.py\", line 307, in _compile\n",
      "    import numpy as np\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/numpy/__init__.py\", line 149, in <module>\n",
      "    p = _compiler.compile(pattern, flags)\n",
      "        ^    ^from . import lib^\n",
      "^^^^^^^^^^^  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/numpy/lib/__init__.py\", line 31, in <module>\n",
      "^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<function _releaseLock at 0x7ff64dbf7100>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/re/_compiler.py\", line 749, in compile\n",
      "    import _compat_pickle\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 921, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 819, in module_from_spec\n",
      "  File \"<frozen importlib._bootstrap>\", line 798, in _init_module_attrs\n",
      "  File \"<frozen importlib._bootstrap>\", line 638, in cached\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 612, in _get_cached\n",
      "KeyboardInterrupt\n",
      "    from concurrent.futures._base import (FIRST_COMPLETED,\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/concurrent/futures/_base.py\", line 7, in <module>\n",
      "    from . import polynomial\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/numpy/lib/polynomial.py\", line 30, in <module>\n",
      "    code = _code(p, flags)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/re/_compiler.py\", line 582, in _code\n",
      "    import logging\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/logging/__init__.py\", line 26, in <module>\n",
      "    class RankWarning(UserWarning):\n",
      "KeyboardInterrupt\n",
      "    _compile(code, p.data, flags)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/re/_compiler.py\", line 111, in _compile\n",
      "    _compile(code, av[2], flags)\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/re/_compiler.py\", line 86, in _compile\n",
      "    charset, hascased = _optimize_charset(av, iscased, tolower, fixes)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/re/_compiler.py\", line 306, in _optimize_charset\n",
      "    break\n",
      "KeyboardInterrupt\n",
      "    import sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/traceback.py\", line 5, in <module>\n",
      "    import linecache\n",
      "  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/linecache.py\", line 11, in <module>\n",
      "    import tokenize\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 991, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1124, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 753, in _compile_bytecode\n",
      "KeyboardInterrupt\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/MinaHossain/anaconda3/envs/Shp_chc/lib/python3.12/multiprocessing/queues.py\", line 108, in get\n    if not self._rlock.acquire(block, timeout):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     75\u001b[39m rf_param_grid = {\n\u001b[32m     76\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m400\u001b[39m],\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m15\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_features\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msqrt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlog2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     81\u001b[39m }\n\u001b[32m     82\u001b[39m rf_search = RandomizedSearchCV(RandomForestRegressor(random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m),  param_distributions=rf_param_grid, n_iter=\u001b[32m30\u001b[39m, cv=\u001b[32m3\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mrf_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m tuned_models[\u001b[33m\"\u001b[39m\u001b[33mRandomForest_Tuned\u001b[39m\u001b[33m\"\u001b[39m] = rf_search.best_estimator_\n\u001b[32m     85\u001b[39m param_logs.append((\u001b[33m\"\u001b[39m\u001b[33mRandomForest_Tuned\u001b[39m\u001b[33m\"\u001b[39m, rf_search.best_params_))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/parallel.py:2005\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1999\u001b[39m \u001b[38;5;28mself\u001b[39m._call_ref = weakref.ref(output)\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2005\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/parallel.py:1643\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1641\u001b[39m detach_generator_exit = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1642\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1643\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1644\u001b[39m     \u001b[38;5;66;03m# first yield returns None, for internal use only. This ensures\u001b[39;00m\n\u001b[32m   1645\u001b[39m     \u001b[38;5;66;03m# that we enter the try/except block and start dispatching the\u001b[39;00m\n\u001b[32m   1646\u001b[39m     \u001b[38;5;66;03m# tasks.\u001b[39;00m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/parallel.py:1629\u001b[39m, in \u001b[36mParallel._start\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1626\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dispatch_one_batch(iterator):\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28mself\u001b[39m._iterating = \u001b[38;5;28mself\u001b[39m._original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1630\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch == \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[32m   1634\u001b[39m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[32m   1635\u001b[39m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/parallel.py:1517\u001b[39m, in \u001b[36mParallel.dispatch_one_batch\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/parallel.py:1418\u001b[39m, in \u001b[36mParallel._dispatch\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28mself\u001b[39m._jobs.append(batch_tracker)\n\u001b[32m   1413\u001b[39m \u001b[38;5;66;03m# If return_ordered is False, the batch_tracker is not stored in the\u001b[39;00m\n\u001b[32m   1414\u001b[39m \u001b[38;5;66;03m# jobs queue at the time of submission. Instead, it will be appended to\u001b[39;00m\n\u001b[32m   1415\u001b[39m \u001b[38;5;66;03m# the queue by itself as soon as the callback is triggered to be able\u001b[39;00m\n\u001b[32m   1416\u001b[39m \u001b[38;5;66;03m# to return the results in the order of completion.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m job = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m batch_tracker.register_job(job)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/_parallel_backends.py:588\u001b[39m, in \u001b[36mLokyBackend.apply_async\u001b[39m\u001b[34m(self, func, callback)\u001b[39m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    587\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     future = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_workers\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    590\u001b[39m         future.add_done_callback(callback)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/externals/loky/reusable_executor.py:225\u001b[39m, in \u001b[36m_ReusablePoolExecutor.submit\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msubmit\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, *args, **kwargs):\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._submit_resize_lock:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:1226\u001b[39m, in \u001b[36mProcessPoolExecutor.submit\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._flags.shutdown_lock:\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._flags.broken \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1226\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._flags.broken\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._flags.shutdown:\n\u001b[32m   1228\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ShutdownExecutorError(\n\u001b[32m   1229\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcannot schedule new futures after shutdown\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1230\u001b[39m         )\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median\"\n",
    "plot_dir = f\"{output_root}/Reg_Results\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA','Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "df_clean = df[collinear_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA (Train/Test Only) ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_collinear, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # === SCALING ONLY (NO PCA) ===\n",
    "# scaler = StandardScaler()\n",
    "# X_train_final = scaler.fit_transform(X_train)\n",
    "# X_test_final = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_train_final = pca.fit_transform(X_train_scaled)\n",
    "X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# === TUNE MODELS USING RANDOMIZED SEARCH ===\n",
    "tuned_models = {}\n",
    "param_logs = []\n",
    "\n",
    "ridge_param_grid = {\"alpha\": np.logspace(-3, 3, 100)}\n",
    "ridge_search = RandomizedSearchCV(Ridge(), param_distributions=ridge_param_grid, n_iter=20, cv=3, scoring='r2', random_state=42)\n",
    "ridge_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Ridge_Tuned\"] = ridge_search.best_estimator_\n",
    "param_logs.append((\"Ridge_Tuned\", ridge_search.best_params_))\n",
    "\n",
    "lasso_param_grid = {\"alpha\": np.logspace(-3, 3, 100), \"max_iter\": [10000]}\n",
    "lasso_search = RandomizedSearchCV(Lasso(random_state=42), param_distributions=lasso_param_grid, n_iter=20, cv=3, scoring='r2', random_state=42)\n",
    "lasso_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"Lasso_Tuned\"] = lasso_search.best_estimator_\n",
    "param_logs.append((\"Lasso_Tuned\", lasso_search.best_params_))\n",
    "\n",
    "enet_param_grid = {\n",
    "    \"alpha\": np.logspace(-3, 3, 50),\n",
    "    \"l1_ratio\": np.linspace(0.1, 0.9, 9),\n",
    "    \"max_iter\": [10000]\n",
    "}\n",
    "enet_search = RandomizedSearchCV(ElasticNet(random_state=42), param_distributions=enet_param_grid, n_iter=30, cv=3, scoring='r2', random_state=42)\n",
    "enet_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"ElasticNet_Tuned\"] = enet_search.best_estimator_\n",
    "param_logs.append((\"ElasticNet_Tuned\", enet_search.best_params_))\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 400],\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(RandomForestRegressor(random_state=42, n_jobs=-1),  param_distributions=rf_param_grid, n_iter=30, cv=3, scoring='r2', random_state=42, n_jobs=-1)\n",
    "rf_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"RandomForest_Tuned\"] = rf_search.best_estimator_\n",
    "param_logs.append((\"RandomForest_Tuned\", rf_search.best_params_))\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 400],\n",
    "    \"max_depth\": [3, 6, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "}\n",
    "xgb_search = RandomizedSearchCV(XGBRegressor(random_state=42, verbosity=0, n_jobs=-1), param_distributions=xgb_param_grid, n_iter=30, cv=3, scoring='r2', random_state=42, n_jobs=-1)\n",
    "xgb_search.fit(X_train_final, y_train)\n",
    "tuned_models[\"XGBoost_Tuned\"] = xgb_search.best_estimator_\n",
    "param_logs.append((\"XGBoost_Tuned\", xgb_search.best_params_))\n",
    "\n",
    "tuned_models[\"LinearRegression_Tuned\"] = LinearRegression()\n",
    "param_logs.append((\"LinearRegression_Tuned\", \"No tuning required\"))\n",
    "\n",
    "# === LOG BEST PARAMS ===\n",
    "param_df = pd.DataFrame(param_logs, columns=[\"Model\", \"Best_Params\"])\n",
    "param_df.to_csv(f\"{plot_dir}/best_model_hyperparameters.csv\", index=False)\n",
    "\n",
    "# === EVALUATE TUNED MODELS ON TEST ===\n",
    "results = {}\n",
    "all_metrics = []\n",
    "for name, model in tuned_models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_test_pred = model.predict(X_test_final)\n",
    "        r2 = r2_score(y_test, y_test_pred)\n",
    "        mse = mean_squared_error(y_test, y_test_pred)\n",
    "        results[name] = r2\n",
    "        all_metrics.append({\"Model\": name, \"R2_Test\": r2, \"MSE_Test\": mse})\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "metrics_df_all = pd.DataFrame(all_metrics)\n",
    "metrics_df_all.to_csv(f\"{plot_dir}/all_model_test_metrics.csv\", index=False)\n",
    "\n",
    "# === COMPARISON PLOT ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=metrics_df_all.sort_values(\"R2_Test\", ascending=False), x=\"R2_Test\", y=\"Model\", palette=\"viridis\")\n",
    "plt.title(\"Test R² Score by Model\")\n",
    "plt.xlabel(\"R² Score\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/model_comparison_r2_test.png\")\n",
    "plt.close()\n",
    "\n",
    "# === SELECT AND RETRAIN BEST MODEL FOR TEST SET ===\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = tuned_models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "print(f\"Best Tuned Model: {best_model_name}\")\n",
    "\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "# === FEATURE IMPORTANCE (if available) ===\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = best_model.feature_importances_\n",
    "    feat_labels = [f\"PC{i+1}\" for i in range(len(importances))]\n",
    "    feat_df = pd.DataFrame({\"Feature\": feat_labels, \"Importance\": importances})\n",
    "    feat_df = feat_df.sort_values(by=\"Importance\", ascending=False)\n",
    "    feat_df.to_csv(f\"{plot_dir}/best_model_feature_importances.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feat_df, x=\"Importance\", y=\"Feature\", palette=\"crest\")\n",
    "    plt.title(\"Feature Importances - Best Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plot_dir}/best_model_feature_importance_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === PRINT FINAL TEST METRICS ===\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test R² Score: {r2:.4f}\")\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"True\": y_test,\n",
    "    \"Predicted\": y_test_pred\n",
    "})\n",
    "pred_df.to_csv(f\"{plot_dir}/best_model_predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfdec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: RandomForest\n",
      "        Split  R-squared        MSE\n",
      "0       Train   0.851260  21.423358\n",
      "1  Validation   0.692919  65.089067\n",
      "2        Test   0.109408  51.414964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3045944/3060033429.py:262: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median\"\n",
    "plot_dir = f\"{output_root}/Ridge_Plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "# spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "spatial_features = ['Centroid_Y_MA',  'Y_Centroid_Distance_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "\n",
    "df_clean = df[collinear_features + spatial_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "X_spatial = df_clean[spatial_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA ===\n",
    "X_col_train, X_col_temp, X_spatial_train, X_spatial_temp, y_train, y_temp = train_test_split(\n",
    "    X_collinear, X_spatial, y, test_size=0.5, random_state=42)\n",
    "X_col_val, X_col_test, X_spatial_val, X_spatial_test, y_val, y_test = train_test_split(\n",
    "    X_col_temp, X_spatial_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_col_train_scaled = scaler.fit_transform(X_col_train)\n",
    "X_col_val_scaled = scaler.transform(X_col_val)\n",
    "X_col_test_scaled = scaler.transform(X_col_test)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca_train = pca.fit_transform(X_col_train_scaled)\n",
    "X_pca_val = pca.transform(X_col_val_scaled)\n",
    "X_pca_test = pca.transform(X_col_test_scaled)\n",
    "\n",
    "X_train_final = np.hstack([X_pca_train, X_spatial_train])\n",
    "X_val_final = np.hstack([X_pca_val, X_spatial_val])\n",
    "X_test_final = np.hstack([X_pca_test, X_spatial_test])\n",
    "\n",
    "# === DEFINE MODELS ===\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5),\n",
    "    \"Lasso\": LassoCV(cv=5, random_state=42, max_iter=10000),\n",
    "    \"ElasticNet\": ElasticNetCV(cv=5, random_state=42, max_iter=10000),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.01, random_state=42, n_jobs=-1, verbosity=0)\n",
    "}\n",
    "\n",
    "# === EVALUATE MODELS ON VALIDATION SET ===\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_val_pred = model.predict(X_val_final)\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        results[name] = r2\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# === SAVE COEFFICIENTS AND PERFORMANCE ===\n",
    "coeff_labels = [f'PC{i+1}' for i in range(3)] + spatial_features\n",
    "coef_df = pd.DataFrame({'Feature': coeff_labels, 'Coefficient': best_model.coef_}) if hasattr(best_model, 'coef_') else pd.DataFrame()\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['R-squared', 'MSE'],\n",
    "    'Value': [r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred)]\n",
    "})\n",
    "coef_df.to_csv(f\"{plot_dir}/best_model_coefficients.csv\", index=False)\n",
    "metrics_df.to_csv(f\"{plot_dir}/best_model_metrics.csv\", index=False)\n",
    "\n",
    "# === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === REPORT ON SPLITS ===\n",
    "y_train_pred = best_model.predict(X_train_final)\n",
    "y_val_pred = best_model.predict(X_val_final)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"R-squared\": [\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_val, y_val_pred),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ],\n",
    "    \"MSE\": [\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_val, y_val_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)\n",
    "    ]\n",
    "})\n",
    "\n",
    "performance.to_csv(f\"{plot_dir}/model_performance_by_split.csv\", index=False)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(performance)\n",
    "\n",
    "\n",
    "# Generate the code block for PCA + Clustering using validation data and applying to test data\n",
    "\n",
    "\n",
    "# === PCA + KMeans CLUSTERING (Fit on Validation, Apply to Test) ===\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "output_directory = plot_dir\n",
    "window_size = 5\n",
    "\n",
    "# --- Fit PCA on Validation Set ---\n",
    "X_scaled_val = X_col_val_scaled\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled_val)\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# --- Plot PCA Cumulative Variance (Elbow Plot) ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='s', linestyle='-', color='red', label=\"Cumulative Variance\")\n",
    "plt.axvline(x=optimal_components, color='green', linestyle='--', label=f\"Optimal Components ({optimal_components})\")\n",
    "plt.scatter(optimal_components, cumulative_variance[optimal_components-1], color='black', s=50, label=\"Chosen Point\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Elbow Plot with Cumulative Variance\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_Elbow_Plot_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Fit 2D PCA on Validation ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_val_2d = pca_2d.fit_transform(X_scaled_val)\n",
    "\n",
    "# --- KMeans on 2D PCA space (Validation) ---\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 15)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "    score = silhouette_score(pca_val_2d, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "optimal_score = max(silhouette_scores)\n",
    "\n",
    "# --- Plot Silhouette Scores ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='-', color='purple', label=\"Silhouette Score\")\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', label=f\"Optimal k = {optimal_k}\")\n",
    "plt.scatter(optimal_k, optimal_score, color='red', s=100, zorder=5)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs Number of Clusters (Validation)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Silhouette_Score_vs_K_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Apply to Test Data ---\n",
    "X_scaled_test = X_col_test_scaled\n",
    "pca_test_2d = pca_2d.transform(X_scaled_test)\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "test_clusters = kmeans_final.predict(pca_test_2d)\n",
    "\n",
    "# --- Create Test DataFrame with PCA + Clusters ---\n",
    "pca_test_df = pd.DataFrame(pca_test_2d, columns=[\"PCA1\", \"PCA2\"])\n",
    "pca_test_df[\"X_Centroid_Velocity_MA\"] = X_col_test[:, 0]  # Area_MA is first collinear feature\n",
    "pca_test_df[\"Cluster\"] = test_clusters\n",
    "\n",
    "# --- Clustered 2D PCA (Colored by Area) ---\n",
    "vmin = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 5)\n",
    "vmax = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 95)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sc = plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"X_Centroid_Velocity_MA\"], cmap=\"plasma\", alpha=0.8, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(sc, label=\"X_Centroid Velocity (MA)\")\n",
    "plt.title(\"2D PCA Test Data Colored by X_Centroid Velocity(MA)\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Cluster Plot with Centers ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"Cluster\"], cmap=\"tab10\", alpha=0.8)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, marker='o', label='Centers')\n",
    "for idx, (x, y) in enumerate(centers):\n",
    "    plt.text(x, y, str(idx), fontsize=12, fontweight='bold', ha='center', va='center', color='white',\n",
    "             bbox=dict(facecolor='black', boxstyle='circle,pad=0.2'))\n",
    "plt.title(f\"Test Data PCA with KMeans Clusters (k={optimal_k})\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_Clusters_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Boxplot: Area by Cluster (Test Data) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n",
    "sns.stripplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", color='gray', alpha=0.5, jitter=0.2, size=3)\n",
    "plt.title(\"X_Centroid Velocity (MA) by Cluster (Test Data)\", fontsize=14)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"X_Centroid Velocity (MA)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Boxplot_X_Centroid_Velocity_MA_By_Cluster_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Save clustered PCA test data and summary ---\n",
    "pca_test_df.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Results_Test_{window_size}.csv\"), index=False)\n",
    "\n",
    "summary = pca_test_df.groupby(\"Cluster\").agg({\n",
    "    \"PCA1\": [\"mean\", \"std\"],\n",
    "    \"PCA2\": [\"mean\", \"std\"],\n",
    "    \"X_Centroid_Velocity_MA\": [\"mean\", \"std\"]\n",
    "}).reset_index()\n",
    "summary.columns = [\"_\".join(col).strip(\"_\") for col in summary.columns.values]\n",
    "summary.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Summary_Test_{window_size}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae6cb0",
   "metadata": {},
   "source": [
    "# Create the updated FNN pipeline script content with PCA + spatial features + grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d207188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Training: lr=0.001, batch=8, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=60.7583\n",
      "Epoch 20: Loss=48.0488\n",
      "Epoch 30: Loss=43.0716\n",
      "Epoch 40: Loss=40.0179\n",
      "Epoch 50: Loss=34.7239\n",
      "Epoch 60: Loss=31.8976\n",
      "Epoch 70: Loss=28.8628\n",
      "Epoch 80: Loss=29.4072\n",
      "Epoch 90: Loss=25.7802\n",
      "Epoch 100: Loss=23.8316\n",
      "✅ MSE: 48.3614 | R²: 0.6188\n",
      "▶ Training: lr=0.001, batch=8, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=77.9203\n",
      "Epoch 20: Loss=50.3887\n",
      "Epoch 30: Loss=55.3319\n",
      "Epoch 40: Loss=40.0342\n",
      "Epoch 50: Loss=36.6373\n",
      "Epoch 60: Loss=38.3620\n",
      "Epoch 70: Loss=28.9798\n",
      "Epoch 80: Loss=28.3483\n",
      "Epoch 90: Loss=27.2999\n",
      "Epoch 100: Loss=25.3773\n",
      "Epoch 110: Loss=25.6427\n",
      "Epoch 120: Loss=24.8958\n",
      "Epoch 130: Loss=21.6184\n",
      "Epoch 140: Loss=20.2393\n",
      "Epoch 150: Loss=19.5229\n",
      "Epoch 160: Loss=18.1424\n",
      "Epoch 170: Loss=16.8603\n",
      "Epoch 180: Loss=19.0569\n",
      "Epoch 190: Loss=17.1586\n",
      "Epoch 200: Loss=15.4078\n",
      "✅ MSE: 60.9424 | R²: 0.5196\n",
      "▶ Training: lr=0.001, batch=8, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=57.9969\n",
      "Epoch 20: Loss=47.9169\n",
      "Epoch 30: Loss=44.9921\n",
      "Epoch 40: Loss=41.2015\n",
      "Epoch 50: Loss=34.7194\n",
      "Epoch 60: Loss=32.6913\n",
      "Epoch 70: Loss=30.4407\n",
      "Epoch 80: Loss=30.9892\n",
      "Epoch 90: Loss=26.7047\n",
      "Epoch 100: Loss=28.9909\n",
      "Epoch 110: Loss=23.8369\n",
      "Epoch 120: Loss=23.6632\n",
      "Epoch 130: Loss=21.0244\n",
      "Epoch 140: Loss=21.8901\n",
      "Epoch 150: Loss=22.3821\n",
      "Epoch 160: Loss=19.4408\n",
      "Epoch 170: Loss=18.9642\n",
      "Epoch 180: Loss=17.8694\n",
      "Epoch 190: Loss=15.8159\n",
      "Epoch 200: Loss=16.9001\n",
      "Epoch 210: Loss=16.0217\n",
      "Epoch 220: Loss=18.1583\n",
      "Epoch 230: Loss=13.4853\n",
      "Epoch 240: Loss=14.9718\n",
      "Epoch 250: Loss=15.1624\n",
      "Epoch 260: Loss=13.0673\n",
      "Epoch 270: Loss=15.0017\n",
      "Epoch 280: Loss=10.8205\n",
      "Epoch 290: Loss=11.3939\n",
      "Epoch 300: Loss=11.5640\n",
      "✅ MSE: 56.9419 | R²: 0.5512\n",
      "▶ Training: lr=0.001, batch=8, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=130.6171\n",
      "Epoch 20: Loss=56.9227\n",
      "Epoch 30: Loss=47.2202\n",
      "Epoch 40: Loss=42.9412\n",
      "Epoch 50: Loss=40.2662\n",
      "Epoch 60: Loss=37.2912\n",
      "Epoch 70: Loss=36.3008\n",
      "Epoch 80: Loss=35.4798\n",
      "Epoch 90: Loss=33.5271\n",
      "Epoch 100: Loss=32.6731\n",
      "✅ MSE: 59.5709 | R²: 0.5304\n",
      "▶ Training: lr=0.001, batch=8, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=111.9273\n",
      "Epoch 20: Loss=55.4834\n",
      "Epoch 30: Loss=47.2556\n",
      "Epoch 40: Loss=42.2475\n",
      "Epoch 50: Loss=38.8381\n",
      "Epoch 60: Loss=38.0661\n",
      "Epoch 70: Loss=38.4059\n",
      "Epoch 80: Loss=34.6006\n",
      "Epoch 90: Loss=34.2924\n",
      "Epoch 100: Loss=31.7714\n",
      "Epoch 110: Loss=30.9420\n",
      "Epoch 120: Loss=30.1843\n",
      "Epoch 130: Loss=27.9511\n",
      "Epoch 140: Loss=25.6118\n",
      "Epoch 150: Loss=24.8970\n",
      "Epoch 160: Loss=24.2381\n",
      "Epoch 170: Loss=22.0197\n",
      "Epoch 180: Loss=21.6110\n",
      "Epoch 190: Loss=20.1653\n",
      "Epoch 200: Loss=20.5519\n",
      "✅ MSE: 54.3406 | R²: 0.5717\n",
      "▶ Training: lr=0.001, batch=8, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=104.6327\n",
      "Epoch 20: Loss=52.4119\n",
      "Epoch 30: Loss=47.8562\n",
      "Epoch 40: Loss=41.9184\n",
      "Epoch 50: Loss=40.3117\n",
      "Epoch 60: Loss=38.9232\n",
      "Epoch 70: Loss=35.2902\n",
      "Epoch 80: Loss=36.0324\n",
      "Epoch 90: Loss=34.8118\n",
      "Epoch 100: Loss=34.2977\n",
      "Epoch 110: Loss=30.6732\n",
      "Epoch 120: Loss=29.8899\n",
      "Epoch 130: Loss=28.9419\n",
      "Epoch 140: Loss=28.3758\n",
      "Epoch 150: Loss=28.0149\n",
      "Epoch 160: Loss=27.2195\n",
      "Epoch 170: Loss=24.6565\n",
      "Epoch 180: Loss=25.1138\n",
      "Epoch 190: Loss=22.9851\n",
      "Epoch 200: Loss=22.8771\n",
      "Epoch 210: Loss=23.0485\n",
      "Epoch 220: Loss=20.3053\n",
      "Epoch 230: Loss=23.1499\n",
      "Epoch 240: Loss=20.4101\n",
      "Epoch 250: Loss=20.3008\n",
      "Epoch 260: Loss=20.4014\n",
      "Epoch 270: Loss=19.9545\n",
      "Epoch 280: Loss=19.0085\n",
      "Epoch 290: Loss=18.5946\n",
      "Epoch 300: Loss=18.4048\n",
      "✅ MSE: 70.3807 | R²: 0.4452\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=134.7275\n",
      "Epoch 20: Loss=86.4976\n",
      "Epoch 30: Loss=54.0315\n",
      "Epoch 40: Loss=46.2673\n",
      "Epoch 50: Loss=43.1532\n",
      "Epoch 60: Loss=39.5473\n",
      "Epoch 70: Loss=40.6775\n",
      "Epoch 80: Loss=37.4044\n",
      "Epoch 90: Loss=36.3938\n",
      "Epoch 100: Loss=34.9967\n",
      "✅ MSE: 54.7554 | R²: 0.5684\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=129.6088\n",
      "Epoch 20: Loss=69.8622\n",
      "Epoch 30: Loss=50.0632\n",
      "Epoch 40: Loss=44.4779\n",
      "Epoch 50: Loss=44.6170\n",
      "Epoch 60: Loss=41.4559\n",
      "Epoch 70: Loss=39.6669\n",
      "Epoch 80: Loss=38.1325\n",
      "Epoch 90: Loss=37.8205\n",
      "Epoch 100: Loss=36.1709\n",
      "Epoch 110: Loss=36.8347\n",
      "Epoch 120: Loss=35.5675\n",
      "Epoch 130: Loss=33.8169\n",
      "Epoch 140: Loss=31.7285\n",
      "Epoch 150: Loss=30.6411\n",
      "Epoch 160: Loss=31.4290\n",
      "Epoch 170: Loss=30.5516\n",
      "Epoch 180: Loss=29.0197\n",
      "Epoch 190: Loss=29.7665\n",
      "Epoch 200: Loss=26.9928\n",
      "✅ MSE: 53.5461 | R²: 0.5779\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=134.6771\n",
      "Epoch 20: Loss=87.3892\n",
      "Epoch 30: Loss=54.6875\n",
      "Epoch 40: Loss=46.5188\n",
      "Epoch 50: Loss=42.2573\n",
      "Epoch 60: Loss=40.2410\n",
      "Epoch 70: Loss=37.9749\n",
      "Epoch 80: Loss=37.3931\n",
      "Epoch 90: Loss=36.8426\n",
      "Epoch 100: Loss=34.9795\n",
      "Epoch 110: Loss=35.0520\n",
      "Epoch 120: Loss=34.3088\n",
      "Epoch 130: Loss=33.2539\n",
      "Epoch 140: Loss=31.8753\n",
      "Epoch 150: Loss=32.8812\n",
      "Epoch 160: Loss=32.2580\n",
      "Epoch 170: Loss=29.7518\n",
      "Epoch 180: Loss=30.0415\n",
      "Epoch 190: Loss=29.1600\n",
      "Epoch 200: Loss=29.0392\n",
      "Epoch 210: Loss=26.6121\n",
      "Epoch 220: Loss=26.2511\n",
      "Epoch 230: Loss=25.8588\n",
      "Epoch 240: Loss=26.0169\n",
      "Epoch 250: Loss=24.9905\n",
      "Epoch 260: Loss=25.5511\n",
      "Epoch 270: Loss=24.1650\n",
      "Epoch 280: Loss=24.0220\n",
      "Epoch 290: Loss=25.0431\n",
      "Epoch 300: Loss=23.5319\n",
      "✅ MSE: 53.9482 | R²: 0.5748\n",
      "▶ Training: lr=0.001, batch=8, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=139.1897\n",
      "Epoch 20: Loss=118.9845\n",
      "Epoch 30: Loss=96.4312\n",
      "Epoch 40: Loss=73.6509\n",
      "Epoch 50: Loss=59.2625\n",
      "Epoch 60: Loss=48.8950\n",
      "Epoch 70: Loss=45.1685\n",
      "Epoch 80: Loss=42.0114\n",
      "Epoch 90: Loss=40.9958\n",
      "Epoch 100: Loss=38.4372\n",
      "✅ MSE: 56.2155 | R²: 0.5569\n",
      "▶ Training: lr=0.001, batch=8, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=140.7222\n",
      "Epoch 20: Loss=123.3526\n",
      "Epoch 30: Loss=96.1943\n",
      "Epoch 40: Loss=71.1367\n",
      "Epoch 50: Loss=53.7644\n",
      "Epoch 60: Loss=47.8010\n",
      "Epoch 70: Loss=42.7430\n",
      "Epoch 80: Loss=41.5585\n",
      "Epoch 90: Loss=41.2272\n",
      "Epoch 100: Loss=38.6298\n",
      "Epoch 110: Loss=38.8087\n",
      "Epoch 120: Loss=37.4832\n",
      "Epoch 130: Loss=35.8129\n",
      "Epoch 140: Loss=36.0784\n",
      "Epoch 150: Loss=35.8556\n",
      "Epoch 160: Loss=35.8434\n",
      "Epoch 170: Loss=35.3287\n",
      "Epoch 180: Loss=34.9245\n",
      "Epoch 190: Loss=34.2708\n",
      "Epoch 200: Loss=32.5419\n",
      "✅ MSE: 54.6092 | R²: 0.5696\n",
      "▶ Training: lr=0.001, batch=8, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=140.1997\n",
      "Epoch 20: Loss=116.2919\n",
      "Epoch 30: Loss=75.0455\n",
      "Epoch 40: Loss=52.6578\n",
      "Epoch 50: Loss=48.1508\n",
      "Epoch 60: Loss=45.1761\n",
      "Epoch 70: Loss=45.0553\n",
      "Epoch 80: Loss=43.2120\n",
      "Epoch 90: Loss=42.6976\n",
      "Epoch 100: Loss=42.9124\n",
      "Epoch 110: Loss=43.1288\n",
      "Epoch 120: Loss=42.1085\n",
      "Epoch 130: Loss=41.0745\n",
      "Epoch 140: Loss=40.2091\n",
      "Epoch 150: Loss=40.9896\n",
      "Epoch 160: Loss=39.6846\n",
      "Epoch 170: Loss=38.9817\n",
      "Epoch 180: Loss=39.4809\n",
      "Epoch 190: Loss=38.5542\n",
      "Epoch 200: Loss=38.2427\n",
      "Epoch 210: Loss=37.5817\n",
      "Epoch 220: Loss=36.8031\n",
      "Epoch 230: Loss=37.1098\n",
      "Epoch 240: Loss=35.4276\n",
      "Epoch 250: Loss=36.4351\n",
      "Epoch 260: Loss=34.9367\n",
      "Epoch 270: Loss=34.4606\n",
      "Epoch 280: Loss=33.6940\n",
      "Epoch 290: Loss=34.1820\n",
      "Epoch 300: Loss=32.7229\n",
      "✅ MSE: 57.7326 | R²: 0.5449\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=137.1289\n",
      "Epoch 20: Loss=80.3464\n",
      "Epoch 30: Loss=53.1592\n",
      "Epoch 40: Loss=51.3259\n",
      "Epoch 50: Loss=46.6493\n",
      "Epoch 60: Loss=45.9025\n",
      "Epoch 70: Loss=43.1254\n",
      "Epoch 80: Loss=42.2669\n",
      "Epoch 90: Loss=39.6301\n",
      "Epoch 100: Loss=38.9056\n",
      "✅ MSE: 61.4479 | R²: 0.5157\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=142.7529\n",
      "Epoch 20: Loss=86.0828\n",
      "Epoch 30: Loss=50.6480\n",
      "Epoch 40: Loss=42.4330\n",
      "Epoch 50: Loss=46.4774\n",
      "Epoch 60: Loss=39.1512\n",
      "Epoch 70: Loss=38.1705\n",
      "Epoch 80: Loss=36.4370\n",
      "Epoch 90: Loss=36.1194\n",
      "Epoch 100: Loss=34.4062\n",
      "Epoch 110: Loss=33.5693\n",
      "Epoch 120: Loss=34.8294\n",
      "Epoch 130: Loss=30.8384\n",
      "Epoch 140: Loss=29.8745\n",
      "Epoch 150: Loss=28.9666\n",
      "Epoch 160: Loss=29.0753\n",
      "Epoch 170: Loss=27.7126\n",
      "Epoch 180: Loss=27.3980\n",
      "Epoch 190: Loss=26.8215\n",
      "Epoch 200: Loss=26.1246\n",
      "✅ MSE: 54.3131 | R²: 0.5719\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=138.5389\n",
      "Epoch 20: Loss=93.9810\n",
      "Epoch 30: Loss=55.4385\n",
      "Epoch 40: Loss=45.2309\n",
      "Epoch 50: Loss=42.5773\n",
      "Epoch 60: Loss=40.2054\n",
      "Epoch 70: Loss=38.2358\n",
      "Epoch 80: Loss=36.1551\n",
      "Epoch 90: Loss=33.8797\n",
      "Epoch 100: Loss=33.0245\n",
      "Epoch 110: Loss=31.8442\n",
      "Epoch 120: Loss=29.7158\n",
      "Epoch 130: Loss=28.7756\n",
      "Epoch 140: Loss=27.6301\n",
      "Epoch 150: Loss=26.0816\n",
      "Epoch 160: Loss=25.5708\n",
      "Epoch 170: Loss=24.9812\n",
      "Epoch 180: Loss=24.6032\n",
      "Epoch 190: Loss=23.4386\n",
      "Epoch 200: Loss=22.0323\n",
      "Epoch 210: Loss=23.4712\n",
      "Epoch 220: Loss=21.8521\n",
      "Epoch 230: Loss=24.3042\n",
      "Epoch 240: Loss=20.6675\n",
      "Epoch 250: Loss=20.2876\n",
      "Epoch 260: Loss=21.0171\n",
      "Epoch 270: Loss=19.7087\n",
      "Epoch 280: Loss=17.8240\n",
      "Epoch 290: Loss=20.6228\n",
      "Epoch 300: Loss=19.6587\n",
      "✅ MSE: 51.3945 | R²: 0.5949\n",
      "▶ Training: lr=0.001, batch=16, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=116.8169\n",
      "Epoch 20: Loss=53.7531\n",
      "Epoch 30: Loss=43.2471\n",
      "Epoch 40: Loss=40.9830\n",
      "Epoch 50: Loss=37.4192\n",
      "Epoch 60: Loss=36.7732\n",
      "Epoch 70: Loss=34.6761\n",
      "Epoch 80: Loss=32.5147\n",
      "Epoch 90: Loss=28.1050\n",
      "Epoch 100: Loss=27.0137\n",
      "✅ MSE: 47.4587 | R²: 0.6259\n",
      "▶ Training: lr=0.001, batch=16, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=76.7509\n",
      "Epoch 20: Loss=54.2249\n",
      "Epoch 30: Loss=47.8899\n",
      "Epoch 40: Loss=38.3926\n",
      "Epoch 50: Loss=42.7490\n",
      "Epoch 60: Loss=35.0444\n",
      "Epoch 70: Loss=34.4466\n",
      "Epoch 80: Loss=29.7326\n",
      "Epoch 90: Loss=28.3967\n",
      "Epoch 100: Loss=30.1811\n",
      "Epoch 110: Loss=24.9839\n",
      "Epoch 120: Loss=25.8602\n",
      "Epoch 130: Loss=24.5214\n",
      "Epoch 140: Loss=21.5809\n",
      "Epoch 150: Loss=23.1392\n",
      "Epoch 160: Loss=20.3969\n",
      "Epoch 170: Loss=20.0850\n",
      "Epoch 180: Loss=19.6072\n",
      "Epoch 190: Loss=20.7446\n",
      "Epoch 200: Loss=16.6427\n",
      "✅ MSE: 53.3163 | R²: 0.5797\n",
      "▶ Training: lr=0.001, batch=16, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=107.9564\n",
      "Epoch 20: Loss=52.0868\n",
      "Epoch 30: Loss=45.6295\n",
      "Epoch 40: Loss=43.4079\n",
      "Epoch 50: Loss=40.2879\n",
      "Epoch 60: Loss=42.4152\n",
      "Epoch 70: Loss=35.9616\n",
      "Epoch 80: Loss=34.8265\n",
      "Epoch 90: Loss=29.9971\n",
      "Epoch 100: Loss=29.1493\n",
      "Epoch 110: Loss=26.7246\n",
      "Epoch 120: Loss=27.0127\n",
      "Epoch 130: Loss=25.3943\n",
      "Epoch 140: Loss=23.1061\n",
      "Epoch 150: Loss=21.8123\n",
      "Epoch 160: Loss=22.7641\n",
      "Epoch 170: Loss=20.4925\n",
      "Epoch 180: Loss=19.3168\n",
      "Epoch 190: Loss=18.2226\n",
      "Epoch 200: Loss=19.3406\n",
      "Epoch 210: Loss=20.0079\n",
      "Epoch 220: Loss=16.1171\n",
      "Epoch 230: Loss=16.0824\n",
      "Epoch 240: Loss=15.4155\n",
      "Epoch 250: Loss=15.1458\n",
      "Epoch 260: Loss=15.3918\n",
      "Epoch 270: Loss=14.1558\n",
      "Epoch 280: Loss=13.9923\n",
      "Epoch 290: Loss=13.3467\n",
      "Epoch 300: Loss=13.6842\n",
      "✅ MSE: 48.7251 | R²: 0.6159\n",
      "▶ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=132.5029\n",
      "Epoch 20: Loss=86.1069\n",
      "Epoch 30: Loss=53.5276\n",
      "Epoch 40: Loss=41.2368\n",
      "Epoch 50: Loss=40.9786\n",
      "Epoch 60: Loss=39.9231\n",
      "Epoch 70: Loss=34.2543\n",
      "Epoch 80: Loss=35.6014\n",
      "Epoch 90: Loss=31.9847\n",
      "Epoch 100: Loss=30.9471\n",
      "✅ MSE: 58.2948 | R²: 0.5405\n",
      "▶ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=120.6227\n",
      "Epoch 20: Loss=65.1204\n",
      "Epoch 30: Loss=48.0497\n",
      "Epoch 40: Loss=46.1017\n",
      "Epoch 50: Loss=43.0278\n",
      "Epoch 60: Loss=35.7711\n",
      "Epoch 70: Loss=35.6236\n",
      "Epoch 80: Loss=35.1924\n",
      "Epoch 90: Loss=33.8267\n",
      "Epoch 100: Loss=32.2322\n",
      "Epoch 110: Loss=30.2978\n",
      "Epoch 120: Loss=28.6493\n",
      "Epoch 130: Loss=29.1618\n",
      "Epoch 140: Loss=26.6908\n",
      "Epoch 150: Loss=25.7863\n",
      "Epoch 160: Loss=25.3806\n",
      "Epoch 170: Loss=23.9160\n",
      "Epoch 180: Loss=22.3670\n",
      "Epoch 190: Loss=24.3237\n",
      "Epoch 200: Loss=20.8507\n",
      "✅ MSE: 48.4141 | R²: 0.6184\n",
      "▶ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=122.1354\n",
      "Epoch 20: Loss=51.9703\n",
      "Epoch 30: Loss=44.4025\n",
      "Epoch 40: Loss=42.0854\n",
      "Epoch 50: Loss=37.8275\n",
      "Epoch 60: Loss=36.3952\n",
      "Epoch 70: Loss=34.4655\n",
      "Epoch 80: Loss=34.7496\n",
      "Epoch 90: Loss=32.4181\n",
      "Epoch 100: Loss=33.3454\n",
      "Epoch 110: Loss=31.7570\n",
      "Epoch 120: Loss=30.3983\n",
      "Epoch 130: Loss=29.8210\n",
      "Epoch 140: Loss=29.8301\n",
      "Epoch 150: Loss=27.4438\n",
      "Epoch 160: Loss=28.3380\n",
      "Epoch 170: Loss=26.0510\n",
      "Epoch 180: Loss=26.3197\n",
      "Epoch 190: Loss=25.1248\n",
      "Epoch 200: Loss=25.6749\n",
      "Epoch 210: Loss=25.4523\n",
      "Epoch 220: Loss=25.3726\n",
      "Epoch 230: Loss=23.3493\n",
      "Epoch 240: Loss=22.9827\n",
      "Epoch 250: Loss=22.4601\n",
      "Epoch 260: Loss=21.6598\n",
      "Epoch 270: Loss=23.2729\n",
      "Epoch 280: Loss=20.7677\n",
      "Epoch 290: Loss=19.8467\n",
      "Epoch 300: Loss=21.5739\n",
      "✅ MSE: 54.7944 | R²: 0.5681\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=131.9637\n",
      "Epoch 20: Loss=93.1899\n",
      "Epoch 30: Loss=62.7034\n",
      "Epoch 40: Loss=49.9616\n",
      "Epoch 50: Loss=47.1969\n",
      "Epoch 60: Loss=45.5936\n",
      "Epoch 70: Loss=45.5388\n",
      "Epoch 80: Loss=44.0403\n",
      "Epoch 90: Loss=43.5529\n",
      "Epoch 100: Loss=42.8138\n",
      "✅ MSE: 62.1840 | R²: 0.5098\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=136.5527\n",
      "Epoch 20: Loss=100.1408\n",
      "Epoch 30: Loss=56.8969\n",
      "Epoch 40: Loss=44.9473\n",
      "Epoch 50: Loss=40.9219\n",
      "Epoch 60: Loss=39.0760\n",
      "Epoch 70: Loss=36.7994\n",
      "Epoch 80: Loss=36.8382\n",
      "Epoch 90: Loss=36.1680\n",
      "Epoch 100: Loss=34.6721\n",
      "Epoch 110: Loss=32.9276\n",
      "Epoch 120: Loss=31.6954\n",
      "Epoch 130: Loss=31.6956\n",
      "Epoch 140: Loss=32.8073\n",
      "Epoch 150: Loss=31.6632\n",
      "Epoch 160: Loss=29.3881\n",
      "Epoch 170: Loss=28.8635\n",
      "Epoch 180: Loss=29.2019\n",
      "Epoch 190: Loss=28.2529\n",
      "Epoch 200: Loss=27.7813\n",
      "✅ MSE: 54.7591 | R²: 0.5684\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=137.6182\n",
      "Epoch 20: Loss=89.6657\n",
      "Epoch 30: Loss=59.5925\n",
      "Epoch 40: Loss=45.1218\n",
      "Epoch 50: Loss=42.4194\n",
      "Epoch 60: Loss=40.2875\n",
      "Epoch 70: Loss=37.6826\n",
      "Epoch 80: Loss=36.5876\n",
      "Epoch 90: Loss=40.3834\n",
      "Epoch 100: Loss=34.9281\n",
      "Epoch 110: Loss=34.1760\n",
      "Epoch 120: Loss=33.5912\n",
      "Epoch 130: Loss=35.0614\n",
      "Epoch 140: Loss=33.0130\n",
      "Epoch 150: Loss=32.4971\n",
      "Epoch 160: Loss=31.6070\n",
      "Epoch 170: Loss=30.5765\n",
      "Epoch 180: Loss=34.0503\n",
      "Epoch 190: Loss=30.7752\n",
      "Epoch 200: Loss=29.7019\n",
      "Epoch 210: Loss=29.5869\n",
      "Epoch 220: Loss=29.1240\n",
      "Epoch 230: Loss=29.8162\n",
      "Epoch 240: Loss=28.5327\n",
      "Epoch 250: Loss=28.5089\n",
      "Epoch 260: Loss=27.5494\n",
      "Epoch 270: Loss=28.7902\n",
      "Epoch 280: Loss=26.9541\n",
      "Epoch 290: Loss=25.7699\n",
      "Epoch 300: Loss=25.7006\n",
      "✅ MSE: 54.1775 | R²: 0.5730\n",
      "▶ Training: lr=0.001, batch=16, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=203.8232\n",
      "Epoch 20: Loss=128.6096\n",
      "Epoch 30: Loss=108.9503\n",
      "Epoch 40: Loss=91.1707\n",
      "Epoch 50: Loss=76.1718\n",
      "Epoch 60: Loss=64.4696\n",
      "Epoch 70: Loss=52.7496\n",
      "Epoch 80: Loss=49.9320\n",
      "Epoch 90: Loss=44.3007\n",
      "Epoch 100: Loss=41.8555\n",
      "✅ MSE: 58.9617 | R²: 0.5352\n",
      "▶ Training: lr=0.001, batch=16, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=139.6947\n",
      "Epoch 20: Loss=128.1184\n",
      "Epoch 30: Loss=107.2210\n",
      "Epoch 40: Loss=88.5410\n",
      "Epoch 50: Loss=70.2025\n",
      "Epoch 60: Loss=58.3858\n",
      "Epoch 70: Loss=50.0637\n",
      "Epoch 80: Loss=47.6671\n",
      "Epoch 90: Loss=44.8319\n",
      "Epoch 100: Loss=44.7793\n",
      "Epoch 110: Loss=43.3838\n",
      "Epoch 120: Loss=42.1853\n",
      "Epoch 130: Loss=42.6065\n",
      "Epoch 140: Loss=43.2274\n",
      "Epoch 150: Loss=40.5952\n",
      "Epoch 160: Loss=40.7193\n",
      "Epoch 170: Loss=38.3914\n",
      "Epoch 180: Loss=37.8534\n",
      "Epoch 190: Loss=36.5434\n",
      "Epoch 200: Loss=35.9555\n",
      "✅ MSE: 53.9512 | R²: 0.5747\n",
      "▶ Training: lr=0.001, batch=16, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=137.9532\n",
      "Epoch 20: Loss=180.9707\n",
      "Epoch 30: Loss=102.1165\n",
      "Epoch 40: Loss=83.4312\n",
      "Epoch 50: Loss=67.6587\n",
      "Epoch 60: Loss=56.6047\n",
      "Epoch 70: Loss=50.1023\n",
      "Epoch 80: Loss=47.3099\n",
      "Epoch 90: Loss=46.4442\n",
      "Epoch 100: Loss=43.7067\n",
      "Epoch 110: Loss=43.8683\n",
      "Epoch 120: Loss=41.8907\n",
      "Epoch 130: Loss=40.8770\n",
      "Epoch 140: Loss=41.2102\n",
      "Epoch 150: Loss=42.3045\n",
      "Epoch 160: Loss=39.5486\n",
      "Epoch 170: Loss=38.3779\n",
      "Epoch 180: Loss=39.3268\n",
      "Epoch 190: Loss=37.0773\n",
      "Epoch 200: Loss=36.4990\n",
      "Epoch 210: Loss=36.7141\n",
      "Epoch 220: Loss=36.2626\n",
      "Epoch 230: Loss=35.2896\n",
      "Epoch 240: Loss=34.7270\n",
      "Epoch 250: Loss=38.7221\n",
      "Epoch 260: Loss=37.6668\n",
      "Epoch 270: Loss=33.1558\n",
      "Epoch 280: Loss=33.1236\n",
      "Epoch 290: Loss=32.9976\n",
      "Epoch 300: Loss=32.4541\n",
      "✅ MSE: 50.7362 | R²: 0.6001\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=139.5710\n",
      "Epoch 20: Loss=96.0291\n",
      "Epoch 30: Loss=55.3023\n",
      "Epoch 40: Loss=47.9967\n",
      "Epoch 50: Loss=48.1204\n",
      "Epoch 60: Loss=50.2791\n",
      "Epoch 70: Loss=48.1958\n",
      "Epoch 80: Loss=48.3156\n",
      "Epoch 90: Loss=44.8153\n",
      "Epoch 100: Loss=45.5973\n",
      "✅ MSE: 64.5321 | R²: 0.4913\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=132.2992\n",
      "Epoch 20: Loss=61.5296\n",
      "Epoch 30: Loss=43.0959\n",
      "Epoch 40: Loss=40.4100\n",
      "Epoch 50: Loss=40.2695\n",
      "Epoch 60: Loss=41.2353\n",
      "Epoch 70: Loss=39.4043\n",
      "Epoch 80: Loss=39.4055\n",
      "Epoch 90: Loss=37.5586\n",
      "Epoch 100: Loss=37.6063\n",
      "Epoch 110: Loss=36.7652\n",
      "Epoch 120: Loss=36.4029\n",
      "Epoch 130: Loss=38.0482\n",
      "Epoch 140: Loss=35.3732\n",
      "Epoch 150: Loss=35.6799\n",
      "Epoch 160: Loss=36.4947\n",
      "Epoch 170: Loss=36.7209\n",
      "Epoch 180: Loss=34.3367\n",
      "Epoch 190: Loss=32.7834\n",
      "Epoch 200: Loss=32.8615\n",
      "✅ MSE: 61.4053 | R²: 0.5160\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=138.1005\n",
      "Epoch 20: Loss=103.3223\n",
      "Epoch 30: Loss=60.1307\n",
      "Epoch 40: Loss=46.2281\n",
      "Epoch 50: Loss=43.3546\n",
      "Epoch 60: Loss=40.6398\n",
      "Epoch 70: Loss=40.1200\n",
      "Epoch 80: Loss=37.7924\n",
      "Epoch 90: Loss=41.3011\n",
      "Epoch 100: Loss=36.5386\n",
      "Epoch 110: Loss=36.3502\n",
      "Epoch 120: Loss=34.8693\n",
      "Epoch 130: Loss=35.6080\n",
      "Epoch 140: Loss=35.6317\n",
      "Epoch 150: Loss=35.6890\n",
      "Epoch 160: Loss=33.1030\n",
      "Epoch 170: Loss=32.6963\n",
      "Epoch 180: Loss=32.2065\n",
      "Epoch 190: Loss=33.4343\n",
      "Epoch 200: Loss=30.4489\n",
      "Epoch 210: Loss=30.7347\n",
      "Epoch 220: Loss=31.6324\n",
      "Epoch 230: Loss=29.0982\n",
      "Epoch 240: Loss=28.9590\n",
      "Epoch 250: Loss=29.6736\n",
      "Epoch 260: Loss=30.6887\n",
      "Epoch 270: Loss=27.2546\n",
      "Epoch 280: Loss=29.9131\n",
      "Epoch 290: Loss=27.1583\n",
      "Epoch 300: Loss=26.3526\n",
      "✅ MSE: 51.3819 | R²: 0.5950\n",
      "▶ Training: lr=0.001, batch=32, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=116.6936\n",
      "Epoch 20: Loss=48.7270\n",
      "Epoch 30: Loss=43.8550\n",
      "Epoch 40: Loss=39.1508\n",
      "Epoch 50: Loss=38.9315\n",
      "Epoch 60: Loss=35.1993\n",
      "Epoch 70: Loss=32.6689\n",
      "Epoch 80: Loss=31.1662\n",
      "Epoch 90: Loss=30.2088\n",
      "Epoch 100: Loss=28.6435\n",
      "✅ MSE: 52.3585 | R²: 0.5873\n",
      "▶ Training: lr=0.001, batch=32, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=115.7310\n",
      "Epoch 20: Loss=57.4346\n",
      "Epoch 30: Loss=49.2888\n",
      "Epoch 40: Loss=43.6136\n",
      "Epoch 50: Loss=46.1736\n",
      "Epoch 60: Loss=36.2129\n",
      "Epoch 70: Loss=36.9378\n",
      "Epoch 80: Loss=33.6002\n",
      "Epoch 90: Loss=32.8366\n",
      "Epoch 100: Loss=31.0042\n",
      "Epoch 110: Loss=28.9027\n",
      "Epoch 120: Loss=28.0116\n",
      "Epoch 130: Loss=29.9529\n",
      "Epoch 140: Loss=24.7403\n",
      "Epoch 150: Loss=23.6551\n",
      "Epoch 160: Loss=23.2628\n",
      "Epoch 170: Loss=24.5068\n",
      "Epoch 180: Loss=20.1657\n",
      "Epoch 190: Loss=20.0370\n",
      "Epoch 200: Loss=19.6635\n",
      "✅ MSE: 56.3340 | R²: 0.5560\n",
      "▶ Training: lr=0.001, batch=32, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=133.4667\n",
      "Epoch 20: Loss=80.0944\n",
      "Epoch 30: Loss=47.2415\n",
      "Epoch 40: Loss=40.9753\n",
      "Epoch 50: Loss=37.9461\n",
      "Epoch 60: Loss=36.0086\n",
      "Epoch 70: Loss=32.3148\n",
      "Epoch 80: Loss=31.9530\n",
      "Epoch 90: Loss=28.3912\n",
      "Epoch 100: Loss=26.3624\n",
      "Epoch 110: Loss=24.4227\n",
      "Epoch 120: Loss=25.0589\n",
      "Epoch 130: Loss=22.5608\n",
      "Epoch 140: Loss=22.2353\n",
      "Epoch 150: Loss=22.7662\n",
      "Epoch 160: Loss=20.8549\n",
      "Epoch 170: Loss=22.8173\n",
      "Epoch 180: Loss=20.3437\n",
      "Epoch 190: Loss=18.6362\n",
      "Epoch 200: Loss=18.3919\n",
      "Epoch 210: Loss=16.7298\n",
      "Epoch 220: Loss=16.7088\n",
      "Epoch 230: Loss=17.9993\n",
      "Epoch 240: Loss=17.5272\n",
      "Epoch 250: Loss=16.9952\n",
      "Epoch 260: Loss=14.7300\n",
      "Epoch 270: Loss=17.4052\n",
      "Epoch 280: Loss=13.6303\n",
      "Epoch 290: Loss=14.9209\n",
      "Epoch 300: Loss=14.1791\n",
      "✅ MSE: 54.8189 | R²: 0.5679\n",
      "▶ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=135.4983\n",
      "Epoch 20: Loss=100.3692\n",
      "Epoch 30: Loss=59.4259\n",
      "Epoch 40: Loss=46.1790\n",
      "Epoch 50: Loss=43.0861\n",
      "Epoch 60: Loss=42.3594\n",
      "Epoch 70: Loss=39.3678\n",
      "Epoch 80: Loss=38.1647\n",
      "Epoch 90: Loss=36.3564\n",
      "Epoch 100: Loss=35.8802\n",
      "✅ MSE: 55.2085 | R²: 0.5648\n",
      "▶ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=136.4133\n",
      "Epoch 20: Loss=100.7563\n",
      "Epoch 30: Loss=62.9034\n",
      "Epoch 40: Loss=48.8672\n",
      "Epoch 50: Loss=45.3920\n",
      "Epoch 60: Loss=40.5924\n",
      "Epoch 70: Loss=37.3964\n",
      "Epoch 80: Loss=38.4540\n",
      "Epoch 90: Loss=35.3381\n",
      "Epoch 100: Loss=34.6161\n",
      "Epoch 110: Loss=32.0746\n",
      "Epoch 120: Loss=35.1031\n",
      "Epoch 130: Loss=31.0535\n",
      "Epoch 140: Loss=29.9577\n",
      "Epoch 150: Loss=31.5698\n",
      "Epoch 160: Loss=29.6402\n",
      "Epoch 170: Loss=28.3196\n",
      "Epoch 180: Loss=29.3765\n",
      "Epoch 190: Loss=29.0777\n",
      "Epoch 200: Loss=27.3379\n",
      "✅ MSE: 56.6953 | R²: 0.5531\n",
      "▶ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=150.4173\n",
      "Epoch 20: Loss=98.4567\n",
      "Epoch 30: Loss=62.7122\n",
      "Epoch 40: Loss=48.9346\n",
      "Epoch 50: Loss=48.5709\n",
      "Epoch 60: Loss=40.7303\n",
      "Epoch 70: Loss=38.1420\n",
      "Epoch 80: Loss=37.4214\n",
      "Epoch 90: Loss=37.1224\n",
      "Epoch 100: Loss=33.4004\n",
      "Epoch 110: Loss=33.9458\n",
      "Epoch 120: Loss=32.3414\n",
      "Epoch 130: Loss=31.7131\n",
      "Epoch 140: Loss=30.6763\n",
      "Epoch 150: Loss=30.5198\n",
      "Epoch 160: Loss=29.1580\n",
      "Epoch 170: Loss=28.0274\n",
      "Epoch 180: Loss=27.2471\n",
      "Epoch 190: Loss=25.9243\n",
      "Epoch 200: Loss=25.8408\n",
      "Epoch 210: Loss=24.8428\n",
      "Epoch 220: Loss=23.9186\n",
      "Epoch 230: Loss=22.9791\n",
      "Epoch 240: Loss=24.4293\n",
      "Epoch 250: Loss=22.6028\n",
      "Epoch 260: Loss=26.6661\n",
      "Epoch 270: Loss=23.4573\n",
      "Epoch 280: Loss=23.7677\n",
      "Epoch 290: Loss=21.5126\n",
      "Epoch 300: Loss=20.5176\n",
      "✅ MSE: 53.4832 | R²: 0.5784\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=138.9294\n",
      "Epoch 20: Loss=125.4569\n",
      "Epoch 30: Loss=98.1647\n",
      "Epoch 40: Loss=69.4426\n",
      "Epoch 50: Loss=54.6888\n",
      "Epoch 60: Loss=47.6050\n",
      "Epoch 70: Loss=43.1151\n",
      "Epoch 80: Loss=42.5743\n",
      "Epoch 90: Loss=39.4329\n",
      "Epoch 100: Loss=38.7237\n",
      "✅ MSE: 57.3514 | R²: 0.5479\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=138.2162\n",
      "Epoch 20: Loss=123.2358\n",
      "Epoch 30: Loss=95.4928\n",
      "Epoch 40: Loss=69.8386\n",
      "Epoch 50: Loss=53.9840\n",
      "Epoch 60: Loss=48.0166\n",
      "Epoch 70: Loss=45.5545\n",
      "Epoch 80: Loss=42.9997\n",
      "Epoch 90: Loss=40.8197\n",
      "Epoch 100: Loss=39.3174\n",
      "Epoch 110: Loss=37.9284\n",
      "Epoch 120: Loss=37.1198\n",
      "Epoch 130: Loss=37.2498\n",
      "Epoch 140: Loss=34.9430\n",
      "Epoch 150: Loss=35.1200\n",
      "Epoch 160: Loss=34.0970\n",
      "Epoch 170: Loss=33.2312\n",
      "Epoch 180: Loss=33.1558\n",
      "Epoch 190: Loss=34.0477\n",
      "Epoch 200: Loss=31.7053\n",
      "✅ MSE: 62.9251 | R²: 0.5040\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=138.6988\n",
      "Epoch 20: Loss=128.6913\n",
      "Epoch 30: Loss=107.2606\n",
      "Epoch 40: Loss=91.7393\n",
      "Epoch 50: Loss=65.1927\n",
      "Epoch 60: Loss=53.6452\n",
      "Epoch 70: Loss=47.2282\n",
      "Epoch 80: Loss=43.2194\n",
      "Epoch 90: Loss=42.0429\n",
      "Epoch 100: Loss=40.7738\n",
      "Epoch 110: Loss=38.3871\n",
      "Epoch 120: Loss=37.9059\n",
      "Epoch 130: Loss=36.7012\n",
      "Epoch 140: Loss=36.5673\n",
      "Epoch 150: Loss=37.4548\n",
      "Epoch 160: Loss=33.5373\n",
      "Epoch 170: Loss=33.4110\n",
      "Epoch 180: Loss=33.1826\n",
      "Epoch 190: Loss=32.5452\n",
      "Epoch 200: Loss=30.1738\n",
      "Epoch 210: Loss=32.2228\n",
      "Epoch 220: Loss=30.7021\n",
      "Epoch 230: Loss=29.9281\n",
      "Epoch 240: Loss=29.3907\n",
      "Epoch 250: Loss=28.5771\n",
      "Epoch 260: Loss=28.3131\n",
      "Epoch 270: Loss=28.4892\n",
      "Epoch 280: Loss=27.2285\n",
      "Epoch 290: Loss=28.5161\n",
      "Epoch 300: Loss=26.1707\n",
      "✅ MSE: 61.4780 | R²: 0.5154\n",
      "▶ Training: lr=0.001, batch=32, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=140.2271\n",
      "Epoch 20: Loss=137.1186\n",
      "Epoch 30: Loss=129.1155\n",
      "Epoch 40: Loss=113.3324\n",
      "Epoch 50: Loss=98.7073\n",
      "Epoch 60: Loss=85.1164\n",
      "Epoch 70: Loss=79.2010\n",
      "Epoch 80: Loss=66.1435\n",
      "Epoch 90: Loss=57.0560\n",
      "Epoch 100: Loss=52.9378\n",
      "✅ MSE: 67.1342 | R²: 0.4708\n",
      "▶ Training: lr=0.001, batch=32, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=140.2791\n",
      "Epoch 20: Loss=156.9120\n",
      "Epoch 30: Loss=137.3598\n",
      "Epoch 40: Loss=130.8135\n",
      "Epoch 50: Loss=115.6364\n",
      "Epoch 60: Loss=98.6917\n",
      "Epoch 70: Loss=81.6034\n",
      "Epoch 80: Loss=67.8495\n",
      "Epoch 90: Loss=57.9727\n",
      "Epoch 100: Loss=54.4337\n",
      "Epoch 110: Loss=50.4365\n",
      "Epoch 120: Loss=45.4543\n",
      "Epoch 130: Loss=43.6984\n",
      "Epoch 140: Loss=40.9899\n",
      "Epoch 150: Loss=40.6148\n",
      "Epoch 160: Loss=38.5734\n",
      "Epoch 170: Loss=38.2622\n",
      "Epoch 180: Loss=38.2427\n",
      "Epoch 190: Loss=36.1897\n",
      "Epoch 200: Loss=34.6820\n",
      "✅ MSE: 67.8264 | R²: 0.4654\n",
      "▶ Training: lr=0.001, batch=32, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=140.7307\n",
      "Epoch 20: Loss=139.1579\n",
      "Epoch 30: Loss=135.2747\n",
      "Epoch 40: Loss=127.5246\n",
      "Epoch 50: Loss=114.1353\n",
      "Epoch 60: Loss=97.8145\n",
      "Epoch 70: Loss=82.0582\n",
      "Epoch 80: Loss=69.1603\n",
      "Epoch 90: Loss=59.9079\n",
      "Epoch 100: Loss=53.4672\n",
      "Epoch 110: Loss=47.7497\n",
      "Epoch 120: Loss=45.5866\n",
      "Epoch 130: Loss=42.5525\n",
      "Epoch 140: Loss=42.0078\n",
      "Epoch 150: Loss=39.9590\n",
      "Epoch 160: Loss=38.6549\n",
      "Epoch 170: Loss=38.3181\n",
      "Epoch 180: Loss=36.9271\n",
      "Epoch 190: Loss=36.3515\n",
      "Epoch 200: Loss=35.7969\n",
      "Epoch 210: Loss=36.0304\n",
      "Epoch 220: Loss=34.3679\n",
      "Epoch 230: Loss=35.2824\n",
      "Epoch 240: Loss=34.4866\n",
      "Epoch 250: Loss=33.9665\n",
      "Epoch 260: Loss=33.3494\n",
      "Epoch 270: Loss=32.7602\n",
      "Epoch 280: Loss=33.1852\n",
      "Epoch 290: Loss=32.7422\n",
      "Epoch 300: Loss=32.5173\n",
      "✅ MSE: 54.6444 | R²: 0.5693\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=140.2258\n",
      "Epoch 20: Loss=138.4734\n",
      "Epoch 30: Loss=136.9329\n",
      "Epoch 40: Loss=136.0641\n",
      "Epoch 50: Loss=136.6087\n",
      "Epoch 60: Loss=133.0950\n",
      "Epoch 70: Loss=131.8002\n",
      "Epoch 80: Loss=130.4657\n",
      "Epoch 90: Loss=131.4682\n",
      "Epoch 100: Loss=129.0154\n",
      "✅ MSE: 123.9554 | R²: 0.0230\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=140.9544\n",
      "Epoch 20: Loss=138.7596\n",
      "Epoch 30: Loss=140.1975\n",
      "Epoch 40: Loss=78.7951\n",
      "Epoch 50: Loss=49.5255\n",
      "Epoch 60: Loss=44.3627\n",
      "Epoch 70: Loss=41.7051\n",
      "Epoch 80: Loss=39.2699\n",
      "Epoch 90: Loss=39.3582\n",
      "Epoch 100: Loss=36.6998\n",
      "Epoch 110: Loss=36.2866\n",
      "Epoch 120: Loss=36.2485\n",
      "Epoch 130: Loss=34.9593\n",
      "Epoch 140: Loss=34.2339\n",
      "Epoch 150: Loss=32.3506\n",
      "Epoch 160: Loss=33.1952\n",
      "Epoch 170: Loss=33.0647\n",
      "Epoch 180: Loss=30.0444\n",
      "Epoch 190: Loss=29.5139\n",
      "Epoch 200: Loss=28.6109\n",
      "✅ MSE: 55.5832 | R²: 0.5619\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=140.9895\n",
      "Epoch 20: Loss=105.8313\n",
      "Epoch 30: Loss=60.3041\n",
      "Epoch 40: Loss=54.7640\n",
      "Epoch 50: Loss=52.2614\n",
      "Epoch 60: Loss=52.4400\n",
      "Epoch 70: Loss=51.9656\n",
      "Epoch 80: Loss=50.9841\n",
      "Epoch 90: Loss=51.0401\n",
      "Epoch 100: Loss=52.7469\n",
      "Epoch 110: Loss=50.5268\n",
      "Epoch 120: Loss=50.2892\n",
      "Epoch 130: Loss=50.7425\n",
      "Epoch 140: Loss=50.1151\n",
      "Epoch 150: Loss=49.2288\n",
      "Epoch 160: Loss=49.5959\n",
      "Epoch 170: Loss=50.8311\n",
      "Epoch 180: Loss=49.2186\n",
      "Epoch 190: Loss=50.1097\n",
      "Epoch 200: Loss=48.9432\n",
      "Epoch 210: Loss=49.4323\n",
      "Epoch 220: Loss=49.7044\n",
      "Epoch 230: Loss=49.1575\n",
      "Epoch 240: Loss=49.9158\n",
      "Epoch 250: Loss=49.5070\n",
      "Epoch 260: Loss=49.1966\n",
      "Epoch 270: Loss=49.3871\n",
      "Epoch 280: Loss=50.1453\n",
      "Epoch 290: Loss=48.6276\n",
      "Epoch 300: Loss=50.3368\n",
      "✅ MSE: 67.0334 | R²: 0.4716\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=140.8732\n",
      "Epoch 20: Loss=135.2912\n",
      "Epoch 30: Loss=124.5347\n",
      "Epoch 40: Loss=109.5541\n",
      "Epoch 50: Loss=95.0484\n",
      "Epoch 60: Loss=82.2871\n",
      "Epoch 70: Loss=71.1165\n",
      "Epoch 80: Loss=62.3734\n",
      "Epoch 90: Loss=56.5897\n",
      "Epoch 100: Loss=51.6717\n",
      "✅ MSE: 65.8317 | R²: 0.4811\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=140.0389\n",
      "Epoch 20: Loss=131.9854\n",
      "Epoch 30: Loss=119.7880\n",
      "Epoch 40: Loss=100.7601\n",
      "Epoch 50: Loss=84.6329\n",
      "Epoch 60: Loss=71.5449\n",
      "Epoch 70: Loss=63.0145\n",
      "Epoch 80: Loss=54.6743\n",
      "Epoch 90: Loss=50.1490\n",
      "Epoch 100: Loss=47.6512\n",
      "Epoch 110: Loss=44.3994\n",
      "Epoch 120: Loss=42.7155\n",
      "Epoch 130: Loss=41.5709\n",
      "Epoch 140: Loss=39.3828\n",
      "Epoch 150: Loss=38.2332\n",
      "Epoch 160: Loss=37.8276\n",
      "Epoch 170: Loss=36.5411\n",
      "Epoch 180: Loss=36.8753\n",
      "Epoch 190: Loss=37.2685\n",
      "Epoch 200: Loss=34.8968\n",
      "✅ MSE: 56.6076 | R²: 0.5538\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=150.0814\n",
      "Epoch 20: Loss=135.3005\n",
      "Epoch 30: Loss=124.7468\n",
      "Epoch 40: Loss=109.9077\n",
      "Epoch 50: Loss=100.9669\n",
      "Epoch 60: Loss=84.6859\n",
      "Epoch 70: Loss=73.6364\n",
      "Epoch 80: Loss=63.7991\n",
      "Epoch 90: Loss=56.8361\n",
      "Epoch 100: Loss=51.7772\n",
      "Epoch 110: Loss=48.0195\n",
      "Epoch 120: Loss=45.5632\n",
      "Epoch 130: Loss=43.6682\n",
      "Epoch 140: Loss=41.8810\n",
      "Epoch 150: Loss=40.2050\n",
      "Epoch 160: Loss=38.5129\n",
      "Epoch 170: Loss=38.2841\n",
      "Epoch 180: Loss=37.2708\n",
      "Epoch 190: Loss=36.0533\n",
      "Epoch 200: Loss=35.1457\n",
      "Epoch 210: Loss=34.2879\n",
      "Epoch 220: Loss=33.8137\n",
      "Epoch 230: Loss=33.5422\n",
      "Epoch 240: Loss=32.7684\n",
      "Epoch 250: Loss=32.4873\n",
      "Epoch 260: Loss=31.9780\n",
      "Epoch 270: Loss=31.5371\n",
      "Epoch 280: Loss=30.5105\n",
      "Epoch 290: Loss=29.8600\n",
      "Epoch 300: Loss=29.4992\n",
      "✅ MSE: 61.5038 | R²: 0.5152\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=142.0378\n",
      "Epoch 20: Loss=140.7664\n",
      "Epoch 30: Loss=137.3713\n",
      "Epoch 40: Loss=133.0643\n",
      "Epoch 50: Loss=126.3765\n",
      "Epoch 60: Loss=119.4145\n",
      "Epoch 70: Loss=111.1140\n",
      "Epoch 80: Loss=103.5485\n",
      "Epoch 90: Loss=96.5270\n",
      "Epoch 100: Loss=89.0895\n",
      "✅ MSE: 96.6935 | R²: 0.2378\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=142.0926\n",
      "Epoch 20: Loss=140.9379\n",
      "Epoch 30: Loss=136.8126\n",
      "Epoch 40: Loss=131.3060\n",
      "Epoch 50: Loss=124.2350\n",
      "Epoch 60: Loss=116.3447\n",
      "Epoch 70: Loss=108.6621\n",
      "Epoch 80: Loss=101.3826\n",
      "Epoch 90: Loss=94.0447\n",
      "Epoch 100: Loss=87.5843\n",
      "Epoch 110: Loss=81.8226\n",
      "Epoch 120: Loss=75.8788\n",
      "Epoch 130: Loss=70.8135\n",
      "Epoch 140: Loss=65.3552\n",
      "Epoch 150: Loss=60.8826\n",
      "Epoch 160: Loss=57.1505\n",
      "Epoch 170: Loss=53.8258\n",
      "Epoch 180: Loss=51.3009\n",
      "Epoch 190: Loss=49.3836\n",
      "Epoch 200: Loss=47.1570\n",
      "✅ MSE: 63.8803 | R²: 0.4965\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=142.1256\n",
      "Epoch 20: Loss=141.0511\n",
      "Epoch 30: Loss=139.2469\n",
      "Epoch 40: Loss=136.3600\n",
      "Epoch 50: Loss=131.9987\n",
      "Epoch 60: Loss=126.6930\n",
      "Epoch 70: Loss=120.6331\n",
      "Epoch 80: Loss=114.1757\n",
      "Epoch 90: Loss=107.9005\n",
      "Epoch 100: Loss=100.3902\n",
      "Epoch 110: Loss=94.1508\n",
      "Epoch 120: Loss=87.9503\n",
      "Epoch 130: Loss=81.8971\n",
      "Epoch 140: Loss=76.4220\n",
      "Epoch 150: Loss=71.4472\n",
      "Epoch 160: Loss=65.8485\n",
      "Epoch 170: Loss=62.8759\n",
      "Epoch 180: Loss=58.0015\n",
      "Epoch 190: Loss=54.1775\n",
      "Epoch 200: Loss=51.1605\n",
      "Epoch 210: Loss=48.1850\n",
      "Epoch 220: Loss=46.7890\n",
      "Epoch 230: Loss=45.1968\n",
      "Epoch 240: Loss=43.0112\n",
      "Epoch 250: Loss=42.6559\n",
      "Epoch 260: Loss=41.4523\n",
      "Epoch 270: Loss=40.0717\n",
      "Epoch 280: Loss=39.7321\n",
      "Epoch 290: Loss=38.3729\n",
      "Epoch 300: Loss=38.0518\n",
      "✅ MSE: 58.8076 | R²: 0.5365\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=143.0006\n",
      "Epoch 20: Loss=141.7716\n",
      "Epoch 30: Loss=141.2416\n",
      "Epoch 40: Loss=139.7446\n",
      "Epoch 50: Loss=138.1368\n",
      "Epoch 60: Loss=149.0416\n",
      "Epoch 70: Loss=133.5040\n",
      "Epoch 80: Loss=131.1042\n",
      "Epoch 90: Loss=130.4894\n",
      "Epoch 100: Loss=123.9261\n",
      "✅ MSE: 115.9378 | R²: 0.0861\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=142.5688\n",
      "Epoch 20: Loss=141.9478\n",
      "Epoch 30: Loss=141.3044\n",
      "Epoch 40: Loss=140.7094\n",
      "Epoch 50: Loss=139.4419\n",
      "Epoch 60: Loss=138.2438\n",
      "Epoch 70: Loss=135.7103\n",
      "Epoch 80: Loss=133.0104\n",
      "Epoch 90: Loss=129.9606\n",
      "Epoch 100: Loss=126.5898\n",
      "Epoch 110: Loss=122.9130\n",
      "Epoch 120: Loss=121.9189\n",
      "Epoch 130: Loss=115.3736\n",
      "Epoch 140: Loss=111.8314\n",
      "Epoch 150: Loss=108.1192\n",
      "Epoch 160: Loss=104.6759\n",
      "Epoch 170: Loss=102.3764\n",
      "Epoch 180: Loss=98.3663\n",
      "Epoch 190: Loss=95.2810\n",
      "Epoch 200: Loss=92.1251\n",
      "✅ MSE: 99.9308 | R²: 0.2123\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=143.4479\n",
      "Epoch 20: Loss=142.4601\n",
      "Epoch 30: Loss=142.2846\n",
      "Epoch 40: Loss=141.8838\n",
      "Epoch 50: Loss=141.0507\n",
      "Epoch 60: Loss=140.5021\n",
      "Epoch 70: Loss=149.5467\n",
      "Epoch 80: Loss=139.8641\n",
      "Epoch 90: Loss=137.0343\n",
      "Epoch 100: Loss=135.5666\n",
      "Epoch 110: Loss=133.5612\n",
      "Epoch 120: Loss=131.6140\n",
      "Epoch 130: Loss=129.6641\n",
      "Epoch 140: Loss=126.5589\n",
      "Epoch 150: Loss=125.1130\n",
      "Epoch 160: Loss=121.6662\n",
      "Epoch 170: Loss=118.6961\n",
      "Epoch 180: Loss=115.4423\n",
      "Epoch 190: Loss=112.6845\n",
      "Epoch 200: Loss=109.9362\n",
      "Epoch 210: Loss=108.9499\n",
      "Epoch 220: Loss=104.7868\n",
      "Epoch 230: Loss=103.7855\n",
      "Epoch 240: Loss=99.8133\n",
      "Epoch 250: Loss=97.4681\n",
      "Epoch 260: Loss=95.0948\n",
      "Epoch 270: Loss=92.7483\n",
      "Epoch 280: Loss=90.1440\n",
      "Epoch 290: Loss=87.7455\n",
      "Epoch 300: Loss=84.9986\n",
      "✅ MSE: 93.6652 | R²: 0.2617\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=143.2276\n",
      "Epoch 20: Loss=143.1497\n",
      "Epoch 30: Loss=156.3390\n",
      "Epoch 40: Loss=142.7712\n",
      "Epoch 50: Loss=142.3657\n",
      "Epoch 60: Loss=142.0129\n",
      "Epoch 70: Loss=141.3026\n",
      "Epoch 80: Loss=140.4192\n",
      "Epoch 90: Loss=139.4733\n",
      "Epoch 100: Loss=138.3887\n",
      "✅ MSE: 123.3503 | R²: 0.0277\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=143.3428\n",
      "Epoch 20: Loss=143.0704\n",
      "Epoch 30: Loss=142.2846\n",
      "Epoch 40: Loss=143.1893\n",
      "Epoch 50: Loss=141.5286\n",
      "Epoch 60: Loss=141.4628\n",
      "Epoch 70: Loss=140.9919\n",
      "Epoch 80: Loss=141.4744\n",
      "Epoch 90: Loss=140.0382\n",
      "Epoch 100: Loss=139.8936\n",
      "Epoch 110: Loss=138.9741\n",
      "Epoch 120: Loss=147.7170\n",
      "Epoch 130: Loss=137.8148\n",
      "Epoch 140: Loss=136.9113\n",
      "Epoch 150: Loss=135.9446\n",
      "Epoch 160: Loss=134.9527\n",
      "Epoch 170: Loss=133.9848\n",
      "Epoch 180: Loss=132.7533\n",
      "Epoch 190: Loss=131.5955\n",
      "Epoch 200: Loss=130.4218\n",
      "✅ MSE: 121.3710 | R²: 0.0433\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=143.8607\n",
      "Epoch 20: Loss=143.2805\n",
      "Epoch 30: Loss=142.6133\n",
      "Epoch 40: Loss=142.3515\n",
      "Epoch 50: Loss=142.0734\n",
      "Epoch 60: Loss=141.9579\n",
      "Epoch 70: Loss=141.6281\n",
      "Epoch 80: Loss=141.0799\n",
      "Epoch 90: Loss=140.6737\n",
      "Epoch 100: Loss=149.8084\n",
      "Epoch 110: Loss=139.6613\n",
      "Epoch 120: Loss=139.9055\n",
      "Epoch 130: Loss=138.4306\n",
      "Epoch 140: Loss=137.6607\n",
      "Epoch 150: Loss=136.5814\n",
      "Epoch 160: Loss=135.6020\n",
      "Epoch 170: Loss=134.5327\n",
      "Epoch 180: Loss=133.3652\n",
      "Epoch 190: Loss=132.0763\n",
      "Epoch 200: Loss=130.8238\n",
      "Epoch 210: Loss=129.4125\n",
      "Epoch 220: Loss=127.7259\n",
      "Epoch 230: Loss=126.0022\n",
      "Epoch 240: Loss=124.4260\n",
      "Epoch 250: Loss=122.5407\n",
      "Epoch 260: Loss=121.2645\n",
      "Epoch 270: Loss=118.7566\n",
      "Epoch 280: Loss=116.5119\n",
      "Epoch 290: Loss=114.3751\n",
      "Epoch 300: Loss=112.1774\n",
      "✅ MSE: 109.3365 | R²: 0.1382\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=143.0338\n",
      "Epoch 20: Loss=142.8153\n",
      "Epoch 30: Loss=142.0473\n",
      "Epoch 40: Loss=142.7475\n",
      "Epoch 50: Loss=140.7682\n",
      "Epoch 60: Loss=139.7276\n",
      "Epoch 70: Loss=138.1653\n",
      "Epoch 80: Loss=135.9153\n",
      "Epoch 90: Loss=133.1141\n",
      "Epoch 100: Loss=129.2879\n",
      "✅ MSE: 118.2739 | R²: 0.0677\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=143.2414\n",
      "Epoch 20: Loss=142.2848\n",
      "Epoch 30: Loss=142.0561\n",
      "Epoch 40: Loss=141.7356\n",
      "Epoch 50: Loss=141.5172\n",
      "Epoch 60: Loss=145.2135\n",
      "Epoch 70: Loss=140.8752\n",
      "Epoch 80: Loss=140.7842\n",
      "Epoch 90: Loss=140.2191\n",
      "Epoch 100: Loss=139.8922\n",
      "Epoch 110: Loss=139.3940\n",
      "Epoch 120: Loss=138.6873\n",
      "Epoch 130: Loss=138.1032\n",
      "Epoch 140: Loss=136.9616\n",
      "Epoch 150: Loss=136.0008\n",
      "Epoch 160: Loss=133.9784\n",
      "Epoch 170: Loss=131.7645\n",
      "Epoch 180: Loss=129.2200\n",
      "Epoch 190: Loss=126.2778\n",
      "Epoch 200: Loss=122.7452\n",
      "✅ MSE: 114.5729 | R²: 0.0969\n",
      "▶ Training: lr=0.0001, batch=8, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=143.1233\n",
      "Epoch 20: Loss=142.4618\n",
      "Epoch 30: Loss=142.1901\n",
      "Epoch 40: Loss=141.2288\n",
      "Epoch 50: Loss=140.4704\n",
      "Epoch 60: Loss=139.4566\n",
      "Epoch 70: Loss=141.8908\n",
      "Epoch 80: Loss=135.8886\n",
      "Epoch 90: Loss=142.0543\n",
      "Epoch 100: Loss=130.2158\n",
      "Epoch 110: Loss=125.5647\n",
      "Epoch 120: Loss=120.6390\n",
      "Epoch 130: Loss=115.4210\n",
      "Epoch 140: Loss=109.3669\n",
      "Epoch 150: Loss=102.4181\n",
      "Epoch 160: Loss=96.1042\n",
      "Epoch 170: Loss=89.7523\n",
      "Epoch 180: Loss=83.2906\n",
      "Epoch 190: Loss=77.6243\n",
      "Epoch 200: Loss=71.8433\n",
      "Epoch 210: Loss=66.7808\n",
      "Epoch 220: Loss=62.8131\n",
      "Epoch 230: Loss=58.7750\n",
      "Epoch 240: Loss=56.6763\n",
      "Epoch 250: Loss=53.1340\n",
      "Epoch 260: Loss=50.5180\n",
      "Epoch 270: Loss=48.7993\n",
      "Epoch 280: Loss=47.1591\n",
      "Epoch 290: Loss=46.2901\n",
      "Epoch 300: Loss=45.0406\n",
      "✅ MSE: 55.8369 | R²: 0.5599\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=140.0904\n",
      "Epoch 20: Loss=137.6200\n",
      "Epoch 30: Loss=134.2062\n",
      "Epoch 40: Loss=127.5186\n",
      "Epoch 50: Loss=116.6377\n",
      "Epoch 60: Loss=107.6207\n",
      "Epoch 70: Loss=99.6512\n",
      "Epoch 80: Loss=90.3423\n",
      "Epoch 90: Loss=83.5119\n",
      "Epoch 100: Loss=74.6034\n",
      "✅ MSE: 82.2162 | R²: 0.3520\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=139.0952\n",
      "Epoch 20: Loss=135.6202\n",
      "Epoch 30: Loss=129.4539\n",
      "Epoch 40: Loss=120.4784\n",
      "Epoch 50: Loss=110.0822\n",
      "Epoch 60: Loss=102.7194\n",
      "Epoch 70: Loss=91.4044\n",
      "Epoch 80: Loss=79.8864\n",
      "Epoch 90: Loss=75.9819\n",
      "Epoch 100: Loss=65.1540\n",
      "Epoch 110: Loss=58.3676\n",
      "Epoch 120: Loss=56.4921\n",
      "Epoch 130: Loss=50.4707\n",
      "Epoch 140: Loss=48.3655\n",
      "Epoch 150: Loss=45.4710\n",
      "Epoch 160: Loss=43.0375\n",
      "Epoch 170: Loss=41.0665\n",
      "Epoch 180: Loss=40.2286\n",
      "Epoch 190: Loss=39.7573\n",
      "Epoch 200: Loss=39.3866\n",
      "✅ MSE: 58.5190 | R²: 0.5387\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=186.9781\n",
      "Epoch 20: Loss=136.4135\n",
      "Epoch 30: Loss=130.3559\n",
      "Epoch 40: Loss=121.3783\n",
      "Epoch 50: Loss=111.8416\n",
      "Epoch 60: Loss=102.2314\n",
      "Epoch 70: Loss=93.5791\n",
      "Epoch 80: Loss=83.9801\n",
      "Epoch 90: Loss=79.7136\n",
      "Epoch 100: Loss=68.0427\n",
      "Epoch 110: Loss=61.8417\n",
      "Epoch 120: Loss=63.9697\n",
      "Epoch 130: Loss=53.5555\n",
      "Epoch 140: Loss=47.9980\n",
      "Epoch 150: Loss=47.0344\n",
      "Epoch 160: Loss=44.4215\n",
      "Epoch 170: Loss=42.3535\n",
      "Epoch 180: Loss=40.9950\n",
      "Epoch 190: Loss=39.1788\n",
      "Epoch 200: Loss=37.8901\n",
      "Epoch 210: Loss=37.0424\n",
      "Epoch 220: Loss=37.0075\n",
      "Epoch 230: Loss=36.6595\n",
      "Epoch 240: Loss=35.3179\n",
      "Epoch 250: Loss=35.7544\n",
      "Epoch 260: Loss=34.9059\n",
      "Epoch 270: Loss=34.2994\n",
      "Epoch 280: Loss=33.6071\n",
      "Epoch 290: Loss=32.7860\n",
      "Epoch 300: Loss=36.3719\n",
      "✅ MSE: 56.1346 | R²: 0.5575\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=140.2043\n",
      "Epoch 20: Loss=139.3347\n",
      "Epoch 30: Loss=138.2837\n",
      "Epoch 40: Loss=137.0965\n",
      "Epoch 50: Loss=134.6515\n",
      "Epoch 60: Loss=134.7470\n",
      "Epoch 70: Loss=130.6282\n",
      "Epoch 80: Loss=124.5978\n",
      "Epoch 90: Loss=121.9579\n",
      "Epoch 100: Loss=114.0854\n",
      "✅ MSE: 112.5764 | R²: 0.1126\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=143.8625\n",
      "Epoch 20: Loss=140.4672\n",
      "Epoch 30: Loss=138.9061\n",
      "Epoch 40: Loss=136.8898\n",
      "Epoch 50: Loss=135.6132\n",
      "Epoch 60: Loss=133.7447\n",
      "Epoch 70: Loss=135.0697\n",
      "Epoch 80: Loss=124.7930\n",
      "Epoch 90: Loss=119.2867\n",
      "Epoch 100: Loss=114.7214\n",
      "Epoch 110: Loss=109.1812\n",
      "Epoch 120: Loss=101.4805\n",
      "Epoch 130: Loss=95.4735\n",
      "Epoch 140: Loss=90.4323\n",
      "Epoch 150: Loss=89.2700\n",
      "Epoch 160: Loss=80.8613\n",
      "Epoch 170: Loss=86.6558\n",
      "Epoch 180: Loss=68.4341\n",
      "Epoch 190: Loss=62.9149\n",
      "Epoch 200: Loss=59.3401\n",
      "✅ MSE: 76.1659 | R²: 0.3996\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=141.4221\n",
      "Epoch 20: Loss=139.1365\n",
      "Epoch 30: Loss=137.2210\n",
      "Epoch 40: Loss=132.7850\n",
      "Epoch 50: Loss=127.6882\n",
      "Epoch 60: Loss=120.7888\n",
      "Epoch 70: Loss=113.4557\n",
      "Epoch 80: Loss=105.9083\n",
      "Epoch 90: Loss=97.9382\n",
      "Epoch 100: Loss=91.4703\n",
      "Epoch 110: Loss=85.3228\n",
      "Epoch 120: Loss=80.3550\n",
      "Epoch 130: Loss=74.7127\n",
      "Epoch 140: Loss=68.7129\n",
      "Epoch 150: Loss=67.1170\n",
      "Epoch 160: Loss=59.5275\n",
      "Epoch 170: Loss=56.4348\n",
      "Epoch 180: Loss=53.7950\n",
      "Epoch 190: Loss=51.2799\n",
      "Epoch 200: Loss=50.2746\n",
      "Epoch 210: Loss=48.5501\n",
      "Epoch 220: Loss=47.9752\n",
      "Epoch 230: Loss=45.8694\n",
      "Epoch 240: Loss=46.2298\n",
      "Epoch 250: Loss=47.7376\n",
      "Epoch 260: Loss=45.4374\n",
      "Epoch 270: Loss=44.6402\n",
      "Epoch 280: Loss=43.2341\n",
      "Epoch 290: Loss=44.1009\n",
      "Epoch 300: Loss=42.6698\n",
      "✅ MSE: 60.9503 | R²: 0.5196\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=158.3924\n",
      "Epoch 20: Loss=141.1129\n",
      "Epoch 30: Loss=139.9855\n",
      "Epoch 40: Loss=141.1591\n",
      "Epoch 50: Loss=141.1497\n",
      "Epoch 60: Loss=139.0029\n",
      "Epoch 70: Loss=139.6452\n",
      "Epoch 80: Loss=138.1001\n",
      "Epoch 90: Loss=137.4917\n",
      "Epoch 100: Loss=157.6752\n",
      "✅ MSE: 125.7376 | R²: 0.0089\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=142.2211\n",
      "Epoch 20: Loss=141.6839\n",
      "Epoch 30: Loss=139.7473\n",
      "Epoch 40: Loss=139.4353\n",
      "Epoch 50: Loss=139.1058\n",
      "Epoch 60: Loss=138.0619\n",
      "Epoch 70: Loss=136.9629\n",
      "Epoch 80: Loss=135.4510\n",
      "Epoch 90: Loss=133.5940\n",
      "Epoch 100: Loss=133.7164\n",
      "Epoch 110: Loss=145.1977\n",
      "Epoch 120: Loss=126.3047\n",
      "Epoch 130: Loss=129.7084\n",
      "Epoch 140: Loss=120.1967\n",
      "Epoch 150: Loss=124.7188\n",
      "Epoch 160: Loss=165.1727\n",
      "Epoch 170: Loss=111.2657\n",
      "Epoch 180: Loss=110.1494\n",
      "Epoch 190: Loss=106.2225\n",
      "Epoch 200: Loss=113.1737\n",
      "✅ MSE: 108.1062 | R²: 0.1479\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=141.1645\n",
      "Epoch 20: Loss=139.6562\n",
      "Epoch 30: Loss=140.3330\n",
      "Epoch 40: Loss=138.3033\n",
      "Epoch 50: Loss=138.2773\n",
      "Epoch 60: Loss=201.5475\n",
      "Epoch 70: Loss=136.4726\n",
      "Epoch 80: Loss=134.6196\n",
      "Epoch 90: Loss=133.1552\n",
      "Epoch 100: Loss=131.6697\n",
      "Epoch 110: Loss=129.4338\n",
      "Epoch 120: Loss=127.3536\n",
      "Epoch 130: Loss=125.0242\n",
      "Epoch 140: Loss=121.9257\n",
      "Epoch 150: Loss=124.9123\n",
      "Epoch 160: Loss=116.6015\n",
      "Epoch 170: Loss=114.3919\n",
      "Epoch 180: Loss=111.6708\n",
      "Epoch 190: Loss=109.9186\n",
      "Epoch 200: Loss=106.3322\n",
      "Epoch 210: Loss=103.4933\n",
      "Epoch 220: Loss=101.2601\n",
      "Epoch 230: Loss=98.2149\n",
      "Epoch 240: Loss=97.9101\n",
      "Epoch 250: Loss=129.5856\n",
      "Epoch 260: Loss=91.0168\n",
      "Epoch 270: Loss=89.2613\n",
      "Epoch 280: Loss=89.1973\n",
      "Epoch 290: Loss=84.2613\n",
      "Epoch 300: Loss=81.8378\n",
      "✅ MSE: 91.8465 | R²: 0.2760\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=142.5959\n",
      "Epoch 20: Loss=140.5835\n",
      "Epoch 30: Loss=141.5539\n",
      "Epoch 40: Loss=140.2974\n",
      "Epoch 50: Loss=187.6520\n",
      "Epoch 60: Loss=139.9380\n",
      "Epoch 70: Loss=139.3195\n",
      "Epoch 80: Loss=140.6642\n",
      "Epoch 90: Loss=138.7230\n",
      "Epoch 100: Loss=139.0515\n",
      "✅ MSE: 125.5734 | R²: 0.0102\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=141.0350\n",
      "Epoch 20: Loss=141.2428\n",
      "Epoch 30: Loss=193.4025\n",
      "Epoch 40: Loss=189.7109\n",
      "Epoch 50: Loss=140.3730\n",
      "Epoch 60: Loss=159.8841\n",
      "Epoch 70: Loss=145.9270\n",
      "Epoch 80: Loss=139.2409\n",
      "Epoch 90: Loss=139.0167\n",
      "Epoch 100: Loss=139.2019\n",
      "Epoch 110: Loss=139.2003\n",
      "Epoch 120: Loss=138.2884\n",
      "Epoch 130: Loss=138.3143\n",
      "Epoch 140: Loss=137.6624\n",
      "Epoch 150: Loss=138.0489\n",
      "Epoch 160: Loss=136.7760\n",
      "Epoch 170: Loss=136.2594\n",
      "Epoch 180: Loss=137.2180\n",
      "Epoch 190: Loss=136.9741\n",
      "Epoch 200: Loss=135.1770\n",
      "✅ MSE: 124.4057 | R²: 0.0194\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=142.0408\n",
      "Epoch 20: Loss=140.4422\n",
      "Epoch 30: Loss=140.3917\n",
      "Epoch 40: Loss=140.5255\n",
      "Epoch 50: Loss=139.6520\n",
      "Epoch 60: Loss=140.7012\n",
      "Epoch 70: Loss=139.2033\n",
      "Epoch 80: Loss=137.8103\n",
      "Epoch 90: Loss=137.0627\n",
      "Epoch 100: Loss=137.1463\n",
      "Epoch 110: Loss=135.8177\n",
      "Epoch 120: Loss=135.4881\n",
      "Epoch 130: Loss=133.6415\n",
      "Epoch 140: Loss=133.3258\n",
      "Epoch 150: Loss=132.0025\n",
      "Epoch 160: Loss=131.0318\n",
      "Epoch 170: Loss=129.3679\n",
      "Epoch 180: Loss=127.8600\n",
      "Epoch 190: Loss=132.0811\n",
      "Epoch 200: Loss=123.8137\n",
      "Epoch 210: Loss=122.6913\n",
      "Epoch 220: Loss=121.7871\n",
      "Epoch 230: Loss=118.9502\n",
      "Epoch 240: Loss=117.1492\n",
      "Epoch 250: Loss=115.9619\n",
      "Epoch 260: Loss=117.9115\n",
      "Epoch 270: Loss=111.7187\n",
      "Epoch 280: Loss=111.2861\n",
      "Epoch 290: Loss=107.8538\n",
      "Epoch 300: Loss=106.7608\n",
      "✅ MSE: 109.2410 | R²: 0.1389\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=141.1483\n",
      "Epoch 20: Loss=141.2769\n",
      "Epoch 30: Loss=140.6303\n",
      "Epoch 40: Loss=141.0771\n",
      "Epoch 50: Loss=140.4002\n",
      "Epoch 60: Loss=140.1029\n",
      "Epoch 70: Loss=139.2209\n",
      "Epoch 80: Loss=138.7034\n",
      "Epoch 90: Loss=137.3890\n",
      "Epoch 100: Loss=137.0267\n",
      "✅ MSE: 123.5518 | R²: 0.0261\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=152.8539\n",
      "Epoch 20: Loss=140.3934\n",
      "Epoch 30: Loss=139.6780\n",
      "Epoch 40: Loss=139.2924\n",
      "Epoch 50: Loss=139.1587\n",
      "Epoch 60: Loss=138.9132\n",
      "Epoch 70: Loss=155.8539\n",
      "Epoch 80: Loss=139.0163\n",
      "Epoch 90: Loss=137.6762\n",
      "Epoch 100: Loss=136.8740\n",
      "Epoch 110: Loss=136.4665\n",
      "Epoch 120: Loss=134.7443\n",
      "Epoch 130: Loss=132.8696\n",
      "Epoch 140: Loss=131.1271\n",
      "Epoch 150: Loss=129.4309\n",
      "Epoch 160: Loss=126.9738\n",
      "Epoch 170: Loss=124.6471\n",
      "Epoch 180: Loss=120.6597\n",
      "Epoch 190: Loss=117.2941\n",
      "Epoch 200: Loss=114.3506\n",
      "✅ MSE: 111.7888 | R²: 0.1189\n",
      "▶ Training: lr=0.0001, batch=16, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=140.8192\n",
      "Epoch 20: Loss=140.4351\n",
      "Epoch 30: Loss=140.2305\n",
      "Epoch 40: Loss=140.2957\n",
      "Epoch 50: Loss=139.3514\n",
      "Epoch 60: Loss=138.7191\n",
      "Epoch 70: Loss=140.1322\n",
      "Epoch 80: Loss=140.9842\n",
      "Epoch 90: Loss=133.9427\n",
      "Epoch 100: Loss=131.5745\n",
      "Epoch 110: Loss=128.1532\n",
      "Epoch 120: Loss=124.9564\n",
      "Epoch 130: Loss=120.7457\n",
      "Epoch 140: Loss=122.7720\n",
      "Epoch 150: Loss=113.0792\n",
      "Epoch 160: Loss=108.3729\n",
      "Epoch 170: Loss=104.8721\n",
      "Epoch 180: Loss=101.3628\n",
      "Epoch 190: Loss=98.5144\n",
      "Epoch 200: Loss=99.5607\n",
      "Epoch 210: Loss=90.6952\n",
      "Epoch 220: Loss=88.9843\n",
      "Epoch 230: Loss=82.9005\n",
      "Epoch 240: Loss=79.6454\n",
      "Epoch 250: Loss=81.1802\n",
      "Epoch 260: Loss=80.7581\n",
      "Epoch 270: Loss=70.1602\n",
      "Epoch 280: Loss=67.1717\n",
      "Epoch 290: Loss=65.0730\n",
      "Epoch 300: Loss=78.2823\n",
      "✅ MSE: 77.6395 | R²: 0.3880\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=144.6001\n",
      "Epoch 20: Loss=138.2833\n",
      "Epoch 30: Loss=135.3299\n",
      "Epoch 40: Loss=152.4549\n",
      "Epoch 50: Loss=124.9319\n",
      "Epoch 60: Loss=117.7458\n",
      "Epoch 70: Loss=123.4959\n",
      "Epoch 80: Loss=101.8615\n",
      "Epoch 90: Loss=94.6428\n",
      "Epoch 100: Loss=87.0291\n",
      "✅ MSE: 96.6362 | R²: 0.2383\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=141.1776\n",
      "Epoch 20: Loss=138.8630\n",
      "Epoch 30: Loss=134.4090\n",
      "Epoch 40: Loss=130.0283\n",
      "Epoch 50: Loss=121.5759\n",
      "Epoch 60: Loss=113.3243\n",
      "Epoch 70: Loss=106.5660\n",
      "Epoch 80: Loss=97.0214\n",
      "Epoch 90: Loss=89.2306\n",
      "Epoch 100: Loss=81.8018\n",
      "Epoch 110: Loss=76.8441\n",
      "Epoch 120: Loss=75.4680\n",
      "Epoch 130: Loss=64.4888\n",
      "Epoch 140: Loss=59.1706\n",
      "Epoch 150: Loss=54.8917\n",
      "Epoch 160: Loss=52.2197\n",
      "Epoch 170: Loss=49.6436\n",
      "Epoch 180: Loss=46.7453\n",
      "Epoch 190: Loss=44.9793\n",
      "Epoch 200: Loss=42.9254\n",
      "✅ MSE: 59.8937 | R²: 0.5279\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=140.1545\n",
      "Epoch 20: Loss=139.1754\n",
      "Epoch 30: Loss=135.7415\n",
      "Epoch 40: Loss=131.8218\n",
      "Epoch 50: Loss=126.4283\n",
      "Epoch 60: Loss=122.1388\n",
      "Epoch 70: Loss=114.9513\n",
      "Epoch 80: Loss=106.6752\n",
      "Epoch 90: Loss=107.5927\n",
      "Epoch 100: Loss=93.5997\n",
      "Epoch 110: Loss=84.2727\n",
      "Epoch 120: Loss=77.9732\n",
      "Epoch 130: Loss=70.6920\n",
      "Epoch 140: Loss=67.5511\n",
      "Epoch 150: Loss=62.1860\n",
      "Epoch 160: Loss=57.2983\n",
      "Epoch 170: Loss=52.6799\n",
      "Epoch 180: Loss=50.4715\n",
      "Epoch 190: Loss=48.1147\n",
      "Epoch 200: Loss=45.1419\n",
      "Epoch 210: Loss=43.2594\n",
      "Epoch 220: Loss=42.6539\n",
      "Epoch 230: Loss=41.4986\n",
      "Epoch 240: Loss=41.6797\n",
      "Epoch 250: Loss=39.3309\n",
      "Epoch 260: Loss=38.1945\n",
      "Epoch 270: Loss=38.8970\n",
      "Epoch 280: Loss=37.8502\n",
      "Epoch 290: Loss=36.5724\n",
      "Epoch 300: Loss=37.2299\n",
      "✅ MSE: 61.2668 | R²: 0.5171\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=141.4359\n",
      "Epoch 20: Loss=140.1538\n",
      "Epoch 30: Loss=138.8536\n",
      "Epoch 40: Loss=137.8149\n",
      "Epoch 50: Loss=153.4386\n",
      "Epoch 60: Loss=134.1196\n",
      "Epoch 70: Loss=130.8111\n",
      "Epoch 80: Loss=128.3024\n",
      "Epoch 90: Loss=124.5283\n",
      "Epoch 100: Loss=120.1916\n",
      "✅ MSE: 117.3751 | R²: 0.0748\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=141.3060\n",
      "Epoch 20: Loss=139.2189\n",
      "Epoch 30: Loss=151.0936\n",
      "Epoch 40: Loss=136.7571\n",
      "Epoch 50: Loss=151.8691\n",
      "Epoch 60: Loss=131.5546\n",
      "Epoch 70: Loss=127.6848\n",
      "Epoch 80: Loss=123.8918\n",
      "Epoch 90: Loss=118.8790\n",
      "Epoch 100: Loss=115.2900\n",
      "Epoch 110: Loss=108.6890\n",
      "Epoch 120: Loss=103.7563\n",
      "Epoch 130: Loss=99.0093\n",
      "Epoch 140: Loss=92.5703\n",
      "Epoch 150: Loss=88.2966\n",
      "Epoch 160: Loss=84.2750\n",
      "Epoch 170: Loss=79.2649\n",
      "Epoch 180: Loss=74.3474\n",
      "Epoch 190: Loss=70.2338\n",
      "Epoch 200: Loss=66.8389\n",
      "✅ MSE: 80.5540 | R²: 0.3651\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=141.0537\n",
      "Epoch 20: Loss=157.1794\n",
      "Epoch 30: Loss=157.0771\n",
      "Epoch 40: Loss=138.1557\n",
      "Epoch 50: Loss=136.4552\n",
      "Epoch 60: Loss=133.2055\n",
      "Epoch 70: Loss=130.6878\n",
      "Epoch 80: Loss=133.0375\n",
      "Epoch 90: Loss=125.7036\n",
      "Epoch 100: Loss=118.9937\n",
      "Epoch 110: Loss=114.5797\n",
      "Epoch 120: Loss=112.9764\n",
      "Epoch 130: Loss=105.6249\n",
      "Epoch 140: Loss=104.1914\n",
      "Epoch 150: Loss=97.4883\n",
      "Epoch 160: Loss=92.8008\n",
      "Epoch 170: Loss=88.4502\n",
      "Epoch 180: Loss=84.7531\n",
      "Epoch 190: Loss=80.9699\n",
      "Epoch 200: Loss=86.1056\n",
      "Epoch 210: Loss=74.0117\n",
      "Epoch 220: Loss=70.7222\n",
      "Epoch 230: Loss=67.6009\n",
      "Epoch 240: Loss=65.2813\n",
      "Epoch 250: Loss=62.4496\n",
      "Epoch 260: Loss=61.5711\n",
      "Epoch 270: Loss=56.4891\n",
      "Epoch 280: Loss=55.2552\n",
      "Epoch 290: Loss=53.6004\n",
      "Epoch 300: Loss=51.2494\n",
      "✅ MSE: 64.0269 | R²: 0.4953\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=141.4085\n",
      "Epoch 20: Loss=141.8355\n",
      "Epoch 30: Loss=140.3981\n",
      "Epoch 40: Loss=140.3800\n",
      "Epoch 50: Loss=140.2314\n",
      "Epoch 60: Loss=140.0464\n",
      "Epoch 70: Loss=143.4150\n",
      "Epoch 80: Loss=140.5690\n",
      "Epoch 90: Loss=139.4485\n",
      "Epoch 100: Loss=137.1570\n",
      "✅ MSE: 125.1153 | R²: 0.0138\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=142.0523\n",
      "Epoch 20: Loss=141.0076\n",
      "Epoch 30: Loss=142.1972\n",
      "Epoch 40: Loss=144.7212\n",
      "Epoch 50: Loss=140.1396\n",
      "Epoch 60: Loss=140.5734\n",
      "Epoch 70: Loss=144.8440\n",
      "Epoch 80: Loss=139.9216\n",
      "Epoch 90: Loss=140.4680\n",
      "Epoch 100: Loss=137.9911\n",
      "Epoch 110: Loss=138.6272\n",
      "Epoch 120: Loss=141.1579\n",
      "Epoch 130: Loss=134.1009\n",
      "Epoch 140: Loss=134.2394\n",
      "Epoch 150: Loss=131.2350\n",
      "Epoch 160: Loss=129.7209\n",
      "Epoch 170: Loss=128.2456\n",
      "Epoch 180: Loss=125.8896\n",
      "Epoch 190: Loss=125.0222\n",
      "Epoch 200: Loss=121.4992\n",
      "✅ MSE: 116.4094 | R²: 0.0824\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=141.2822\n",
      "Epoch 20: Loss=141.7888\n",
      "Epoch 30: Loss=144.9974\n",
      "Epoch 40: Loss=143.6881\n",
      "Epoch 50: Loss=140.2190\n",
      "Epoch 60: Loss=139.9068\n",
      "Epoch 70: Loss=138.0367\n",
      "Epoch 80: Loss=137.6147\n",
      "Epoch 90: Loss=136.8483\n",
      "Epoch 100: Loss=147.8536\n",
      "Epoch 110: Loss=135.8716\n",
      "Epoch 120: Loss=132.7412\n",
      "Epoch 130: Loss=135.7804\n",
      "Epoch 140: Loss=129.9966\n",
      "Epoch 150: Loss=128.7406\n",
      "Epoch 160: Loss=125.3767\n",
      "Epoch 170: Loss=123.1527\n",
      "Epoch 180: Loss=121.9513\n",
      "Epoch 190: Loss=119.3180\n",
      "Epoch 200: Loss=116.7746\n",
      "Epoch 210: Loss=113.2780\n",
      "Epoch 220: Loss=110.2388\n",
      "Epoch 230: Loss=115.6420\n",
      "Epoch 240: Loss=116.2625\n",
      "Epoch 250: Loss=100.7502\n",
      "Epoch 260: Loss=108.9765\n",
      "Epoch 270: Loss=95.0224\n",
      "Epoch 280: Loss=93.2006\n",
      "Epoch 290: Loss=89.9278\n",
      "Epoch 300: Loss=85.7362\n",
      "✅ MSE: 95.5785 | R²: 0.2466\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=141.4834\n",
      "Epoch 20: Loss=141.3498\n",
      "Epoch 30: Loss=141.0834\n",
      "Epoch 40: Loss=141.4183\n",
      "Epoch 50: Loss=141.4444\n",
      "Epoch 60: Loss=140.3649\n",
      "Epoch 70: Loss=158.3722\n",
      "Epoch 80: Loss=153.2103\n",
      "Epoch 90: Loss=140.2050\n",
      "Epoch 100: Loss=141.2215\n",
      "✅ MSE: 125.1563 | R²: 0.0135\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=143.0962\n",
      "Epoch 20: Loss=141.7638\n",
      "Epoch 30: Loss=141.6875\n",
      "Epoch 40: Loss=141.0138\n",
      "Epoch 50: Loss=142.8062\n",
      "Epoch 60: Loss=146.6953\n",
      "Epoch 70: Loss=141.0870\n",
      "Epoch 80: Loss=141.1627\n",
      "Epoch 90: Loss=140.6043\n",
      "Epoch 100: Loss=153.6882\n",
      "Epoch 110: Loss=140.6360\n",
      "Epoch 120: Loss=140.9163\n",
      "Epoch 130: Loss=140.5524\n",
      "Epoch 140: Loss=153.1681\n",
      "Epoch 150: Loss=147.0555\n",
      "Epoch 160: Loss=144.0293\n",
      "Epoch 170: Loss=140.2913\n",
      "Epoch 180: Loss=141.2788\n",
      "Epoch 190: Loss=158.1340\n",
      "Epoch 200: Loss=140.0360\n",
      "✅ MSE: 126.3519 | R²: 0.0041\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=142.4147\n",
      "Epoch 20: Loss=140.7819\n",
      "Epoch 30: Loss=140.9151\n",
      "Epoch 40: Loss=140.7844\n",
      "Epoch 50: Loss=142.2911\n",
      "Epoch 60: Loss=140.8463\n",
      "Epoch 70: Loss=153.4378\n",
      "Epoch 80: Loss=141.8480\n",
      "Epoch 90: Loss=140.2696\n",
      "Epoch 100: Loss=142.2436\n",
      "Epoch 110: Loss=153.0753\n",
      "Epoch 120: Loss=138.9051\n",
      "Epoch 130: Loss=138.1366\n",
      "Epoch 140: Loss=141.8770\n",
      "Epoch 150: Loss=138.0389\n",
      "Epoch 160: Loss=137.0404\n",
      "Epoch 170: Loss=140.7032\n",
      "Epoch 180: Loss=134.8811\n",
      "Epoch 190: Loss=135.3466\n",
      "Epoch 200: Loss=133.6237\n",
      "Epoch 210: Loss=133.2416\n",
      "Epoch 220: Loss=131.1032\n",
      "Epoch 230: Loss=130.2722\n",
      "Epoch 240: Loss=134.6099\n",
      "Epoch 250: Loss=127.6653\n",
      "Epoch 260: Loss=127.3651\n",
      "Epoch 270: Loss=125.4857\n",
      "Epoch 280: Loss=124.1313\n",
      "Epoch 290: Loss=123.2010\n",
      "Epoch 300: Loss=123.2309\n",
      "✅ MSE: 116.1452 | R²: 0.0845\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=154.4746\n",
      "Epoch 20: Loss=142.8013\n",
      "Epoch 30: Loss=140.6101\n",
      "Epoch 40: Loss=153.5894\n",
      "Epoch 50: Loss=140.4638\n",
      "Epoch 60: Loss=140.6828\n",
      "Epoch 70: Loss=145.1725\n",
      "Epoch 80: Loss=139.1808\n",
      "Epoch 90: Loss=141.3456\n",
      "Epoch 100: Loss=141.1026\n",
      "✅ MSE: 126.8844 | R²: -0.0001\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=140.8818\n",
      "Epoch 20: Loss=141.5041\n",
      "Epoch 30: Loss=141.2510\n",
      "Epoch 40: Loss=159.5480\n",
      "Epoch 50: Loss=141.7484\n",
      "Epoch 60: Loss=141.0649\n",
      "Epoch 70: Loss=146.3388\n",
      "Epoch 80: Loss=145.9455\n",
      "Epoch 90: Loss=139.7771\n",
      "Epoch 100: Loss=141.3369\n",
      "Epoch 110: Loss=138.9361\n",
      "Epoch 120: Loss=144.9760\n",
      "Epoch 130: Loss=137.6660\n",
      "Epoch 140: Loss=136.6822\n",
      "Epoch 150: Loss=135.4872\n",
      "Epoch 160: Loss=133.8959\n",
      "Epoch 170: Loss=132.7781\n",
      "Epoch 180: Loss=132.2171\n",
      "Epoch 190: Loss=133.9426\n",
      "Epoch 200: Loss=126.0866\n",
      "✅ MSE: 119.1760 | R²: 0.0606\n",
      "▶ Training: lr=0.0001, batch=32, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=143.9327\n",
      "Epoch 20: Loss=154.9915\n",
      "Epoch 30: Loss=141.3790\n",
      "Epoch 40: Loss=153.7040\n",
      "Epoch 50: Loss=140.8612\n",
      "Epoch 60: Loss=145.5554\n",
      "Epoch 70: Loss=140.1876\n",
      "Epoch 80: Loss=140.1937\n",
      "Epoch 90: Loss=140.9407\n",
      "Epoch 100: Loss=140.4831\n",
      "Epoch 110: Loss=138.1982\n",
      "Epoch 120: Loss=155.1426\n",
      "Epoch 130: Loss=135.4532\n",
      "Epoch 140: Loss=134.2631\n",
      "Epoch 150: Loss=133.7709\n",
      "Epoch 160: Loss=130.3114\n",
      "Epoch 170: Loss=127.7981\n",
      "Epoch 180: Loss=125.4523\n",
      "Epoch 190: Loss=122.2699\n",
      "Epoch 200: Loss=124.9902\n",
      "Epoch 210: Loss=117.4465\n",
      "Epoch 220: Loss=119.4571\n",
      "Epoch 230: Loss=110.5789\n",
      "Epoch 240: Loss=127.6698\n",
      "Epoch 250: Loss=104.6807\n",
      "Epoch 260: Loss=101.2761\n",
      "Epoch 270: Loss=98.6420\n",
      "Epoch 280: Loss=99.2971\n",
      "Epoch 290: Loss=98.6489\n",
      "Epoch 300: Loss=91.1997\n",
      "✅ MSE: 99.6663 | R²: 0.2144\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=142.9782\n",
      "Epoch 20: Loss=142.7186\n",
      "Epoch 30: Loss=142.4508\n",
      "Epoch 40: Loss=141.6959\n",
      "Epoch 50: Loss=142.4023\n",
      "Epoch 60: Loss=144.9903\n",
      "Epoch 70: Loss=140.5218\n",
      "Epoch 80: Loss=140.2083\n",
      "Epoch 90: Loss=139.8880\n",
      "Epoch 100: Loss=139.5206\n",
      "✅ MSE: 125.1722 | R²: 0.0134\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=143.2841\n",
      "Epoch 20: Loss=142.6580\n",
      "Epoch 30: Loss=142.2610\n",
      "Epoch 40: Loss=142.0701\n",
      "Epoch 50: Loss=141.6993\n",
      "Epoch 60: Loss=141.4343\n",
      "Epoch 70: Loss=140.9709\n",
      "Epoch 80: Loss=140.5672\n",
      "Epoch 90: Loss=140.1931\n",
      "Epoch 100: Loss=140.0937\n",
      "Epoch 110: Loss=139.6493\n",
      "Epoch 120: Loss=138.9211\n",
      "Epoch 130: Loss=138.5388\n",
      "Epoch 140: Loss=138.1992\n",
      "Epoch 150: Loss=137.5715\n",
      "Epoch 160: Loss=136.8805\n",
      "Epoch 170: Loss=136.3846\n",
      "Epoch 180: Loss=135.7098\n",
      "Epoch 190: Loss=135.0161\n",
      "Epoch 200: Loss=134.3591\n",
      "✅ MSE: 122.8727 | R²: 0.0315\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=142.9834\n",
      "Epoch 20: Loss=142.5782\n",
      "Epoch 30: Loss=142.2002\n",
      "Epoch 40: Loss=141.8096\n",
      "Epoch 50: Loss=141.5028\n",
      "Epoch 60: Loss=141.1482\n",
      "Epoch 70: Loss=141.1587\n",
      "Epoch 80: Loss=140.5859\n",
      "Epoch 90: Loss=142.5150\n",
      "Epoch 100: Loss=144.1212\n",
      "Epoch 110: Loss=139.4254\n",
      "Epoch 120: Loss=139.2064\n",
      "Epoch 130: Loss=138.6819\n",
      "Epoch 140: Loss=138.1544\n",
      "Epoch 150: Loss=138.8656\n",
      "Epoch 160: Loss=137.2567\n",
      "Epoch 170: Loss=136.6975\n",
      "Epoch 180: Loss=136.2196\n",
      "Epoch 190: Loss=135.4013\n",
      "Epoch 200: Loss=134.8506\n",
      "Epoch 210: Loss=134.7656\n",
      "Epoch 220: Loss=133.6282\n",
      "Epoch 230: Loss=133.1241\n",
      "Epoch 240: Loss=132.1470\n",
      "Epoch 250: Loss=131.2796\n",
      "Epoch 260: Loss=130.5690\n",
      "Epoch 270: Loss=130.2226\n",
      "Epoch 280: Loss=128.9119\n",
      "Epoch 290: Loss=128.1853\n",
      "Epoch 300: Loss=127.0119\n",
      "✅ MSE: 118.8860 | R²: 0.0629\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=143.4473\n",
      "Epoch 20: Loss=143.2978\n",
      "Epoch 30: Loss=143.5291\n",
      "Epoch 40: Loss=143.0414\n",
      "Epoch 50: Loss=142.9251\n",
      "Epoch 60: Loss=143.0203\n",
      "Epoch 70: Loss=142.4297\n",
      "Epoch 80: Loss=142.6413\n",
      "Epoch 90: Loss=142.2281\n",
      "Epoch 100: Loss=141.9237\n",
      "✅ MSE: 125.7814 | R²: 0.0086\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=143.5410\n",
      "Epoch 20: Loss=143.1753\n",
      "Epoch 30: Loss=143.0281\n",
      "Epoch 40: Loss=142.8510\n",
      "Epoch 50: Loss=142.6893\n",
      "Epoch 60: Loss=142.6672\n",
      "Epoch 70: Loss=142.4761\n",
      "Epoch 80: Loss=142.2957\n",
      "Epoch 90: Loss=142.4790\n",
      "Epoch 100: Loss=141.8798\n",
      "Epoch 110: Loss=141.6979\n",
      "Epoch 120: Loss=141.5547\n",
      "Epoch 130: Loss=141.6808\n",
      "Epoch 140: Loss=141.5078\n",
      "Epoch 150: Loss=141.2779\n",
      "Epoch 160: Loss=141.4920\n",
      "Epoch 170: Loss=141.2986\n",
      "Epoch 180: Loss=140.8243\n",
      "Epoch 190: Loss=140.7070\n",
      "Epoch 200: Loss=140.5244\n",
      "✅ MSE: 125.3520 | R²: 0.0119\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=143.5111\n",
      "Epoch 20: Loss=143.3790\n",
      "Epoch 30: Loss=142.7579\n",
      "Epoch 40: Loss=143.7039\n",
      "Epoch 50: Loss=142.3850\n",
      "Epoch 60: Loss=142.7450\n",
      "Epoch 70: Loss=142.2335\n",
      "Epoch 80: Loss=142.1023\n",
      "Epoch 90: Loss=141.9814\n",
      "Epoch 100: Loss=141.7176\n",
      "Epoch 110: Loss=141.6532\n",
      "Epoch 120: Loss=141.6235\n",
      "Epoch 130: Loss=156.0107\n",
      "Epoch 140: Loss=141.7083\n",
      "Epoch 150: Loss=141.3612\n",
      "Epoch 160: Loss=141.0819\n",
      "Epoch 170: Loss=141.0716\n",
      "Epoch 180: Loss=141.2851\n",
      "Epoch 190: Loss=140.9950\n",
      "Epoch 200: Loss=140.9834\n",
      "Epoch 210: Loss=150.5736\n",
      "Epoch 220: Loss=141.1374\n",
      "Epoch 230: Loss=141.5791\n",
      "Epoch 240: Loss=140.1041\n",
      "Epoch 250: Loss=139.9312\n",
      "Epoch 260: Loss=140.0044\n",
      "Epoch 270: Loss=139.5687\n",
      "Epoch 280: Loss=139.2583\n",
      "Epoch 290: Loss=139.1869\n",
      "Epoch 300: Loss=138.8556\n",
      "✅ MSE: 125.5226 | R²: 0.0106\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=143.3928\n",
      "Epoch 20: Loss=143.2071\n",
      "Epoch 30: Loss=143.1397\n",
      "Epoch 40: Loss=143.1131\n",
      "Epoch 50: Loss=143.0571\n",
      "Epoch 60: Loss=146.3969\n",
      "Epoch 70: Loss=143.0294\n",
      "Epoch 80: Loss=142.7851\n",
      "Epoch 90: Loss=143.0095\n",
      "Epoch 100: Loss=142.6851\n",
      "✅ MSE: 126.2960 | R²: 0.0045\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=143.5846\n",
      "Epoch 20: Loss=143.5491\n",
      "Epoch 30: Loss=144.6854\n",
      "Epoch 40: Loss=143.5410\n",
      "Epoch 50: Loss=143.2080\n",
      "Epoch 60: Loss=143.2745\n",
      "Epoch 70: Loss=143.0878\n",
      "Epoch 80: Loss=143.3009\n",
      "Epoch 90: Loss=142.7637\n",
      "Epoch 100: Loss=142.6733\n",
      "Epoch 110: Loss=142.8341\n",
      "Epoch 120: Loss=142.5749\n",
      "Epoch 130: Loss=142.3888\n",
      "Epoch 140: Loss=142.2842\n",
      "Epoch 150: Loss=142.5134\n",
      "Epoch 160: Loss=142.2624\n",
      "Epoch 170: Loss=142.4888\n",
      "Epoch 180: Loss=142.1056\n",
      "Epoch 190: Loss=142.5497\n",
      "Epoch 200: Loss=141.9564\n",
      "✅ MSE: 125.9156 | R²: 0.0075\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=143.7808\n",
      "Epoch 20: Loss=144.7946\n",
      "Epoch 30: Loss=143.6066\n",
      "Epoch 40: Loss=143.6046\n",
      "Epoch 50: Loss=143.7914\n",
      "Epoch 60: Loss=143.5533\n",
      "Epoch 70: Loss=143.3638\n",
      "Epoch 80: Loss=143.2168\n",
      "Epoch 90: Loss=143.2951\n",
      "Epoch 100: Loss=143.1517\n",
      "Epoch 110: Loss=143.9217\n",
      "Epoch 120: Loss=142.8651\n",
      "Epoch 130: Loss=142.6564\n",
      "Epoch 140: Loss=142.4614\n",
      "Epoch 150: Loss=142.3954\n",
      "Epoch 160: Loss=142.2372\n",
      "Epoch 170: Loss=142.3595\n",
      "Epoch 180: Loss=141.9688\n",
      "Epoch 190: Loss=142.1374\n",
      "Epoch 200: Loss=141.8126\n",
      "Epoch 210: Loss=141.5945\n",
      "Epoch 220: Loss=141.5427\n",
      "Epoch 230: Loss=141.4438\n",
      "Epoch 240: Loss=141.4721\n",
      "Epoch 250: Loss=141.3490\n",
      "Epoch 260: Loss=141.0371\n",
      "Epoch 270: Loss=141.0920\n",
      "Epoch 280: Loss=140.8822\n",
      "Epoch 290: Loss=140.8532\n",
      "Epoch 300: Loss=140.7264\n",
      "✅ MSE: 125.2109 | R²: 0.0131\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=143.3714\n",
      "Epoch 20: Loss=143.4949\n",
      "Epoch 30: Loss=143.6042\n",
      "Epoch 40: Loss=156.6333\n",
      "Epoch 50: Loss=143.2733\n",
      "Epoch 60: Loss=143.2976\n",
      "Epoch 70: Loss=143.2113\n",
      "Epoch 80: Loss=143.1982\n",
      "Epoch 90: Loss=143.3448\n",
      "Epoch 100: Loss=143.2206\n",
      "✅ MSE: 126.8415 | R²: 0.0002\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=143.1683\n",
      "Epoch 20: Loss=143.0308\n",
      "Epoch 30: Loss=143.0328\n",
      "Epoch 40: Loss=144.0188\n",
      "Epoch 50: Loss=142.9558\n",
      "Epoch 60: Loss=143.0098\n",
      "Epoch 70: Loss=143.0636\n",
      "Epoch 80: Loss=147.0668\n",
      "Epoch 90: Loss=143.0578\n",
      "Epoch 100: Loss=142.8552\n",
      "Epoch 110: Loss=143.9686\n",
      "Epoch 120: Loss=143.0050\n",
      "Epoch 130: Loss=152.6367\n",
      "Epoch 140: Loss=142.7177\n",
      "Epoch 150: Loss=142.8930\n",
      "Epoch 160: Loss=142.6316\n",
      "Epoch 170: Loss=143.0312\n",
      "Epoch 180: Loss=142.6164\n",
      "Epoch 190: Loss=142.6360\n",
      "Epoch 200: Loss=142.7606\n",
      "✅ MSE: 126.7078 | R²: 0.0013\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=143.7123\n",
      "Epoch 20: Loss=143.6045\n",
      "Epoch 30: Loss=143.5135\n",
      "Epoch 40: Loss=143.4540\n",
      "Epoch 50: Loss=143.4514\n",
      "Epoch 60: Loss=144.2781\n",
      "Epoch 70: Loss=143.4155\n",
      "Epoch 80: Loss=143.5251\n",
      "Epoch 90: Loss=143.4395\n",
      "Epoch 100: Loss=143.4113\n",
      "Epoch 110: Loss=143.5238\n",
      "Epoch 120: Loss=143.5537\n",
      "Epoch 130: Loss=147.7883\n",
      "Epoch 140: Loss=143.4542\n",
      "Epoch 150: Loss=143.3420\n",
      "Epoch 160: Loss=143.3350\n",
      "Epoch 170: Loss=143.2610\n",
      "Epoch 180: Loss=156.6503\n",
      "Epoch 190: Loss=143.1825\n",
      "Epoch 200: Loss=143.3746\n",
      "Epoch 210: Loss=143.2092\n",
      "Epoch 220: Loss=143.7611\n",
      "Epoch 230: Loss=143.6411\n",
      "Epoch 240: Loss=143.1955\n",
      "Epoch 250: Loss=143.0224\n",
      "Epoch 260: Loss=142.9640\n",
      "Epoch 270: Loss=143.3571\n",
      "Epoch 280: Loss=143.0241\n",
      "Epoch 290: Loss=142.8336\n",
      "Epoch 300: Loss=142.8894\n",
      "✅ MSE: 126.2734 | R²: 0.0047\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=144.3339\n",
      "Epoch 20: Loss=143.6607\n",
      "Epoch 30: Loss=143.5970\n",
      "Epoch 40: Loss=143.6119\n",
      "Epoch 50: Loss=144.7466\n",
      "Epoch 60: Loss=147.5766\n",
      "Epoch 70: Loss=143.3843\n",
      "Epoch 80: Loss=143.2819\n",
      "Epoch 90: Loss=143.4186\n",
      "Epoch 100: Loss=143.3310\n",
      "✅ MSE: 126.5726 | R²: 0.0023\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=143.2269\n",
      "Epoch 20: Loss=156.4240\n",
      "Epoch 30: Loss=143.0264\n",
      "Epoch 40: Loss=143.1821\n",
      "Epoch 50: Loss=143.0753\n",
      "Epoch 60: Loss=142.9320\n",
      "Epoch 70: Loss=142.9506\n",
      "Epoch 80: Loss=142.9768\n",
      "Epoch 90: Loss=142.8594\n",
      "Epoch 100: Loss=143.2929\n",
      "Epoch 110: Loss=143.0732\n",
      "Epoch 120: Loss=142.8096\n",
      "Epoch 130: Loss=142.7844\n",
      "Epoch 140: Loss=142.7677\n",
      "Epoch 150: Loss=142.8798\n",
      "Epoch 160: Loss=142.6926\n",
      "Epoch 170: Loss=142.7212\n",
      "Epoch 180: Loss=142.9534\n",
      "Epoch 190: Loss=142.7663\n",
      "Epoch 200: Loss=142.5255\n",
      "✅ MSE: 126.7626 | R²: 0.0008\n",
      "▶ Training: lr=1e-05, batch=8, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=143.9364\n",
      "Epoch 20: Loss=143.4996\n",
      "Epoch 30: Loss=143.5784\n",
      "Epoch 40: Loss=143.5562\n",
      "Epoch 50: Loss=143.5979\n",
      "Epoch 60: Loss=143.4970\n",
      "Epoch 70: Loss=143.3211\n",
      "Epoch 80: Loss=143.2928\n",
      "Epoch 90: Loss=143.3863\n",
      "Epoch 100: Loss=143.2117\n",
      "Epoch 110: Loss=157.5418\n",
      "Epoch 120: Loss=143.3449\n",
      "Epoch 130: Loss=143.1377\n",
      "Epoch 140: Loss=144.4759\n",
      "Epoch 150: Loss=142.9164\n",
      "Epoch 160: Loss=143.0712\n",
      "Epoch 170: Loss=152.8001\n",
      "Epoch 180: Loss=146.8604\n",
      "Epoch 190: Loss=142.8041\n",
      "Epoch 200: Loss=142.9169\n",
      "Epoch 210: Loss=142.5543\n",
      "Epoch 220: Loss=142.5842\n",
      "Epoch 230: Loss=145.9255\n",
      "Epoch 240: Loss=142.5997\n",
      "Epoch 250: Loss=142.5980\n",
      "Epoch 260: Loss=142.6106\n",
      "Epoch 270: Loss=142.2364\n",
      "Epoch 280: Loss=142.0645\n",
      "Epoch 290: Loss=142.0672\n",
      "Epoch 300: Loss=141.9712\n",
      "✅ MSE: 125.9458 | R²: 0.0073\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=141.2009\n",
      "Epoch 20: Loss=158.0670\n",
      "Epoch 30: Loss=140.6770\n",
      "Epoch 40: Loss=140.3650\n",
      "Epoch 50: Loss=139.8923\n",
      "Epoch 60: Loss=139.3537\n",
      "Epoch 70: Loss=139.6447\n",
      "Epoch 80: Loss=140.0600\n",
      "Epoch 90: Loss=139.8518\n",
      "Epoch 100: Loss=202.4472\n",
      "✅ MSE: 124.9113 | R²: 0.0154\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=141.0129\n",
      "Epoch 20: Loss=140.5294\n",
      "Epoch 30: Loss=141.9009\n",
      "Epoch 40: Loss=145.8719\n",
      "Epoch 50: Loss=139.7395\n",
      "Epoch 60: Loss=139.3609\n",
      "Epoch 70: Loss=141.5582\n",
      "Epoch 80: Loss=139.6546\n",
      "Epoch 90: Loss=139.0779\n",
      "Epoch 100: Loss=138.6321\n",
      "Epoch 110: Loss=138.1065\n",
      "Epoch 120: Loss=138.2034\n",
      "Epoch 130: Loss=137.7923\n",
      "Epoch 140: Loss=137.5573\n",
      "Epoch 150: Loss=137.0516\n",
      "Epoch 160: Loss=136.5522\n",
      "Epoch 170: Loss=136.9547\n",
      "Epoch 180: Loss=136.3935\n",
      "Epoch 190: Loss=141.7868\n",
      "Epoch 200: Loss=152.6756\n",
      "✅ MSE: 124.2991 | R²: 0.0202\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=141.5604\n",
      "Epoch 20: Loss=145.4231\n",
      "Epoch 30: Loss=140.7751\n",
      "Epoch 40: Loss=139.7424\n",
      "Epoch 50: Loss=139.9460\n",
      "Epoch 60: Loss=139.9361\n",
      "Epoch 70: Loss=139.8246\n",
      "Epoch 80: Loss=140.8644\n",
      "Epoch 90: Loss=139.4751\n",
      "Epoch 100: Loss=138.6182\n",
      "Epoch 110: Loss=143.8546\n",
      "Epoch 120: Loss=138.1647\n",
      "Epoch 130: Loss=138.0086\n",
      "Epoch 140: Loss=137.9885\n",
      "Epoch 150: Loss=137.6800\n",
      "Epoch 160: Loss=137.9013\n",
      "Epoch 170: Loss=140.8345\n",
      "Epoch 180: Loss=137.7480\n",
      "Epoch 190: Loss=136.9997\n",
      "Epoch 200: Loss=136.0695\n",
      "Epoch 210: Loss=136.6556\n",
      "Epoch 220: Loss=135.9977\n",
      "Epoch 230: Loss=135.1242\n",
      "Epoch 240: Loss=139.8971\n",
      "Epoch 250: Loss=152.5235\n",
      "Epoch 260: Loss=133.9933\n",
      "Epoch 270: Loss=134.2121\n",
      "Epoch 280: Loss=133.4989\n",
      "Epoch 290: Loss=148.4688\n",
      "Epoch 300: Loss=132.0179\n",
      "✅ MSE: 122.9546 | R²: 0.0308\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=142.9278\n",
      "Epoch 20: Loss=142.6249\n",
      "Epoch 30: Loss=140.4571\n",
      "Epoch 40: Loss=140.9304\n",
      "Epoch 50: Loss=140.7990\n",
      "Epoch 60: Loss=140.1847\n",
      "Epoch 70: Loss=143.1081\n",
      "Epoch 80: Loss=141.0974\n",
      "Epoch 90: Loss=140.1486\n",
      "Epoch 100: Loss=139.9990\n",
      "✅ MSE: 126.2838 | R²: 0.0046\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=140.9989\n",
      "Epoch 20: Loss=140.7684\n",
      "Epoch 30: Loss=140.9567\n",
      "Epoch 40: Loss=141.0830\n",
      "Epoch 50: Loss=141.5926\n",
      "Epoch 60: Loss=140.4281\n",
      "Epoch 70: Loss=140.7039\n",
      "Epoch 80: Loss=140.0052\n",
      "Epoch 90: Loss=140.1167\n",
      "Epoch 100: Loss=140.5645\n",
      "Epoch 110: Loss=139.8490\n",
      "Epoch 120: Loss=139.5260\n",
      "Epoch 130: Loss=188.4011\n",
      "Epoch 140: Loss=143.0149\n",
      "Epoch 150: Loss=139.6632\n",
      "Epoch 160: Loss=139.4845\n",
      "Epoch 170: Loss=139.5605\n",
      "Epoch 180: Loss=139.6952\n",
      "Epoch 190: Loss=138.5514\n",
      "Epoch 200: Loss=138.2853\n",
      "✅ MSE: 125.6672 | R²: 0.0095\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=142.1710\n",
      "Epoch 20: Loss=141.5622\n",
      "Epoch 30: Loss=140.9971\n",
      "Epoch 40: Loss=147.7747\n",
      "Epoch 50: Loss=140.6622\n",
      "Epoch 60: Loss=140.8697\n",
      "Epoch 70: Loss=140.8741\n",
      "Epoch 80: Loss=140.5352\n",
      "Epoch 90: Loss=140.0371\n",
      "Epoch 100: Loss=141.1812\n",
      "Epoch 110: Loss=140.4215\n",
      "Epoch 120: Loss=141.0579\n",
      "Epoch 130: Loss=140.5324\n",
      "Epoch 140: Loss=140.7019\n",
      "Epoch 150: Loss=140.6412\n",
      "Epoch 160: Loss=139.4371\n",
      "Epoch 170: Loss=139.3054\n",
      "Epoch 180: Loss=140.3931\n",
      "Epoch 190: Loss=141.6210\n",
      "Epoch 200: Loss=139.3037\n",
      "Epoch 210: Loss=138.6630\n",
      "Epoch 220: Loss=139.0735\n",
      "Epoch 230: Loss=141.6729\n",
      "Epoch 240: Loss=139.6107\n",
      "Epoch 250: Loss=185.1166\n",
      "Epoch 260: Loss=144.6550\n",
      "Epoch 270: Loss=138.2622\n",
      "Epoch 280: Loss=137.8047\n",
      "Epoch 290: Loss=138.6521\n",
      "Epoch 300: Loss=138.0921\n",
      "✅ MSE: 125.0408 | R²: 0.0144\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=141.2348\n",
      "Epoch 20: Loss=142.8365\n",
      "Epoch 30: Loss=140.7843\n",
      "Epoch 40: Loss=141.4843\n",
      "Epoch 50: Loss=140.9751\n",
      "Epoch 60: Loss=140.7454\n",
      "Epoch 70: Loss=141.7034\n",
      "Epoch 80: Loss=140.6439\n",
      "Epoch 90: Loss=140.6229\n",
      "Epoch 100: Loss=190.0310\n",
      "✅ MSE: 126.3396 | R²: 0.0042\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=145.8166\n",
      "Epoch 20: Loss=140.6512\n",
      "Epoch 30: Loss=140.8634\n",
      "Epoch 40: Loss=142.0627\n",
      "Epoch 50: Loss=141.9577\n",
      "Epoch 60: Loss=141.4798\n",
      "Epoch 70: Loss=141.1500\n",
      "Epoch 80: Loss=160.9297\n",
      "Epoch 90: Loss=140.8256\n",
      "Epoch 100: Loss=140.7826\n",
      "Epoch 110: Loss=140.9670\n",
      "Epoch 120: Loss=141.0077\n",
      "Epoch 130: Loss=141.0015\n",
      "Epoch 140: Loss=141.5549\n",
      "Epoch 150: Loss=141.2786\n",
      "Epoch 160: Loss=140.1434\n",
      "Epoch 170: Loss=140.3176\n",
      "Epoch 180: Loss=140.3188\n",
      "Epoch 190: Loss=140.1743\n",
      "Epoch 200: Loss=140.2533\n",
      "✅ MSE: 126.2188 | R²: 0.0051\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=141.1052\n",
      "Epoch 20: Loss=140.9913\n",
      "Epoch 30: Loss=141.7268\n",
      "Epoch 40: Loss=146.9516\n",
      "Epoch 50: Loss=142.0288\n",
      "Epoch 60: Loss=141.7174\n",
      "Epoch 70: Loss=141.1543\n",
      "Epoch 80: Loss=141.0339\n",
      "Epoch 90: Loss=142.6723\n",
      "Epoch 100: Loss=141.0297\n",
      "Epoch 110: Loss=146.4772\n",
      "Epoch 120: Loss=140.7336\n",
      "Epoch 130: Loss=140.6516\n",
      "Epoch 140: Loss=140.6329\n",
      "Epoch 150: Loss=148.6409\n",
      "Epoch 160: Loss=141.0026\n",
      "Epoch 170: Loss=141.2443\n",
      "Epoch 180: Loss=140.5614\n",
      "Epoch 190: Loss=140.7098\n",
      "Epoch 200: Loss=140.5215\n",
      "Epoch 210: Loss=140.4123\n",
      "Epoch 220: Loss=140.0511\n",
      "Epoch 230: Loss=204.7984\n",
      "Epoch 240: Loss=142.4134\n",
      "Epoch 250: Loss=140.2416\n",
      "Epoch 260: Loss=140.6490\n",
      "Epoch 270: Loss=140.4827\n",
      "Epoch 280: Loss=140.8871\n",
      "Epoch 290: Loss=141.5064\n",
      "Epoch 300: Loss=140.1213\n",
      "✅ MSE: 125.9021 | R²: 0.0076\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=141.5459\n",
      "Epoch 20: Loss=141.1918\n",
      "Epoch 30: Loss=146.5611\n",
      "Epoch 40: Loss=144.5627\n",
      "Epoch 50: Loss=159.1476\n",
      "Epoch 60: Loss=142.0040\n",
      "Epoch 70: Loss=141.2418\n",
      "Epoch 80: Loss=140.7097\n",
      "Epoch 90: Loss=141.1157\n",
      "Epoch 100: Loss=140.9324\n",
      "✅ MSE: 126.8980 | R²: -0.0002\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=141.6966\n",
      "Epoch 20: Loss=141.4147\n",
      "Epoch 30: Loss=141.5536\n",
      "Epoch 40: Loss=142.0146\n",
      "Epoch 50: Loss=140.9694\n",
      "Epoch 60: Loss=141.7233\n",
      "Epoch 70: Loss=141.6478\n",
      "Epoch 80: Loss=141.3421\n",
      "Epoch 90: Loss=142.7014\n",
      "Epoch 100: Loss=140.9500\n",
      "Epoch 110: Loss=141.3202\n",
      "Epoch 120: Loss=141.0427\n",
      "Epoch 130: Loss=142.9457\n",
      "Epoch 140: Loss=141.2494\n",
      "Epoch 150: Loss=140.8434\n",
      "Epoch 160: Loss=141.4486\n",
      "Epoch 170: Loss=141.8208\n",
      "Epoch 180: Loss=141.1465\n",
      "Epoch 190: Loss=140.8322\n",
      "Epoch 200: Loss=141.0086\n",
      "✅ MSE: 127.0252 | R²: -0.0012\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=141.0856\n",
      "Epoch 20: Loss=141.4037\n",
      "Epoch 30: Loss=141.5505\n",
      "Epoch 40: Loss=141.5956\n",
      "Epoch 50: Loss=141.5112\n",
      "Epoch 60: Loss=141.4639\n",
      "Epoch 70: Loss=142.8034\n",
      "Epoch 80: Loss=142.0102\n",
      "Epoch 90: Loss=140.9837\n",
      "Epoch 100: Loss=140.9375\n",
      "Epoch 110: Loss=141.4026\n",
      "Epoch 120: Loss=142.1167\n",
      "Epoch 130: Loss=141.1431\n",
      "Epoch 140: Loss=141.4789\n",
      "Epoch 150: Loss=141.1694\n",
      "Epoch 160: Loss=141.2165\n",
      "Epoch 170: Loss=140.9645\n",
      "Epoch 180: Loss=140.8996\n",
      "Epoch 190: Loss=140.8512\n",
      "Epoch 200: Loss=141.6883\n",
      "Epoch 210: Loss=140.7176\n",
      "Epoch 220: Loss=143.3952\n",
      "Epoch 230: Loss=140.7597\n",
      "Epoch 240: Loss=141.7750\n",
      "Epoch 250: Loss=141.7277\n",
      "Epoch 260: Loss=140.8182\n",
      "Epoch 270: Loss=188.8481\n",
      "Epoch 280: Loss=141.0713\n",
      "Epoch 290: Loss=141.0870\n",
      "Epoch 300: Loss=140.6879\n",
      "✅ MSE: 126.8571 | R²: 0.0001\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=140.8234\n",
      "Epoch 20: Loss=141.5095\n",
      "Epoch 30: Loss=141.7463\n",
      "Epoch 40: Loss=141.3890\n",
      "Epoch 50: Loss=141.1123\n",
      "Epoch 60: Loss=140.6583\n",
      "Epoch 70: Loss=141.0499\n",
      "Epoch 80: Loss=141.6110\n",
      "Epoch 90: Loss=140.9265\n",
      "Epoch 100: Loss=141.2943\n",
      "✅ MSE: 126.8181 | R²: 0.0004\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=140.7633\n",
      "Epoch 20: Loss=140.9569\n",
      "Epoch 30: Loss=146.3401\n",
      "Epoch 40: Loss=140.9344\n",
      "Epoch 50: Loss=141.7528\n",
      "Epoch 60: Loss=141.2117\n",
      "Epoch 70: Loss=140.9002\n",
      "Epoch 80: Loss=143.2854\n",
      "Epoch 90: Loss=140.9107\n",
      "Epoch 100: Loss=141.1177\n",
      "Epoch 110: Loss=140.7333\n",
      "Epoch 120: Loss=142.8665\n",
      "Epoch 130: Loss=144.5394\n",
      "Epoch 140: Loss=140.6532\n",
      "Epoch 150: Loss=165.3408\n",
      "Epoch 160: Loss=140.7183\n",
      "Epoch 170: Loss=140.7161\n",
      "Epoch 180: Loss=141.2458\n",
      "Epoch 190: Loss=142.5447\n",
      "Epoch 200: Loss=207.5697\n",
      "✅ MSE: 127.0192 | R²: -0.0012\n",
      "▶ Training: lr=1e-05, batch=16, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=141.3895\n",
      "Epoch 20: Loss=141.2340\n",
      "Epoch 30: Loss=141.4767\n",
      "Epoch 40: Loss=144.0306\n",
      "Epoch 50: Loss=140.7511\n",
      "Epoch 60: Loss=141.0614\n",
      "Epoch 70: Loss=141.1613\n",
      "Epoch 80: Loss=141.7619\n",
      "Epoch 90: Loss=141.4758\n",
      "Epoch 100: Loss=207.8219\n",
      "Epoch 110: Loss=140.6160\n",
      "Epoch 120: Loss=140.9743\n",
      "Epoch 130: Loss=141.1091\n",
      "Epoch 140: Loss=140.9461\n",
      "Epoch 150: Loss=142.8759\n",
      "Epoch 160: Loss=140.7236\n",
      "Epoch 170: Loss=140.8658\n",
      "Epoch 180: Loss=143.2849\n",
      "Epoch 190: Loss=140.9373\n",
      "Epoch 200: Loss=140.6130\n",
      "Epoch 210: Loss=140.9702\n",
      "Epoch 220: Loss=141.9428\n",
      "Epoch 230: Loss=140.8341\n",
      "Epoch 240: Loss=142.2505\n",
      "Epoch 250: Loss=140.4804\n",
      "Epoch 260: Loss=140.4054\n",
      "Epoch 270: Loss=140.9016\n",
      "Epoch 280: Loss=141.1924\n",
      "Epoch 290: Loss=140.2421\n",
      "Epoch 300: Loss=140.9456\n",
      "✅ MSE: 126.3109 | R²: 0.0044\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=141.5891\n",
      "Epoch 20: Loss=140.8155\n",
      "Epoch 30: Loss=140.7780\n",
      "Epoch 40: Loss=140.5328\n",
      "Epoch 50: Loss=140.6089\n",
      "Epoch 60: Loss=139.9703\n",
      "Epoch 70: Loss=139.8491\n",
      "Epoch 80: Loss=139.9820\n",
      "Epoch 90: Loss=139.5761\n",
      "Epoch 100: Loss=144.1145\n",
      "✅ MSE: 125.6442 | R²: 0.0096\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=141.1519\n",
      "Epoch 20: Loss=154.3811\n",
      "Epoch 30: Loss=142.6565\n",
      "Epoch 40: Loss=140.6038\n",
      "Epoch 50: Loss=140.9024\n",
      "Epoch 60: Loss=141.2857\n",
      "Epoch 70: Loss=142.7728\n",
      "Epoch 80: Loss=139.9912\n",
      "Epoch 90: Loss=152.5873\n",
      "Epoch 100: Loss=139.5902\n",
      "Epoch 110: Loss=141.6641\n",
      "Epoch 120: Loss=156.8136\n",
      "Epoch 130: Loss=140.6018\n",
      "Epoch 140: Loss=138.7229\n",
      "Epoch 150: Loss=139.5078\n",
      "Epoch 160: Loss=140.4239\n",
      "Epoch 170: Loss=138.5641\n",
      "Epoch 180: Loss=139.8082\n",
      "Epoch 190: Loss=137.6390\n",
      "Epoch 200: Loss=137.9265\n",
      "✅ MSE: 125.2338 | R²: 0.0129\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=142.1492\n",
      "Epoch 20: Loss=141.3254\n",
      "Epoch 30: Loss=141.0919\n",
      "Epoch 40: Loss=141.7167\n",
      "Epoch 50: Loss=140.6054\n",
      "Epoch 60: Loss=141.8109\n",
      "Epoch 70: Loss=139.7216\n",
      "Epoch 80: Loss=144.8632\n",
      "Epoch 90: Loss=145.1296\n",
      "Epoch 100: Loss=139.8796\n",
      "Epoch 110: Loss=139.6640\n",
      "Epoch 120: Loss=140.4355\n",
      "Epoch 130: Loss=138.9954\n",
      "Epoch 140: Loss=140.0572\n",
      "Epoch 150: Loss=138.6635\n",
      "Epoch 160: Loss=138.2540\n",
      "Epoch 170: Loss=137.8913\n",
      "Epoch 180: Loss=155.4772\n",
      "Epoch 190: Loss=138.2337\n",
      "Epoch 200: Loss=143.7713\n",
      "Epoch 210: Loss=142.9139\n",
      "Epoch 220: Loss=155.8906\n",
      "Epoch 230: Loss=140.0680\n",
      "Epoch 240: Loss=136.7927\n",
      "Epoch 250: Loss=138.2793\n",
      "Epoch 260: Loss=136.2765\n",
      "Epoch 270: Loss=153.8420\n",
      "Epoch 280: Loss=135.3720\n",
      "Epoch 290: Loss=137.4645\n",
      "Epoch 300: Loss=134.8556\n",
      "✅ MSE: 123.8487 | R²: 0.0238\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=158.7040\n",
      "Epoch 20: Loss=140.9119\n",
      "Epoch 30: Loss=142.7674\n",
      "Epoch 40: Loss=141.0551\n",
      "Epoch 50: Loss=141.1408\n",
      "Epoch 60: Loss=140.5736\n",
      "Epoch 70: Loss=140.4975\n",
      "Epoch 80: Loss=141.0677\n",
      "Epoch 90: Loss=140.9542\n",
      "Epoch 100: Loss=141.6787\n",
      "✅ MSE: 126.2514 | R²: 0.0049\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=141.4855\n",
      "Epoch 20: Loss=154.4769\n",
      "Epoch 30: Loss=159.2868\n",
      "Epoch 40: Loss=141.7583\n",
      "Epoch 50: Loss=143.5003\n",
      "Epoch 60: Loss=142.6308\n",
      "Epoch 70: Loss=141.2272\n",
      "Epoch 80: Loss=146.5819\n",
      "Epoch 90: Loss=141.3925\n",
      "Epoch 100: Loss=141.1513\n",
      "Epoch 110: Loss=141.1295\n",
      "Epoch 120: Loss=153.9530\n",
      "Epoch 130: Loss=142.1956\n",
      "Epoch 140: Loss=153.6161\n",
      "Epoch 150: Loss=140.3273\n",
      "Epoch 160: Loss=140.5385\n",
      "Epoch 170: Loss=140.1607\n",
      "Epoch 180: Loss=141.2574\n",
      "Epoch 190: Loss=141.1853\n",
      "Epoch 200: Loss=159.6193\n",
      "✅ MSE: 125.9792 | R²: 0.0070\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=142.4488\n",
      "Epoch 20: Loss=154.8430\n",
      "Epoch 30: Loss=141.5896\n",
      "Epoch 40: Loss=141.2079\n",
      "Epoch 50: Loss=143.5959\n",
      "Epoch 60: Loss=142.2852\n",
      "Epoch 70: Loss=141.3396\n",
      "Epoch 80: Loss=141.6359\n",
      "Epoch 90: Loss=153.7071\n",
      "Epoch 100: Loss=145.8058\n",
      "Epoch 110: Loss=140.3297\n",
      "Epoch 120: Loss=140.5377\n",
      "Epoch 130: Loss=141.3286\n",
      "Epoch 140: Loss=140.3759\n",
      "Epoch 150: Loss=140.0537\n",
      "Epoch 160: Loss=139.6176\n",
      "Epoch 170: Loss=140.6723\n",
      "Epoch 180: Loss=141.4222\n",
      "Epoch 190: Loss=139.6423\n",
      "Epoch 200: Loss=139.8370\n",
      "Epoch 210: Loss=139.0807\n",
      "Epoch 220: Loss=140.1819\n",
      "Epoch 230: Loss=140.2334\n",
      "Epoch 240: Loss=139.2714\n",
      "Epoch 250: Loss=139.2168\n",
      "Epoch 260: Loss=139.0432\n",
      "Epoch 270: Loss=138.5970\n",
      "Epoch 280: Loss=139.1256\n",
      "Epoch 290: Loss=140.5402\n",
      "Epoch 300: Loss=138.8868\n",
      "✅ MSE: 125.0784 | R²: 0.0141\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=154.2850\n",
      "Epoch 20: Loss=141.5142\n",
      "Epoch 30: Loss=146.1828\n",
      "Epoch 40: Loss=141.2187\n",
      "Epoch 50: Loss=141.3117\n",
      "Epoch 60: Loss=141.2574\n",
      "Epoch 70: Loss=143.3309\n",
      "Epoch 80: Loss=142.4913\n",
      "Epoch 90: Loss=141.3403\n",
      "Epoch 100: Loss=141.2368\n",
      "✅ MSE: 126.7614 | R²: 0.0008\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=141.9556\n",
      "Epoch 20: Loss=141.5085\n",
      "Epoch 30: Loss=142.0554\n",
      "Epoch 40: Loss=141.5537\n",
      "Epoch 50: Loss=141.5973\n",
      "Epoch 60: Loss=141.2178\n",
      "Epoch 70: Loss=142.4851\n",
      "Epoch 80: Loss=142.5095\n",
      "Epoch 90: Loss=151.4201\n",
      "Epoch 100: Loss=142.1495\n",
      "Epoch 110: Loss=141.7879\n",
      "Epoch 120: Loss=145.3693\n",
      "Epoch 130: Loss=146.2084\n",
      "Epoch 140: Loss=141.5205\n",
      "Epoch 150: Loss=142.5916\n",
      "Epoch 160: Loss=146.3756\n",
      "Epoch 170: Loss=141.6415\n",
      "Epoch 180: Loss=141.4782\n",
      "Epoch 190: Loss=140.8053\n",
      "Epoch 200: Loss=141.7898\n",
      "✅ MSE: 126.6421 | R²: 0.0018\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=141.1074\n",
      "Epoch 20: Loss=141.9643\n",
      "Epoch 30: Loss=142.6807\n",
      "Epoch 40: Loss=143.2429\n",
      "Epoch 50: Loss=141.4894\n",
      "Epoch 60: Loss=141.2996\n",
      "Epoch 70: Loss=159.7082\n",
      "Epoch 80: Loss=141.0751\n",
      "Epoch 90: Loss=141.0205\n",
      "Epoch 100: Loss=141.1285\n",
      "Epoch 110: Loss=141.1282\n",
      "Epoch 120: Loss=141.4290\n",
      "Epoch 130: Loss=141.2148\n",
      "Epoch 140: Loss=146.3817\n",
      "Epoch 150: Loss=141.1408\n",
      "Epoch 160: Loss=154.6132\n",
      "Epoch 170: Loss=142.4983\n",
      "Epoch 180: Loss=165.1616\n",
      "Epoch 190: Loss=141.6873\n",
      "Epoch 200: Loss=146.9055\n",
      "Epoch 210: Loss=140.7929\n",
      "Epoch 220: Loss=140.2080\n",
      "Epoch 230: Loss=140.5988\n",
      "Epoch 240: Loss=140.7413\n",
      "Epoch 250: Loss=165.2222\n",
      "Epoch 260: Loss=140.5600\n",
      "Epoch 270: Loss=140.4566\n",
      "Epoch 280: Loss=142.4606\n",
      "Epoch 290: Loss=140.5349\n",
      "Epoch 300: Loss=140.0560\n",
      "✅ MSE: 125.9551 | R²: 0.0072\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=147.0126\n",
      "Epoch 20: Loss=142.1373\n",
      "Epoch 30: Loss=141.6754\n",
      "Epoch 40: Loss=147.1319\n",
      "Epoch 50: Loss=143.2483\n",
      "Epoch 60: Loss=141.2908\n",
      "Epoch 70: Loss=141.3864\n",
      "Epoch 80: Loss=141.1013\n",
      "Epoch 90: Loss=141.5291\n",
      "Epoch 100: Loss=143.4591\n",
      "✅ MSE: 126.8791 | R²: -0.0001\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=143.3873\n",
      "Epoch 20: Loss=143.6080\n",
      "Epoch 30: Loss=146.0230\n",
      "Epoch 40: Loss=141.5382\n",
      "Epoch 50: Loss=142.6285\n",
      "Epoch 60: Loss=160.1546\n",
      "Epoch 70: Loss=142.6998\n",
      "Epoch 80: Loss=141.8083\n",
      "Epoch 90: Loss=141.7627\n",
      "Epoch 100: Loss=141.1105\n",
      "Epoch 110: Loss=140.9861\n",
      "Epoch 120: Loss=141.5386\n",
      "Epoch 130: Loss=146.1467\n",
      "Epoch 140: Loss=142.9267\n",
      "Epoch 150: Loss=141.1098\n",
      "Epoch 160: Loss=146.9283\n",
      "Epoch 170: Loss=141.5308\n",
      "Epoch 180: Loss=142.9306\n",
      "Epoch 190: Loss=146.6316\n",
      "Epoch 200: Loss=141.1761\n",
      "✅ MSE: 126.9105 | R²: -0.0003\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=142.1557\n",
      "Epoch 20: Loss=143.2963\n",
      "Epoch 30: Loss=141.6380\n",
      "Epoch 40: Loss=154.7838\n",
      "Epoch 50: Loss=142.1481\n",
      "Epoch 60: Loss=141.9528\n",
      "Epoch 70: Loss=141.8163\n",
      "Epoch 80: Loss=142.6130\n",
      "Epoch 90: Loss=143.2967\n",
      "Epoch 100: Loss=142.1569\n",
      "Epoch 110: Loss=141.6684\n",
      "Epoch 120: Loss=142.2027\n",
      "Epoch 130: Loss=160.0548\n",
      "Epoch 140: Loss=142.4180\n",
      "Epoch 150: Loss=141.4856\n",
      "Epoch 160: Loss=143.4919\n",
      "Epoch 170: Loss=141.6453\n",
      "Epoch 180: Loss=161.1156\n",
      "Epoch 190: Loss=141.3373\n",
      "Epoch 200: Loss=143.2089\n",
      "Epoch 210: Loss=159.2144\n",
      "Epoch 220: Loss=141.7381\n",
      "Epoch 230: Loss=142.0306\n",
      "Epoch 240: Loss=142.6055\n",
      "Epoch 250: Loss=141.5571\n",
      "Epoch 260: Loss=141.8616\n",
      "Epoch 270: Loss=142.0827\n",
      "Epoch 280: Loss=160.7600\n",
      "Epoch 290: Loss=143.3769\n",
      "Epoch 300: Loss=142.3719\n",
      "✅ MSE: 126.5774 | R²: 0.0023\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=142.2990\n",
      "Epoch 20: Loss=140.8863\n",
      "Epoch 30: Loss=155.1700\n",
      "Epoch 40: Loss=140.9728\n",
      "Epoch 50: Loss=140.8847\n",
      "Epoch 60: Loss=141.1623\n",
      "Epoch 70: Loss=141.1996\n",
      "Epoch 80: Loss=154.0310\n",
      "Epoch 90: Loss=141.7816\n",
      "Epoch 100: Loss=141.6960\n",
      "✅ MSE: 127.0413 | R²: -0.0014\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=141.6198\n",
      "Epoch 20: Loss=141.2775\n",
      "Epoch 30: Loss=141.0754\n",
      "Epoch 40: Loss=141.0704\n",
      "Epoch 50: Loss=164.3411\n",
      "Epoch 60: Loss=141.2406\n",
      "Epoch 70: Loss=141.4174\n",
      "Epoch 80: Loss=142.2560\n",
      "Epoch 90: Loss=141.2379\n",
      "Epoch 100: Loss=141.4733\n",
      "Epoch 110: Loss=141.2097\n",
      "Epoch 120: Loss=140.9639\n",
      "Epoch 130: Loss=159.2048\n",
      "Epoch 140: Loss=141.2966\n",
      "Epoch 150: Loss=143.3298\n",
      "Epoch 160: Loss=141.4326\n",
      "Epoch 170: Loss=140.8500\n",
      "Epoch 180: Loss=141.1889\n",
      "Epoch 190: Loss=142.5605\n",
      "Epoch 200: Loss=140.8681\n",
      "✅ MSE: 126.8769 | R²: -0.0001\n",
      "▶ Training: lr=1e-05, batch=32, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=142.9738\n",
      "Epoch 20: Loss=141.4824\n",
      "Epoch 30: Loss=142.7989\n",
      "Epoch 40: Loss=143.0111\n",
      "Epoch 50: Loss=145.9809\n",
      "Epoch 60: Loss=164.4919\n",
      "Epoch 70: Loss=141.0022\n",
      "Epoch 80: Loss=148.5361\n",
      "Epoch 90: Loss=141.9173\n",
      "Epoch 100: Loss=142.6547\n",
      "Epoch 110: Loss=141.5635\n",
      "Epoch 120: Loss=143.1465\n",
      "Epoch 130: Loss=159.9427\n",
      "Epoch 140: Loss=141.8535\n",
      "Epoch 150: Loss=159.4294\n",
      "Epoch 160: Loss=147.9278\n",
      "Epoch 170: Loss=141.4237\n",
      "Epoch 180: Loss=146.0041\n",
      "Epoch 190: Loss=141.0008\n",
      "Epoch 200: Loss=145.4649\n",
      "Epoch 210: Loss=159.5514\n",
      "Epoch 220: Loss=141.6162\n",
      "Epoch 230: Loss=140.6765\n",
      "Epoch 240: Loss=140.5181\n",
      "Epoch 250: Loss=158.9485\n",
      "Epoch 260: Loss=141.6473\n",
      "Epoch 270: Loss=140.8910\n",
      "Epoch 280: Loss=141.0158\n",
      "Epoch 290: Loss=147.0231\n",
      "Epoch 300: Loss=141.3037\n",
      "✅ MSE: 126.7187 | R²: 0.0012\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=143.2767\n",
      "Epoch 20: Loss=143.1726\n",
      "Epoch 30: Loss=143.2112\n",
      "Epoch 40: Loss=143.1824\n",
      "Epoch 50: Loss=143.1005\n",
      "Epoch 60: Loss=143.0637\n",
      "Epoch 70: Loss=143.6639\n",
      "Epoch 80: Loss=142.8880\n",
      "Epoch 90: Loss=143.0138\n",
      "Epoch 100: Loss=152.5075\n",
      "✅ MSE: 126.4782 | R²: 0.0031\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=143.5644\n",
      "Epoch 20: Loss=143.6545\n",
      "Epoch 30: Loss=146.9087\n",
      "Epoch 40: Loss=143.5043\n",
      "Epoch 50: Loss=143.3529\n",
      "Epoch 60: Loss=143.3219\n",
      "Epoch 70: Loss=143.1664\n",
      "Epoch 80: Loss=143.1713\n",
      "Epoch 90: Loss=143.1037\n",
      "Epoch 100: Loss=143.1995\n",
      "Epoch 110: Loss=152.7418\n",
      "Epoch 120: Loss=142.9919\n",
      "Epoch 130: Loss=143.2015\n",
      "Epoch 140: Loss=142.9697\n",
      "Epoch 150: Loss=143.0269\n",
      "Epoch 160: Loss=142.8968\n",
      "Epoch 170: Loss=142.8927\n",
      "Epoch 180: Loss=142.7883\n",
      "Epoch 190: Loss=142.7786\n",
      "Epoch 200: Loss=156.0689\n",
      "✅ MSE: 126.4857 | R²: 0.0030\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=144.0520\n",
      "Epoch 20: Loss=143.7043\n",
      "Epoch 30: Loss=143.2841\n",
      "Epoch 40: Loss=143.3423\n",
      "Epoch 50: Loss=143.1729\n",
      "Epoch 60: Loss=143.2132\n",
      "Epoch 70: Loss=143.7513\n",
      "Epoch 80: Loss=143.3623\n",
      "Epoch 90: Loss=143.0501\n",
      "Epoch 100: Loss=142.9725\n",
      "Epoch 110: Loss=143.0326\n",
      "Epoch 120: Loss=143.0519\n",
      "Epoch 130: Loss=142.8488\n",
      "Epoch 140: Loss=142.9108\n",
      "Epoch 150: Loss=142.9659\n",
      "Epoch 160: Loss=143.9594\n",
      "Epoch 170: Loss=142.7297\n",
      "Epoch 180: Loss=144.2978\n",
      "Epoch 190: Loss=142.7347\n",
      "Epoch 200: Loss=142.7609\n",
      "Epoch 210: Loss=146.2061\n",
      "Epoch 220: Loss=142.5371\n",
      "Epoch 230: Loss=142.6729\n",
      "Epoch 240: Loss=142.4379\n",
      "Epoch 250: Loss=142.4515\n",
      "Epoch 260: Loss=146.1127\n",
      "Epoch 270: Loss=142.4335\n",
      "Epoch 280: Loss=142.3057\n",
      "Epoch 290: Loss=142.2928\n",
      "Epoch 300: Loss=142.3116\n",
      "✅ MSE: 126.0523 | R²: 0.0064\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=143.7590\n",
      "Epoch 20: Loss=143.6925\n",
      "Epoch 30: Loss=143.7854\n",
      "Epoch 40: Loss=143.6387\n",
      "Epoch 50: Loss=143.7619\n",
      "Epoch 60: Loss=143.6035\n",
      "Epoch 70: Loss=143.5903\n",
      "Epoch 80: Loss=143.7067\n",
      "Epoch 90: Loss=143.6651\n",
      "Epoch 100: Loss=143.7116\n",
      "✅ MSE: 126.8256 | R²: 0.0003\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=143.5361\n",
      "Epoch 20: Loss=143.6312\n",
      "Epoch 30: Loss=143.5210\n",
      "Epoch 40: Loss=143.5299\n",
      "Epoch 50: Loss=143.9657\n",
      "Epoch 60: Loss=143.7675\n",
      "Epoch 70: Loss=143.6405\n",
      "Epoch 80: Loss=143.7931\n",
      "Epoch 90: Loss=144.0871\n",
      "Epoch 100: Loss=143.4941\n",
      "Epoch 110: Loss=143.3602\n",
      "Epoch 120: Loss=143.3918\n",
      "Epoch 130: Loss=143.5796\n",
      "Epoch 140: Loss=148.0995\n",
      "Epoch 150: Loss=143.6326\n",
      "Epoch 160: Loss=143.2355\n",
      "Epoch 170: Loss=143.3075\n",
      "Epoch 180: Loss=143.2042\n",
      "Epoch 190: Loss=143.4952\n",
      "Epoch 200: Loss=146.7146\n",
      "✅ MSE: 126.7370 | R²: 0.0010\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=144.4955\n",
      "Epoch 20: Loss=143.4754\n",
      "Epoch 30: Loss=143.4819\n",
      "Epoch 40: Loss=143.4864\n",
      "Epoch 50: Loss=143.3900\n",
      "Epoch 60: Loss=143.5067\n",
      "Epoch 70: Loss=143.3475\n",
      "Epoch 80: Loss=143.5420\n",
      "Epoch 90: Loss=143.9421\n",
      "Epoch 100: Loss=143.3319\n",
      "Epoch 110: Loss=143.8508\n",
      "Epoch 120: Loss=143.4165\n",
      "Epoch 130: Loss=143.3368\n",
      "Epoch 140: Loss=143.4710\n",
      "Epoch 150: Loss=143.2605\n",
      "Epoch 160: Loss=143.2638\n",
      "Epoch 170: Loss=144.5173\n",
      "Epoch 180: Loss=143.4421\n",
      "Epoch 190: Loss=143.3899\n",
      "Epoch 200: Loss=143.2726\n",
      "Epoch 210: Loss=146.8097\n",
      "Epoch 220: Loss=143.3061\n",
      "Epoch 230: Loss=156.6846\n",
      "Epoch 240: Loss=143.2731\n",
      "Epoch 250: Loss=143.1431\n",
      "Epoch 260: Loss=143.1761\n",
      "Epoch 270: Loss=143.0968\n",
      "Epoch 280: Loss=143.1718\n",
      "Epoch 290: Loss=143.1409\n",
      "Epoch 300: Loss=143.3448\n",
      "✅ MSE: 126.5330 | R²: 0.0026\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=144.2674\n",
      "Epoch 20: Loss=143.8987\n",
      "Epoch 30: Loss=147.1967\n",
      "Epoch 40: Loss=143.6826\n",
      "Epoch 50: Loss=144.1621\n",
      "Epoch 60: Loss=143.8827\n",
      "Epoch 70: Loss=143.7460\n",
      "Epoch 80: Loss=143.8141\n",
      "Epoch 90: Loss=143.6201\n",
      "Epoch 100: Loss=143.6503\n",
      "✅ MSE: 126.8918 | R²: -0.0002\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=143.7364\n",
      "Epoch 20: Loss=143.6633\n",
      "Epoch 30: Loss=143.6063\n",
      "Epoch 40: Loss=143.5891\n",
      "Epoch 50: Loss=153.3253\n",
      "Epoch 60: Loss=147.6919\n",
      "Epoch 70: Loss=143.6039\n",
      "Epoch 80: Loss=143.5142\n",
      "Epoch 90: Loss=143.6813\n",
      "Epoch 100: Loss=143.5829\n",
      "Epoch 110: Loss=143.6275\n",
      "Epoch 120: Loss=143.5208\n",
      "Epoch 130: Loss=147.0467\n",
      "Epoch 140: Loss=144.0288\n",
      "Epoch 150: Loss=143.6600\n",
      "Epoch 160: Loss=143.5215\n",
      "Epoch 170: Loss=143.4311\n",
      "Epoch 180: Loss=143.4761\n",
      "Epoch 190: Loss=143.5278\n",
      "Epoch 200: Loss=143.5444\n",
      "✅ MSE: 126.9083 | R²: -0.0003\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=143.6688\n",
      "Epoch 20: Loss=143.7166\n",
      "Epoch 30: Loss=156.9809\n",
      "Epoch 40: Loss=143.5431\n",
      "Epoch 50: Loss=143.5685\n",
      "Epoch 60: Loss=143.6685\n",
      "Epoch 70: Loss=143.5974\n",
      "Epoch 80: Loss=143.6467\n",
      "Epoch 90: Loss=143.9035\n",
      "Epoch 100: Loss=143.5700\n",
      "Epoch 110: Loss=144.0303\n",
      "Epoch 120: Loss=144.3337\n",
      "Epoch 130: Loss=144.6070\n",
      "Epoch 140: Loss=143.5854\n",
      "Epoch 150: Loss=143.4885\n",
      "Epoch 160: Loss=158.1673\n",
      "Epoch 170: Loss=143.5727\n",
      "Epoch 180: Loss=143.6011\n",
      "Epoch 190: Loss=143.5314\n",
      "Epoch 200: Loss=143.5428\n",
      "Epoch 210: Loss=143.4709\n",
      "Epoch 220: Loss=144.1692\n",
      "Epoch 230: Loss=143.7696\n",
      "Epoch 240: Loss=143.4464\n",
      "Epoch 250: Loss=144.4636\n",
      "Epoch 260: Loss=144.0360\n",
      "Epoch 270: Loss=143.5320\n",
      "Epoch 280: Loss=144.2210\n",
      "Epoch 290: Loss=143.5903\n",
      "Epoch 300: Loss=153.0722\n",
      "✅ MSE: 126.7522 | R²: 0.0009\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=143.8569\n",
      "Epoch 20: Loss=144.0389\n",
      "Epoch 30: Loss=144.1039\n",
      "Epoch 40: Loss=143.8958\n",
      "Epoch 50: Loss=143.8491\n",
      "Epoch 60: Loss=143.8274\n",
      "Epoch 70: Loss=143.7961\n",
      "Epoch 80: Loss=143.9413\n",
      "Epoch 90: Loss=143.8919\n",
      "Epoch 100: Loss=143.9745\n",
      "✅ MSE: 126.8319 | R²: 0.0003\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=147.2141\n",
      "Epoch 20: Loss=143.1006\n",
      "Epoch 30: Loss=143.1465\n",
      "Epoch 40: Loss=143.1547\n",
      "Epoch 50: Loss=143.2175\n",
      "Epoch 60: Loss=143.1037\n",
      "Epoch 70: Loss=143.2825\n",
      "Epoch 80: Loss=143.1314\n",
      "Epoch 90: Loss=143.1200\n",
      "Epoch 100: Loss=144.1719\n",
      "Epoch 110: Loss=143.1265\n",
      "Epoch 120: Loss=143.2423\n",
      "Epoch 130: Loss=143.2909\n",
      "Epoch 140: Loss=143.1744\n",
      "Epoch 150: Loss=143.1579\n",
      "Epoch 160: Loss=143.2884\n",
      "Epoch 170: Loss=144.1975\n",
      "Epoch 180: Loss=143.2055\n",
      "Epoch 190: Loss=143.1971\n",
      "Epoch 200: Loss=143.1833\n",
      "✅ MSE: 126.8250 | R²: 0.0003\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=143.6574\n",
      "Epoch 20: Loss=143.7020\n",
      "Epoch 30: Loss=143.7802\n",
      "Epoch 40: Loss=144.3550\n",
      "Epoch 50: Loss=143.6323\n",
      "Epoch 60: Loss=143.8725\n",
      "Epoch 70: Loss=143.7272\n",
      "Epoch 80: Loss=143.8199\n",
      "Epoch 90: Loss=143.6956\n",
      "Epoch 100: Loss=143.6242\n",
      "Epoch 110: Loss=143.6353\n",
      "Epoch 120: Loss=143.6822\n",
      "Epoch 130: Loss=143.8092\n",
      "Epoch 140: Loss=143.6408\n",
      "Epoch 150: Loss=143.9178\n",
      "Epoch 160: Loss=143.5968\n",
      "Epoch 170: Loss=143.7086\n",
      "Epoch 180: Loss=143.6120\n",
      "Epoch 190: Loss=144.0652\n",
      "Epoch 200: Loss=143.6775\n",
      "Epoch 210: Loss=143.6309\n",
      "Epoch 220: Loss=143.6027\n",
      "Epoch 230: Loss=143.5562\n",
      "Epoch 240: Loss=143.5883\n",
      "Epoch 250: Loss=143.9774\n",
      "Epoch 260: Loss=143.5464\n",
      "Epoch 270: Loss=143.5313\n",
      "Epoch 280: Loss=143.5239\n",
      "Epoch 290: Loss=143.5201\n",
      "Epoch 300: Loss=143.7073\n",
      "✅ MSE: 126.8454 | R²: 0.0002\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=143.6059\n",
      "Epoch 20: Loss=143.9191\n",
      "Epoch 30: Loss=143.5909\n",
      "Epoch 40: Loss=143.5815\n",
      "Epoch 50: Loss=143.7409\n",
      "Epoch 60: Loss=143.8390\n",
      "Epoch 70: Loss=143.9315\n",
      "Epoch 80: Loss=143.5834\n",
      "Epoch 90: Loss=143.5918\n",
      "Epoch 100: Loss=143.5639\n",
      "✅ MSE: 126.8713 | R²: -0.0000\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=143.4723\n",
      "Epoch 20: Loss=143.4662\n",
      "Epoch 30: Loss=143.4465\n",
      "Epoch 40: Loss=143.5180\n",
      "Epoch 50: Loss=143.3836\n",
      "Epoch 60: Loss=143.3903\n",
      "Epoch 70: Loss=143.5455\n",
      "Epoch 80: Loss=143.3148\n",
      "Epoch 90: Loss=143.3315\n",
      "Epoch 100: Loss=143.3627\n",
      "Epoch 110: Loss=143.3124\n",
      "Epoch 120: Loss=155.4944\n",
      "Epoch 130: Loss=143.4356\n",
      "Epoch 140: Loss=153.0106\n",
      "Epoch 150: Loss=143.6688\n",
      "Epoch 160: Loss=143.2926\n",
      "Epoch 170: Loss=143.4691\n",
      "Epoch 180: Loss=143.3339\n",
      "Epoch 190: Loss=143.3390\n",
      "Epoch 200: Loss=143.5955\n",
      "✅ MSE: 126.8672 | R²: -0.0000\n",
      "▶ Training: lr=1e-06, batch=8, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=144.0450\n",
      "Epoch 20: Loss=153.3697\n",
      "Epoch 30: Loss=143.5041\n",
      "Epoch 40: Loss=143.4157\n",
      "Epoch 50: Loss=143.7149\n",
      "Epoch 60: Loss=143.6776\n",
      "Epoch 70: Loss=143.3967\n",
      "Epoch 80: Loss=143.6061\n",
      "Epoch 90: Loss=143.4468\n",
      "Epoch 100: Loss=143.4979\n",
      "Epoch 110: Loss=143.5579\n",
      "Epoch 120: Loss=144.8907\n",
      "Epoch 130: Loss=143.4716\n",
      "Epoch 140: Loss=143.6110\n",
      "Epoch 150: Loss=143.7383\n",
      "Epoch 160: Loss=143.5299\n",
      "Epoch 170: Loss=147.6458\n",
      "Epoch 180: Loss=143.7348\n",
      "Epoch 190: Loss=143.7933\n",
      "Epoch 200: Loss=143.5835\n",
      "Epoch 210: Loss=143.4057\n",
      "Epoch 220: Loss=143.4635\n",
      "Epoch 230: Loss=143.4316\n",
      "Epoch 240: Loss=143.7345\n",
      "Epoch 250: Loss=143.3463\n",
      "Epoch 260: Loss=143.3303\n",
      "Epoch 270: Loss=143.5219\n",
      "Epoch 280: Loss=143.5342\n",
      "Epoch 290: Loss=156.6760\n",
      "Epoch 300: Loss=143.2711\n",
      "✅ MSE: 126.7603 | R²: 0.0008\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=142.2483\n",
      "Epoch 20: Loss=143.5051\n",
      "Epoch 30: Loss=141.1191\n",
      "Epoch 40: Loss=140.8797\n",
      "Epoch 50: Loss=141.3871\n",
      "Epoch 60: Loss=141.1911\n",
      "Epoch 70: Loss=141.5868\n",
      "Epoch 80: Loss=141.7360\n",
      "Epoch 90: Loss=146.6908\n",
      "Epoch 100: Loss=142.8781\n",
      "✅ MSE: 126.7849 | R²: 0.0006\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=143.4539\n",
      "Epoch 20: Loss=141.2723\n",
      "Epoch 30: Loss=160.5038\n",
      "Epoch 40: Loss=140.9199\n",
      "Epoch 50: Loss=148.3528\n",
      "Epoch 60: Loss=140.9417\n",
      "Epoch 70: Loss=140.7833\n",
      "Epoch 80: Loss=141.2582\n",
      "Epoch 90: Loss=141.4502\n",
      "Epoch 100: Loss=140.5692\n",
      "Epoch 110: Loss=145.7798\n",
      "Epoch 120: Loss=140.9965\n",
      "Epoch 130: Loss=140.4703\n",
      "Epoch 140: Loss=141.3882\n",
      "Epoch 150: Loss=141.3427\n",
      "Epoch 160: Loss=140.5164\n",
      "Epoch 170: Loss=140.4710\n",
      "Epoch 180: Loss=146.5088\n",
      "Epoch 190: Loss=140.4340\n",
      "Epoch 200: Loss=140.4112\n",
      "✅ MSE: 126.4578 | R²: 0.0032\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=141.9682\n",
      "Epoch 20: Loss=140.7731\n",
      "Epoch 30: Loss=144.6111\n",
      "Epoch 40: Loss=142.1862\n",
      "Epoch 50: Loss=142.6078\n",
      "Epoch 60: Loss=141.3707\n",
      "Epoch 70: Loss=142.8158\n",
      "Epoch 80: Loss=143.2139\n",
      "Epoch 90: Loss=140.9628\n",
      "Epoch 100: Loss=141.5343\n",
      "Epoch 110: Loss=145.5253\n",
      "Epoch 120: Loss=140.9501\n",
      "Epoch 130: Loss=142.9066\n",
      "Epoch 140: Loss=145.5654\n",
      "Epoch 150: Loss=142.4815\n",
      "Epoch 160: Loss=140.4361\n",
      "Epoch 170: Loss=141.8496\n",
      "Epoch 180: Loss=140.4962\n",
      "Epoch 190: Loss=140.8525\n",
      "Epoch 200: Loss=141.1250\n",
      "Epoch 210: Loss=140.3961\n",
      "Epoch 220: Loss=141.0297\n",
      "Epoch 230: Loss=140.6110\n",
      "Epoch 240: Loss=140.2507\n",
      "Epoch 250: Loss=188.3419\n",
      "Epoch 260: Loss=140.7829\n",
      "Epoch 270: Loss=142.9856\n",
      "Epoch 280: Loss=140.3012\n",
      "Epoch 290: Loss=140.4504\n",
      "Epoch 300: Loss=140.0472\n",
      "✅ MSE: 126.3254 | R²: 0.0043\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=141.6295\n",
      "Epoch 20: Loss=140.8599\n",
      "Epoch 30: Loss=141.7582\n",
      "Epoch 40: Loss=141.9969\n",
      "Epoch 50: Loss=142.2397\n",
      "Epoch 60: Loss=142.8667\n",
      "Epoch 70: Loss=140.9689\n",
      "Epoch 80: Loss=143.2541\n",
      "Epoch 90: Loss=148.1515\n",
      "Epoch 100: Loss=141.0806\n",
      "✅ MSE: 126.9795 | R²: -0.0009\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=140.8408\n",
      "Epoch 20: Loss=141.1211\n",
      "Epoch 30: Loss=141.1831\n",
      "Epoch 40: Loss=146.6691\n",
      "Epoch 50: Loss=141.8281\n",
      "Epoch 60: Loss=141.2564\n",
      "Epoch 70: Loss=140.7776\n",
      "Epoch 80: Loss=158.9397\n",
      "Epoch 90: Loss=160.8799\n",
      "Epoch 100: Loss=141.1419\n",
      "Epoch 110: Loss=141.6547\n",
      "Epoch 120: Loss=140.8530\n",
      "Epoch 130: Loss=141.6249\n",
      "Epoch 140: Loss=141.9159\n",
      "Epoch 150: Loss=141.6248\n",
      "Epoch 160: Loss=141.7874\n",
      "Epoch 170: Loss=141.2970\n",
      "Epoch 180: Loss=145.7534\n",
      "Epoch 190: Loss=145.2370\n",
      "Epoch 200: Loss=142.4768\n",
      "✅ MSE: 126.5819 | R²: 0.0022\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=140.9111\n",
      "Epoch 20: Loss=141.9262\n",
      "Epoch 30: Loss=141.8508\n",
      "Epoch 40: Loss=140.9858\n",
      "Epoch 50: Loss=160.8540\n",
      "Epoch 60: Loss=141.2843\n",
      "Epoch 70: Loss=141.5972\n",
      "Epoch 80: Loss=141.1150\n",
      "Epoch 90: Loss=141.1922\n",
      "Epoch 100: Loss=141.2916\n",
      "Epoch 110: Loss=142.0987\n",
      "Epoch 120: Loss=140.7647\n",
      "Epoch 130: Loss=142.8958\n",
      "Epoch 140: Loss=140.7274\n",
      "Epoch 150: Loss=140.8251\n",
      "Epoch 160: Loss=140.9951\n",
      "Epoch 170: Loss=158.1963\n",
      "Epoch 180: Loss=143.4517\n",
      "Epoch 190: Loss=140.7727\n",
      "Epoch 200: Loss=141.9356\n",
      "Epoch 210: Loss=140.9481\n",
      "Epoch 220: Loss=140.6372\n",
      "Epoch 230: Loss=148.0558\n",
      "Epoch 240: Loss=141.4922\n",
      "Epoch 250: Loss=141.1627\n",
      "Epoch 260: Loss=140.5861\n",
      "Epoch 270: Loss=141.4917\n",
      "Epoch 280: Loss=140.6302\n",
      "Epoch 290: Loss=160.9556\n",
      "Epoch 300: Loss=140.9522\n",
      "✅ MSE: 126.6335 | R²: 0.0018\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=141.3518\n",
      "Epoch 20: Loss=142.9117\n",
      "Epoch 30: Loss=141.6734\n",
      "Epoch 40: Loss=143.9837\n",
      "Epoch 50: Loss=141.4259\n",
      "Epoch 60: Loss=142.4133\n",
      "Epoch 70: Loss=142.9023\n",
      "Epoch 80: Loss=141.5308\n",
      "Epoch 90: Loss=143.7878\n",
      "Epoch 100: Loss=143.2531\n",
      "✅ MSE: 126.8437 | R²: 0.0002\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=141.4063\n",
      "Epoch 20: Loss=141.1733\n",
      "Epoch 30: Loss=140.8270\n",
      "Epoch 40: Loss=142.4516\n",
      "Epoch 50: Loss=189.0258\n",
      "Epoch 60: Loss=141.4163\n",
      "Epoch 70: Loss=208.4232\n",
      "Epoch 80: Loss=140.8838\n",
      "Epoch 90: Loss=141.8081\n",
      "Epoch 100: Loss=141.1796\n",
      "Epoch 110: Loss=141.2673\n",
      "Epoch 120: Loss=141.2445\n",
      "Epoch 130: Loss=144.7195\n",
      "Epoch 140: Loss=141.6168\n",
      "Epoch 150: Loss=141.5656\n",
      "Epoch 160: Loss=140.9541\n",
      "Epoch 170: Loss=141.1345\n",
      "Epoch 180: Loss=142.2075\n",
      "Epoch 190: Loss=140.8970\n",
      "Epoch 200: Loss=141.4741\n",
      "✅ MSE: 126.8873 | R²: -0.0002\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=159.5727\n",
      "Epoch 20: Loss=143.1677\n",
      "Epoch 30: Loss=141.4054\n",
      "Epoch 40: Loss=141.1711\n",
      "Epoch 50: Loss=141.7947\n",
      "Epoch 60: Loss=141.8805\n",
      "Epoch 70: Loss=141.7813\n",
      "Epoch 80: Loss=143.3087\n",
      "Epoch 90: Loss=141.7660\n",
      "Epoch 100: Loss=141.4877\n",
      "Epoch 110: Loss=141.6340\n",
      "Epoch 120: Loss=142.5795\n",
      "Epoch 130: Loss=146.3771\n",
      "Epoch 140: Loss=141.9549\n",
      "Epoch 150: Loss=141.6554\n",
      "Epoch 160: Loss=141.5096\n",
      "Epoch 170: Loss=141.6691\n",
      "Epoch 180: Loss=142.1315\n",
      "Epoch 190: Loss=142.2720\n",
      "Epoch 200: Loss=141.2938\n",
      "Epoch 210: Loss=141.3095\n",
      "Epoch 220: Loss=141.1515\n",
      "Epoch 230: Loss=142.9148\n",
      "Epoch 240: Loss=142.5756\n",
      "Epoch 250: Loss=141.1147\n",
      "Epoch 260: Loss=141.7773\n",
      "Epoch 270: Loss=141.4819\n",
      "Epoch 280: Loss=141.9104\n",
      "Epoch 290: Loss=141.2972\n",
      "Epoch 300: Loss=141.1600\n",
      "✅ MSE: 126.7985 | R²: 0.0005\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=141.2745\n",
      "Epoch 20: Loss=141.8217\n",
      "Epoch 30: Loss=140.8924\n",
      "Epoch 40: Loss=141.1394\n",
      "Epoch 50: Loss=141.4375\n",
      "Epoch 60: Loss=146.6551\n",
      "Epoch 70: Loss=140.7926\n",
      "Epoch 80: Loss=140.7816\n",
      "Epoch 90: Loss=141.5054\n",
      "Epoch 100: Loss=141.1461\n",
      "✅ MSE: 126.7963 | R²: 0.0006\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=141.4850\n",
      "Epoch 20: Loss=141.4774\n",
      "Epoch 30: Loss=142.7993\n",
      "Epoch 40: Loss=141.2235\n",
      "Epoch 50: Loss=141.1769\n",
      "Epoch 60: Loss=142.3602\n",
      "Epoch 70: Loss=145.9720\n",
      "Epoch 80: Loss=142.6272\n",
      "Epoch 90: Loss=141.4691\n",
      "Epoch 100: Loss=141.8032\n",
      "Epoch 110: Loss=141.7422\n",
      "Epoch 120: Loss=188.8114\n",
      "Epoch 130: Loss=141.7296\n",
      "Epoch 140: Loss=145.2764\n",
      "Epoch 150: Loss=158.4649\n",
      "Epoch 160: Loss=142.0454\n",
      "Epoch 170: Loss=140.9629\n",
      "Epoch 180: Loss=141.5094\n",
      "Epoch 190: Loss=141.8967\n",
      "Epoch 200: Loss=141.6804\n",
      "✅ MSE: 126.9593 | R²: -0.0007\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=142.4318\n",
      "Epoch 20: Loss=141.4868\n",
      "Epoch 30: Loss=141.4654\n",
      "Epoch 40: Loss=141.6400\n",
      "Epoch 50: Loss=141.5120\n",
      "Epoch 60: Loss=141.1868\n",
      "Epoch 70: Loss=141.3876\n",
      "Epoch 80: Loss=143.7430\n",
      "Epoch 90: Loss=141.9735\n",
      "Epoch 100: Loss=141.3213\n",
      "Epoch 110: Loss=143.4181\n",
      "Epoch 120: Loss=159.2046\n",
      "Epoch 130: Loss=141.1061\n",
      "Epoch 140: Loss=141.2542\n",
      "Epoch 150: Loss=141.6993\n",
      "Epoch 160: Loss=141.5006\n",
      "Epoch 170: Loss=142.0054\n",
      "Epoch 180: Loss=141.6517\n",
      "Epoch 190: Loss=143.4826\n",
      "Epoch 200: Loss=141.2088\n",
      "Epoch 210: Loss=141.1586\n",
      "Epoch 220: Loss=141.1916\n",
      "Epoch 230: Loss=141.5173\n",
      "Epoch 240: Loss=141.3702\n",
      "Epoch 250: Loss=141.9582\n",
      "Epoch 260: Loss=141.2328\n",
      "Epoch 270: Loss=141.4975\n",
      "Epoch 280: Loss=142.6515\n",
      "Epoch 290: Loss=141.8779\n",
      "Epoch 300: Loss=141.1609\n",
      "✅ MSE: 126.7588 | R²: 0.0009\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=141.1316\n",
      "Epoch 20: Loss=144.5637\n",
      "Epoch 30: Loss=141.8868\n",
      "Epoch 40: Loss=141.1990\n",
      "Epoch 50: Loss=141.5583\n",
      "Epoch 60: Loss=146.3314\n",
      "Epoch 70: Loss=141.9301\n",
      "Epoch 80: Loss=143.0693\n",
      "Epoch 90: Loss=141.0514\n",
      "Epoch 100: Loss=141.3846\n",
      "✅ MSE: 126.8293 | R²: 0.0003\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=141.8057\n",
      "Epoch 20: Loss=158.7882\n",
      "Epoch 30: Loss=141.7847\n",
      "Epoch 40: Loss=143.2516\n",
      "Epoch 50: Loss=143.7086\n",
      "Epoch 60: Loss=141.7513\n",
      "Epoch 70: Loss=141.5401\n",
      "Epoch 80: Loss=142.5397\n",
      "Epoch 90: Loss=142.4831\n",
      "Epoch 100: Loss=141.6080\n",
      "Epoch 110: Loss=141.6244\n",
      "Epoch 120: Loss=141.3215\n",
      "Epoch 130: Loss=141.1523\n",
      "Epoch 140: Loss=142.0832\n",
      "Epoch 150: Loss=143.4935\n",
      "Epoch 160: Loss=141.3478\n",
      "Epoch 170: Loss=141.3423\n",
      "Epoch 180: Loss=141.8821\n",
      "Epoch 190: Loss=141.2293\n",
      "Epoch 200: Loss=141.7157\n",
      "✅ MSE: 126.8161 | R²: 0.0004\n",
      "▶ Training: lr=1e-06, batch=16, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=141.2554\n",
      "Epoch 20: Loss=143.9663\n",
      "Epoch 30: Loss=141.5108\n",
      "Epoch 40: Loss=141.1769\n",
      "Epoch 50: Loss=141.6992\n",
      "Epoch 60: Loss=141.6557\n",
      "Epoch 70: Loss=140.8785\n",
      "Epoch 80: Loss=141.8521\n",
      "Epoch 90: Loss=142.3479\n",
      "Epoch 100: Loss=141.6190\n",
      "Epoch 110: Loss=141.4906\n",
      "Epoch 120: Loss=140.8865\n",
      "Epoch 130: Loss=141.3316\n",
      "Epoch 140: Loss=141.7577\n",
      "Epoch 150: Loss=141.4730\n",
      "Epoch 160: Loss=141.5293\n",
      "Epoch 170: Loss=141.2708\n",
      "Epoch 180: Loss=142.1580\n",
      "Epoch 190: Loss=143.4526\n",
      "Epoch 200: Loss=141.4155\n",
      "Epoch 210: Loss=142.2588\n",
      "Epoch 220: Loss=141.2504\n",
      "Epoch 230: Loss=141.2511\n",
      "Epoch 240: Loss=141.1326\n",
      "Epoch 250: Loss=141.4185\n",
      "Epoch 260: Loss=144.7285\n",
      "Epoch 270: Loss=142.3706\n",
      "Epoch 280: Loss=142.0233\n",
      "Epoch 290: Loss=142.5387\n",
      "Epoch 300: Loss=141.1807\n",
      "✅ MSE: 126.9508 | R²: -0.0007\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=146.6328\n",
      "Epoch 20: Loss=141.6559\n",
      "Epoch 30: Loss=141.7128\n",
      "Epoch 40: Loss=146.7800\n",
      "Epoch 50: Loss=141.5632\n",
      "Epoch 60: Loss=141.1331\n",
      "Epoch 70: Loss=154.1398\n",
      "Epoch 80: Loss=141.9876\n",
      "Epoch 90: Loss=141.2892\n",
      "Epoch 100: Loss=141.2202\n",
      "✅ MSE: 126.7399 | R²: 0.0010\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=141.5938\n",
      "Epoch 20: Loss=143.4808\n",
      "Epoch 30: Loss=146.1302\n",
      "Epoch 40: Loss=147.6635\n",
      "Epoch 50: Loss=159.3972\n",
      "Epoch 60: Loss=159.1154\n",
      "Epoch 70: Loss=141.4227\n",
      "Epoch 80: Loss=141.4827\n",
      "Epoch 90: Loss=141.7533\n",
      "Epoch 100: Loss=142.4186\n",
      "Epoch 110: Loss=159.7809\n",
      "Epoch 120: Loss=141.7321\n",
      "Epoch 130: Loss=142.6909\n",
      "Epoch 140: Loss=141.0380\n",
      "Epoch 150: Loss=155.3380\n",
      "Epoch 160: Loss=141.2812\n",
      "Epoch 170: Loss=154.4592\n",
      "Epoch 180: Loss=141.3544\n",
      "Epoch 190: Loss=161.0558\n",
      "Epoch 200: Loss=141.5639\n",
      "✅ MSE: 126.5340 | R²: 0.0026\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=142.0444\n",
      "Epoch 20: Loss=141.6201\n",
      "Epoch 30: Loss=143.4923\n",
      "Epoch 40: Loss=141.7645\n",
      "Epoch 50: Loss=142.1712\n",
      "Epoch 60: Loss=141.2850\n",
      "Epoch 70: Loss=143.3584\n",
      "Epoch 80: Loss=141.7876\n",
      "Epoch 90: Loss=141.8319\n",
      "Epoch 100: Loss=146.3366\n",
      "Epoch 110: Loss=141.3093\n",
      "Epoch 120: Loss=141.3778\n",
      "Epoch 130: Loss=141.3520\n",
      "Epoch 140: Loss=141.0194\n",
      "Epoch 150: Loss=141.2532\n",
      "Epoch 160: Loss=141.4851\n",
      "Epoch 170: Loss=141.2544\n",
      "Epoch 180: Loss=141.8261\n",
      "Epoch 190: Loss=142.4909\n",
      "Epoch 200: Loss=141.0513\n",
      "Epoch 210: Loss=140.7626\n",
      "Epoch 220: Loss=159.2112\n",
      "Epoch 230: Loss=141.6808\n",
      "Epoch 240: Loss=145.7259\n",
      "Epoch 250: Loss=148.3215\n",
      "Epoch 260: Loss=146.0142\n",
      "Epoch 270: Loss=141.0369\n",
      "Epoch 280: Loss=140.8016\n",
      "Epoch 290: Loss=141.0890\n",
      "Epoch 300: Loss=142.5794\n",
      "✅ MSE: 126.4185 | R²: 0.0035\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=143.0729\n",
      "Epoch 20: Loss=141.2500\n",
      "Epoch 30: Loss=142.6563\n",
      "Epoch 40: Loss=141.3777\n",
      "Epoch 50: Loss=141.9677\n",
      "Epoch 60: Loss=142.0124\n",
      "Epoch 70: Loss=142.6505\n",
      "Epoch 80: Loss=143.7597\n",
      "Epoch 90: Loss=154.3076\n",
      "Epoch 100: Loss=141.2738\n",
      "✅ MSE: 126.8235 | R²: 0.0003\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=142.8650\n",
      "Epoch 20: Loss=141.4613\n",
      "Epoch 30: Loss=155.2554\n",
      "Epoch 40: Loss=141.7745\n",
      "Epoch 50: Loss=142.5261\n",
      "Epoch 60: Loss=142.5964\n",
      "Epoch 70: Loss=142.6753\n",
      "Epoch 80: Loss=142.3792\n",
      "Epoch 90: Loss=141.9821\n",
      "Epoch 100: Loss=141.7404\n",
      "Epoch 110: Loss=141.8995\n",
      "Epoch 120: Loss=142.0285\n",
      "Epoch 130: Loss=142.3487\n",
      "Epoch 140: Loss=141.5469\n",
      "Epoch 150: Loss=141.7834\n",
      "Epoch 160: Loss=141.4281\n",
      "Epoch 170: Loss=141.6267\n",
      "Epoch 180: Loss=142.8557\n",
      "Epoch 190: Loss=141.6884\n",
      "Epoch 200: Loss=159.3041\n",
      "✅ MSE: 126.7249 | R²: 0.0011\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=141.5584\n",
      "Epoch 20: Loss=142.5917\n",
      "Epoch 30: Loss=141.7621\n",
      "Epoch 40: Loss=142.1162\n",
      "Epoch 50: Loss=159.2373\n",
      "Epoch 60: Loss=141.8786\n",
      "Epoch 70: Loss=141.9548\n",
      "Epoch 80: Loss=141.1356\n",
      "Epoch 90: Loss=147.3101\n",
      "Epoch 100: Loss=142.4962\n",
      "Epoch 110: Loss=142.1437\n",
      "Epoch 120: Loss=142.2963\n",
      "Epoch 130: Loss=141.6647\n",
      "Epoch 140: Loss=143.0643\n",
      "Epoch 150: Loss=146.5807\n",
      "Epoch 160: Loss=141.0629\n",
      "Epoch 170: Loss=141.9370\n",
      "Epoch 180: Loss=147.1670\n",
      "Epoch 190: Loss=142.4423\n",
      "Epoch 200: Loss=142.0631\n",
      "Epoch 210: Loss=141.5552\n",
      "Epoch 220: Loss=141.6160\n",
      "Epoch 230: Loss=141.9471\n",
      "Epoch 240: Loss=156.2925\n",
      "Epoch 250: Loss=159.9309\n",
      "Epoch 260: Loss=142.6353\n",
      "Epoch 270: Loss=141.1696\n",
      "Epoch 280: Loss=141.7205\n",
      "Epoch 290: Loss=141.7442\n",
      "Epoch 300: Loss=141.7954\n",
      "✅ MSE: 126.7233 | R²: 0.0011\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=147.0250\n",
      "Epoch 20: Loss=141.3815\n",
      "Epoch 30: Loss=142.4171\n",
      "Epoch 40: Loss=141.2681\n",
      "Epoch 50: Loss=141.8622\n",
      "Epoch 60: Loss=142.8618\n",
      "Epoch 70: Loss=142.7174\n",
      "Epoch 80: Loss=143.2161\n",
      "Epoch 90: Loss=141.9998\n",
      "Epoch 100: Loss=146.8939\n",
      "✅ MSE: 126.6909 | R²: 0.0014\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=143.1373\n",
      "Epoch 20: Loss=141.9938\n",
      "Epoch 30: Loss=143.1906\n",
      "Epoch 40: Loss=142.5654\n",
      "Epoch 50: Loss=141.5918\n",
      "Epoch 60: Loss=155.2501\n",
      "Epoch 70: Loss=142.3179\n",
      "Epoch 80: Loss=141.4952\n",
      "Epoch 90: Loss=142.1078\n",
      "Epoch 100: Loss=142.7276\n",
      "Epoch 110: Loss=141.5559\n",
      "Epoch 120: Loss=141.4656\n",
      "Epoch 130: Loss=142.0676\n",
      "Epoch 140: Loss=141.8167\n",
      "Epoch 150: Loss=141.4616\n",
      "Epoch 160: Loss=141.8943\n",
      "Epoch 170: Loss=142.5181\n",
      "Epoch 180: Loss=141.9339\n",
      "Epoch 190: Loss=141.1946\n",
      "Epoch 200: Loss=141.5877\n",
      "✅ MSE: 126.6339 | R²: 0.0018\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=146.0220\n",
      "Epoch 20: Loss=143.0578\n",
      "Epoch 30: Loss=143.0937\n",
      "Epoch 40: Loss=148.9879\n",
      "Epoch 50: Loss=143.8112\n",
      "Epoch 60: Loss=141.5381\n",
      "Epoch 70: Loss=142.0021\n",
      "Epoch 80: Loss=141.8705\n",
      "Epoch 90: Loss=159.8450\n",
      "Epoch 100: Loss=141.8727\n",
      "Epoch 110: Loss=143.0111\n",
      "Epoch 120: Loss=141.9083\n",
      "Epoch 130: Loss=141.9654\n",
      "Epoch 140: Loss=141.6310\n",
      "Epoch 150: Loss=144.1621\n",
      "Epoch 160: Loss=141.9089\n",
      "Epoch 170: Loss=141.4843\n",
      "Epoch 180: Loss=141.3597\n",
      "Epoch 190: Loss=141.6797\n",
      "Epoch 200: Loss=143.4638\n",
      "Epoch 210: Loss=141.6605\n",
      "Epoch 220: Loss=141.7440\n",
      "Epoch 230: Loss=142.1095\n",
      "Epoch 240: Loss=142.4794\n",
      "Epoch 250: Loss=141.3459\n",
      "Epoch 260: Loss=159.3302\n",
      "Epoch 270: Loss=142.4132\n",
      "Epoch 280: Loss=144.3318\n",
      "Epoch 290: Loss=141.8150\n",
      "Epoch 300: Loss=143.2250\n",
      "✅ MSE: 126.8765 | R²: -0.0001\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=141.7881\n",
      "Epoch 20: Loss=141.6794\n",
      "Epoch 30: Loss=141.6341\n",
      "Epoch 40: Loss=141.3194\n",
      "Epoch 50: Loss=154.1948\n",
      "Epoch 60: Loss=146.0966\n",
      "Epoch 70: Loss=143.4743\n",
      "Epoch 80: Loss=141.7334\n",
      "Epoch 90: Loss=141.5072\n",
      "Epoch 100: Loss=146.4285\n",
      "✅ MSE: 126.9056 | R²: -0.0003\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=154.8269\n",
      "Epoch 20: Loss=141.8104\n",
      "Epoch 30: Loss=142.0027\n",
      "Epoch 40: Loss=160.8261\n",
      "Epoch 50: Loss=142.9385\n",
      "Epoch 60: Loss=142.1650\n",
      "Epoch 70: Loss=141.9296\n",
      "Epoch 80: Loss=143.6839\n",
      "Epoch 90: Loss=142.5668\n",
      "Epoch 100: Loss=142.5965\n",
      "Epoch 110: Loss=142.5206\n",
      "Epoch 120: Loss=143.2946\n",
      "Epoch 130: Loss=146.1595\n",
      "Epoch 140: Loss=141.3923\n",
      "Epoch 150: Loss=142.2744\n",
      "Epoch 160: Loss=147.1219\n",
      "Epoch 170: Loss=142.6083\n",
      "Epoch 180: Loss=143.2020\n",
      "Epoch 190: Loss=142.8296\n",
      "Epoch 200: Loss=142.1590\n",
      "✅ MSE: 126.8603 | R²: 0.0001\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=142.6568\n",
      "Epoch 20: Loss=148.8342\n",
      "Epoch 30: Loss=142.1218\n",
      "Epoch 40: Loss=141.6407\n",
      "Epoch 50: Loss=141.1980\n",
      "Epoch 60: Loss=141.7016\n",
      "Epoch 70: Loss=141.1432\n",
      "Epoch 80: Loss=142.0229\n",
      "Epoch 90: Loss=141.6264\n",
      "Epoch 100: Loss=141.1533\n",
      "Epoch 110: Loss=146.6923\n",
      "Epoch 120: Loss=142.0865\n",
      "Epoch 130: Loss=141.4722\n",
      "Epoch 140: Loss=159.1693\n",
      "Epoch 150: Loss=141.9390\n",
      "Epoch 160: Loss=143.2766\n",
      "Epoch 170: Loss=141.6699\n",
      "Epoch 180: Loss=141.2349\n",
      "Epoch 190: Loss=142.4111\n",
      "Epoch 200: Loss=142.9432\n",
      "Epoch 210: Loss=141.4660\n",
      "Epoch 220: Loss=142.1514\n",
      "Epoch 230: Loss=141.7709\n",
      "Epoch 240: Loss=144.9569\n",
      "Epoch 250: Loss=141.4522\n",
      "Epoch 260: Loss=143.7297\n",
      "Epoch 270: Loss=141.6663\n",
      "Epoch 280: Loss=142.4747\n",
      "Epoch 290: Loss=154.3392\n",
      "Epoch 300: Loss=154.2108\n",
      "✅ MSE: 126.8954 | R²: -0.0002\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=142.3036\n",
      "Epoch 20: Loss=142.4858\n",
      "Epoch 30: Loss=142.4983\n",
      "Epoch 40: Loss=142.3842\n",
      "Epoch 50: Loss=142.4219\n",
      "Epoch 60: Loss=142.1840\n",
      "Epoch 70: Loss=141.9852\n",
      "Epoch 80: Loss=142.3993\n",
      "Epoch 90: Loss=142.5490\n",
      "Epoch 100: Loss=142.1664\n",
      "✅ MSE: 126.9278 | R²: -0.0005\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=142.0930\n",
      "Epoch 20: Loss=141.5025\n",
      "Epoch 30: Loss=142.2234\n",
      "Epoch 40: Loss=141.7647\n",
      "Epoch 50: Loss=141.5751\n",
      "Epoch 60: Loss=141.4395\n",
      "Epoch 70: Loss=142.2023\n",
      "Epoch 80: Loss=141.6426\n",
      "Epoch 90: Loss=141.9248\n",
      "Epoch 100: Loss=141.8420\n",
      "Epoch 110: Loss=142.1154\n",
      "Epoch 120: Loss=141.6094\n",
      "Epoch 130: Loss=141.2540\n",
      "Epoch 140: Loss=142.2128\n",
      "Epoch 150: Loss=142.6034\n",
      "Epoch 160: Loss=160.1170\n",
      "Epoch 170: Loss=141.4267\n",
      "Epoch 180: Loss=141.6479\n",
      "Epoch 190: Loss=143.0753\n",
      "Epoch 200: Loss=147.8992\n",
      "✅ MSE: 126.8208 | R²: 0.0004\n",
      "▶ Training: lr=1e-06, batch=32, hidden=(64, 32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=141.4467\n",
      "Epoch 20: Loss=159.8473\n",
      "Epoch 30: Loss=143.7482\n",
      "Epoch 40: Loss=142.1292\n",
      "Epoch 50: Loss=141.8385\n",
      "Epoch 60: Loss=141.6847\n",
      "Epoch 70: Loss=142.7575\n",
      "Epoch 80: Loss=142.7408\n",
      "Epoch 90: Loss=141.4520\n",
      "Epoch 100: Loss=141.5230\n",
      "Epoch 110: Loss=142.2301\n",
      "Epoch 120: Loss=159.5120\n",
      "Epoch 130: Loss=141.6581\n",
      "Epoch 140: Loss=141.7504\n",
      "Epoch 150: Loss=142.1181\n",
      "Epoch 160: Loss=142.6946\n",
      "Epoch 170: Loss=147.7064\n",
      "Epoch 180: Loss=160.7144\n",
      "Epoch 190: Loss=160.7301\n",
      "Epoch 200: Loss=141.9993\n",
      "Epoch 210: Loss=141.4358\n",
      "Epoch 220: Loss=159.1606\n",
      "Epoch 230: Loss=141.9454\n",
      "Epoch 240: Loss=141.4490\n",
      "Epoch 250: Loss=141.5773\n",
      "Epoch 260: Loss=141.3246\n",
      "Epoch 270: Loss=142.1665\n",
      "Epoch 280: Loss=141.3966\n",
      "Epoch 290: Loss=142.9703\n",
      "Epoch 300: Loss=142.9984\n",
      "✅ MSE: 126.8705 | R²: -0.0000\n",
      "🏆 Best config: {'lr': 0.001, 'batch_size': 16, 'hidden_sizes': (256, 128, 64), 'epochs': 100}, R²=0.6259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIG ---\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Reg_Analysis_FNN_PCA\"\n",
    "os.makedirs(f\"{output_root}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/plots\", exist_ok=True)\n",
    "\n",
    "# --- FNN Model ---\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"✅ MSE: {mse:.4f} | R²: {r2:.4f}\")\n",
    "    return mse, r2, y_true, y_pred\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_results(y_true, y_pred, train_losses=None):\n",
    "    residuals = y_true - y_pred\n",
    "    if train_losses:\n",
    "        plt.figure(); plt.plot(train_losses); plt.title(\"Training Loss\"); plt.tight_layout()\n",
    "        plt.savefig(f\"{output_root}/plots/loss_curve.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_pred, y=residuals); plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(\"Residuals vs Predicted\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_vs_predicted.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=y_pred); plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(\"Predicted vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/predicted_vs_actual.png\"); plt.close()\n",
    "    plt.figure(); sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Residuals\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_distribution.png\"); plt.close()\n",
    "    plt.figure(); plt.plot(np.sort(np.abs(residuals)), np.linspace(0, 1, len(residuals)))\n",
    "    plt.title(\"Cumulative Absolute Errors\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/cumulative_absolute_errors.png\"); plt.close()\n",
    "    plt.figure(); stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/qq_plot_residuals.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=np.abs(residuals))\n",
    "    plt.title(\"Absolute Error vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/absolute_error_vs_actual.png\"); plt.close()\n",
    "\n",
    "# --- Run FNN ---\n",
    "def run_fnn(X_train,  X_test, y_train,  y_test, input_size, hidden_sizes, lr, batch_size, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FNN(input_size=input_size, hidden_sizes=hidden_sizes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
    "    # X_val_tensor = torch.tensor(X_val).float().to(device)\n",
    "    # y_val_tensor = torch.tensor(y_val).float().to(device)\n",
    "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, X_test_tensor, y_test_tensor\n",
    "\n",
    "# --- Grid Search ---\n",
    "def grid_search_fnn(csv_path, param_grid):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    shape_features = ['Area_MA', 'Perimeter_MA', 'Extent_MA', 'Solidity_MA', 'Compactness_MA', 'Elongation_MA', 'Circularity_MA', 'Convexity_MA']\n",
    "    # spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "    target = 'X_Centroid_Velocity_MA'\n",
    "    df = df.dropna(subset=shape_features  + [target])\n",
    "\n",
    "    X_shape = df[shape_features].values\n",
    "    # X_spatial = df[spatial_features].values\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_shape, y, test_size=0.2, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=5)\n",
    "    X_train_final = pca.fit_transform(X_train_scaled)\n",
    "    X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "    results = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_losses = None\n",
    "    best_test_tensors = None\n",
    "\n",
    "    for lr, batch_size, hidden_sizes, epochs in product(param_grid[\"lr\"], param_grid[\"batch_size\"],\n",
    "                                                        param_grid[\"hidden_sizes\"], param_grid[\"epochs\"]):\n",
    "        print(f\"▶ Training: lr={lr}, batch={batch_size}, hidden={hidden_sizes}, epochs={epochs}\")\n",
    "        model, train_losses, X_test_tensor, y_test_tensor = run_fnn(\n",
    "            X_train_final,  X_test_final,\n",
    "            y_train,  y_test,\n",
    "            input_size=X_train_final.shape[1],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "        results.append({\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"hidden_sizes\": str(hidden_sizes),\n",
    "            \"epochs\": epochs, \"mse\": mse, \"r2\": r2\n",
    "        })\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_config = {\n",
    "                \"lr\": lr, \"batch_size\": batch_size,\n",
    "                \"hidden_sizes\": hidden_sizes, \"epochs\": epochs\n",
    "            }\n",
    "            best_losses = train_losses\n",
    "            best_test_tensors = (X_test_tensor, y_test_tensor, y_true, y_pred)\n",
    "\n",
    "    # Save best model\n",
    "    model_path = f\"{output_root}/models/fnn_best_model.pt\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(f\"{output_root}/results/grid_search_results.csv\", index=False)\n",
    "    print(f\"🏆 Best config: {best_config}, R²={best_r2:.4f}\")\n",
    "    return best_model, best_config, best_losses, best_test_tensors\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        \"lr\": [0.001,0.0001,0.00001,0.000001],\n",
    "        \"batch_size\": [8,16,32],\n",
    "        \"hidden_sizes\": [(256,128,64),(128, 64, 32), (64, 32, 16),(32,16,8),(64,32,16,8)],\n",
    "        \"epochs\": [100, 200,300]\n",
    "        }\n",
    "    # csv_path = \"/home/MinaHossain/EmbedTrack/Track_EMBD_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "    csv_path=\"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "    best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = grid_search_fnn(csv_path, param_grid)\n",
    "    plot_results(y_true, y_pred, train_losses=best_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIG ---\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Reg_Analysis_FNN\"\n",
    "os.makedirs(f\"{output_root}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/plots\", exist_ok=True)\n",
    "\n",
    "# --- FNN Model ---\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"✅ MSE: {mse:.4f} | R²: {r2:.4f}\")\n",
    "    return mse, r2, y_true, y_pred\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_results(y_true, y_pred, train_losses=None):\n",
    "    residuals = y_true - y_pred\n",
    "    if train_losses:\n",
    "        plt.figure(); plt.plot(train_losses); plt.title(\"Training Loss\"); plt.tight_layout()\n",
    "        plt.savefig(f\"{output_root}/plots/loss_curve.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_pred, y=residuals); plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(\"Residuals vs Predicted\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_vs_predicted.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=y_pred); plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(\"Predicted vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/predicted_vs_actual.png\"); plt.close()\n",
    "    plt.figure(); sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Residuals\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_distribution.png\"); plt.close()\n",
    "    plt.figure(); plt.plot(np.sort(np.abs(residuals)), np.linspace(0, 1, len(residuals)))\n",
    "    plt.title(\"Cumulative Absolute Errors\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/cumulative_absolute_errors.png\"); plt.close()\n",
    "    plt.figure(); stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/qq_plot_residuals.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=np.abs(residuals))\n",
    "    plt.title(\"Absolute Error vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/absolute_error_vs_actual.png\"); plt.close()\n",
    "\n",
    "# --- Run FNN ---\n",
    "def run_fnn(X_train,  X_test, y_train,  y_test, input_size, hidden_sizes, lr, batch_size, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FNN(input_size=input_size, hidden_sizes=hidden_sizes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
    "    # X_val_tensor = torch.tensor(X_val).float().to(device)\n",
    "    # y_val_tensor = torch.tensor(y_val).float().to(device)\n",
    "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, X_test_tensor, y_test_tensor\n",
    "\n",
    "# --- Grid Search ---\n",
    "def grid_search_fnn(csv_path, param_grid):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    shape_features = ['Area_MA', 'Perimeter_MA', 'Extent_MA', 'Solidity_MA', 'Compactness_MA', 'Elongation_MA', 'Circularity_MA', 'Convexity_MA']\n",
    "    # spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "    target = 'X_Centroid_Velocity_MA'\n",
    "    df = df.dropna(subset=shape_features  + [target])\n",
    "\n",
    "    X_shape = df[shape_features].values\n",
    "    # X_spatial = df[spatial_features].values\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_shape, y, test_size=0.2, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=5)\n",
    "    X_train_final = pca.fit_transform(X_train_scaled)\n",
    "    X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "    results = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_losses = None\n",
    "    best_test_tensors = None\n",
    "\n",
    "    for lr, batch_size, hidden_sizes, epochs in product(param_grid[\"lr\"], param_grid[\"batch_size\"],\n",
    "                                                        param_grid[\"hidden_sizes\"], param_grid[\"epochs\"]):\n",
    "        print(f\"▶ Training: lr={lr}, batch={batch_size}, hidden={hidden_sizes}, epochs={epochs}\")\n",
    "        model, train_losses, X_test_tensor, y_test_tensor = run_fnn(\n",
    "            X_train_final,  X_test_final,\n",
    "            y_train,  y_test,\n",
    "            input_size=X_train_final.shape[1],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "        results.append({\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"hidden_sizes\": str(hidden_sizes),\n",
    "            \"epochs\": epochs, \"mse\": mse, \"r2\": r2\n",
    "        })\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_config = {\n",
    "                \"lr\": lr, \"batch_size\": batch_size,\n",
    "                \"hidden_sizes\": hidden_sizes, \"epochs\": epochs\n",
    "            }\n",
    "            best_losses = train_losses\n",
    "            best_test_tensors = (X_test_tensor, y_test_tensor, y_true, y_pred)\n",
    "\n",
    "    # Save best model\n",
    "    model_path = f\"{output_root}/models/fnn_best_model.pt\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(f\"{output_root}/results/grid_search_results.csv\", index=False)\n",
    "    print(f\"🏆 Best config: {best_config}, R²={best_r2:.4f}\")\n",
    "    return best_model, best_config, best_losses, best_test_tensors\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        \"lr\": [0.001,0.0001,0.00001,0.000001],\n",
    "        \"batch_size\": [8,16,32],\n",
    "        \"hidden_sizes\": [(256,128,64),(128, 64, 32), (64, 32, 16),(32,16,8),(64,32,16,8)],\n",
    "        \"epochs\": [100, 200,300]\n",
    "        }\n",
    "    # csv_path = \"/home/MinaHossain/EmbedTrack/Track_EMBD_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "    csv_path=\"/home/MinaHossain/EmbedTrack/Track_New_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "    best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = grid_search_fnn(csv_path, param_grid)\n",
    "    plot_results(y_true, y_pred, train_losses=best_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shp_chc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
