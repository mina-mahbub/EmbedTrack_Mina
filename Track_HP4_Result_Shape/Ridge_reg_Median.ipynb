{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122fb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phate\n",
    "import scprep\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from scipy.stats import stats\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from skimage.morphology import dilation, erosion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from skimage import measure\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D Plotting\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from skimage.morphology import dilation, erosion\n",
    "from skimage import measure\n",
    "from scipy.ndimage import center_of_mass\n",
    "from glob import glob\n",
    "import random\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from tifffile import imread\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712b629",
   "metadata": {},
   "source": [
    "# Final Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfdec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: XGBoost\n",
      "        Split  R-squared        MSE\n",
      "0       Train   0.769936  39.093813\n",
      "1  Validation   0.692821  43.525304\n",
      "2        Test   0.723148  14.166064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2872437/2969134252.py:261: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median\"\n",
    "plot_dir = f\"{output_root}/Ridge_Plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "\n",
    "df_clean = df[collinear_features + spatial_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "X_spatial = df_clean[spatial_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA ===\n",
    "X_col_train, X_col_temp, X_spatial_train, X_spatial_temp, y_train, y_temp = train_test_split(\n",
    "    X_collinear, X_spatial, y, test_size=0.4, random_state=42)\n",
    "X_col_val, X_col_test, X_spatial_val, X_spatial_test, y_val, y_test = train_test_split(\n",
    "    X_col_temp, X_spatial_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_col_train_scaled = scaler.fit_transform(X_col_train)\n",
    "X_col_val_scaled = scaler.transform(X_col_val)\n",
    "X_col_test_scaled = scaler.transform(X_col_test)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca_train = pca.fit_transform(X_col_train_scaled)\n",
    "X_pca_val = pca.transform(X_col_val_scaled)\n",
    "X_pca_test = pca.transform(X_col_test_scaled)\n",
    "\n",
    "X_train_final = np.hstack([X_pca_train, X_spatial_train])\n",
    "X_val_final = np.hstack([X_pca_val, X_spatial_val])\n",
    "X_test_final = np.hstack([X_pca_test, X_spatial_test])\n",
    "\n",
    "# === DEFINE MODELS ===\n",
    "models = {\n",
    "    # \"LinearRegression\": LinearRegression(),\n",
    "    # \"Ridge\": RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5),\n",
    "    # \"Lasso\": LassoCV(cv=5, random_state=42, max_iter=10000),\n",
    "    # \"ElasticNet\": ElasticNetCV(cv=5, random_state=42, max_iter=10000),\n",
    "    # \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.01, random_state=42, n_jobs=-1, verbosity=0)\n",
    "}\n",
    "\n",
    "# === EVALUATE MODELS ON VALIDATION SET ===\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_val_pred = model.predict(X_val_final)\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        results[name] = r2\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# === SAVE COEFFICIENTS AND PERFORMANCE ===\n",
    "coeff_labels = [f'PC{i+1}' for i in range(3)] + spatial_features\n",
    "coef_df = pd.DataFrame({'Feature': coeff_labels, 'Coefficient': best_model.coef_}) if hasattr(best_model, 'coef_') else pd.DataFrame()\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['R-squared', 'MSE'],\n",
    "    'Value': [r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred)]\n",
    "})\n",
    "coef_df.to_csv(f\"{plot_dir}/best_model_coefficients.csv\", index=False)\n",
    "metrics_df.to_csv(f\"{plot_dir}/best_model_metrics.csv\", index=False)\n",
    "\n",
    "# === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === REPORT ON SPLITS ===\n",
    "y_train_pred = best_model.predict(X_train_final)\n",
    "y_val_pred = best_model.predict(X_val_final)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"R-squared\": [\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_val, y_val_pred),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ],\n",
    "    \"MSE\": [\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_val, y_val_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)\n",
    "    ]\n",
    "})\n",
    "\n",
    "performance.to_csv(f\"{plot_dir}/model_performance_by_split.csv\", index=False)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(performance)\n",
    "\n",
    "\n",
    "# Generate the code block for PCA + Clustering using validation data and applying to test data\n",
    "\n",
    "\n",
    "# === PCA + KMeans CLUSTERING (Fit on Validation, Apply to Test) ===\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "output_directory = plot_dir\n",
    "window_size = 5\n",
    "\n",
    "# --- Fit PCA on Validation Set ---\n",
    "X_scaled_val = X_col_val_scaled\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled_val)\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# --- Plot PCA Cumulative Variance (Elbow Plot) ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='s', linestyle='-', color='red', label=\"Cumulative Variance\")\n",
    "plt.axvline(x=optimal_components, color='green', linestyle='--', label=f\"Optimal Components ({optimal_components})\")\n",
    "plt.scatter(optimal_components, cumulative_variance[optimal_components-1], color='black', s=50, label=\"Chosen Point\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Elbow Plot with Cumulative Variance\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_Elbow_Plot_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Fit 2D PCA on Validation ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_val_2d = pca_2d.fit_transform(X_scaled_val)\n",
    "\n",
    "# --- KMeans on 2D PCA space (Validation) ---\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 15)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "    score = silhouette_score(pca_val_2d, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "optimal_score = max(silhouette_scores)\n",
    "\n",
    "# --- Plot Silhouette Scores ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='-', color='purple', label=\"Silhouette Score\")\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', label=f\"Optimal k = {optimal_k}\")\n",
    "plt.scatter(optimal_k, optimal_score, color='red', s=100, zorder=5)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs Number of Clusters (Validation)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Silhouette_Score_vs_K_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Apply to Test Data ---\n",
    "X_scaled_test = X_col_test_scaled\n",
    "pca_test_2d = pca_2d.transform(X_scaled_test)\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "test_clusters = kmeans_final.predict(pca_test_2d)\n",
    "\n",
    "# --- Create Test DataFrame with PCA + Clusters ---\n",
    "pca_test_df = pd.DataFrame(pca_test_2d, columns=[\"PCA1\", \"PCA2\"])\n",
    "pca_test_df[\"X_Centroid_Velocity_MA\"] = X_col_test[:, 0]  # Area_MA is first collinear feature\n",
    "pca_test_df[\"Cluster\"] = test_clusters\n",
    "\n",
    "# --- Clustered 2D PCA (Colored by Area) ---\n",
    "vmin = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 5)\n",
    "vmax = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 95)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sc = plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"X_Centroid_Velocity_MA\"], cmap=\"plasma\", alpha=0.8, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(sc, label=\"X_Centroid Velocity (MA)\")\n",
    "plt.title(\"2D PCA Test Data Colored by X_Centroid Velocity(MA)\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Cluster Plot with Centers ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"Cluster\"], cmap=\"tab10\", alpha=0.8)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, marker='o', label='Centers')\n",
    "for idx, (x, y) in enumerate(centers):\n",
    "    plt.text(x, y, str(idx), fontsize=12, fontweight='bold', ha='center', va='center', color='white',\n",
    "             bbox=dict(facecolor='black', boxstyle='circle,pad=0.2'))\n",
    "plt.title(f\"Test Data PCA with KMeans Clusters (k={optimal_k})\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_Clusters_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Boxplot: Area by Cluster (Test Data) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n",
    "sns.stripplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", color='gray', alpha=0.5, jitter=0.2, size=3)\n",
    "plt.title(\"X_Centroid Velocity (MA) by Cluster (Test Data)\", fontsize=14)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"X_Centroid Velocity (MA)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Boxplot_X_Centroid_Velocity_MA_By_Cluster_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Save clustered PCA test data and summary ---\n",
    "pca_test_df.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Results_Test_{window_size}.csv\"), index=False)\n",
    "\n",
    "summary = pca_test_df.groupby(\"Cluster\").agg({\n",
    "    \"PCA1\": [\"mean\", \"std\"],\n",
    "    \"PCA2\": [\"mean\", \"std\"],\n",
    "    \"X_Centroid_Velocity_MA\": [\"mean\", \"std\"]\n",
    "}).reset_index()\n",
    "summary.columns = [\"_\".join(col).strip(\"_\") for col in summary.columns.values]\n",
    "summary.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Summary_Test_{window_size}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae6cb0",
   "metadata": {},
   "source": [
    "# Create the updated FNN pipeline script content with PCA + spatial features + grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d207188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=180.2021\n",
      "Epoch 20: Loss=159.1746\n",
      "Epoch 30: Loss=27.5343\n",
      "Epoch 40: Loss=6.0811\n",
      "Epoch 50: Loss=3.4708\n",
      "Epoch 60: Loss=6.1826\n",
      "Epoch 70: Loss=2.0877\n",
      "Epoch 80: Loss=1.3206\n",
      "Epoch 90: Loss=0.9274\n",
      "Epoch 100: Loss=0.2339\n",
      "✅ MSE: 0.3915 | R²: 0.9923\n",
      "▶ Training: lr=0.001, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=158.8641\n",
      "Epoch 20: Loss=76.9462\n",
      "Epoch 30: Loss=5.3613\n",
      "Epoch 40: Loss=2.4533\n",
      "Epoch 50: Loss=0.5742\n",
      "Epoch 60: Loss=1.0539\n",
      "Epoch 70: Loss=7.4840\n",
      "Epoch 80: Loss=0.6867\n",
      "Epoch 90: Loss=0.6917\n",
      "Epoch 100: Loss=0.3495\n",
      "Epoch 110: Loss=1.3038\n",
      "Epoch 120: Loss=2.5555\n",
      "Epoch 130: Loss=0.7209\n",
      "Epoch 140: Loss=0.2123\n",
      "Epoch 150: Loss=1.8882\n",
      "Epoch 160: Loss=0.8968\n",
      "Epoch 170: Loss=3.7466\n",
      "Epoch 180: Loss=0.2369\n",
      "Epoch 190: Loss=4.9224\n",
      "Epoch 200: Loss=0.1604\n",
      "✅ MSE: 0.2267 | R²: 0.9956\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=151.2480\n",
      "Epoch 20: Loss=132.6863\n",
      "Epoch 30: Loss=24.2789\n",
      "Epoch 40: Loss=5.5879\n",
      "Epoch 50: Loss=2.6110\n",
      "Epoch 60: Loss=1.9914\n",
      "Epoch 70: Loss=1.2847\n",
      "Epoch 80: Loss=0.4464\n",
      "Epoch 90: Loss=0.5614\n",
      "Epoch 100: Loss=0.6169\n",
      "✅ MSE: 0.8068 | R²: 0.9842\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=154.7807\n",
      "Epoch 20: Loss=139.5617\n",
      "Epoch 30: Loss=60.4292\n",
      "Epoch 40: Loss=9.2979\n",
      "Epoch 50: Loss=6.7161\n",
      "Epoch 60: Loss=5.6531\n",
      "Epoch 70: Loss=2.6421\n",
      "Epoch 80: Loss=1.4596\n",
      "Epoch 90: Loss=0.8417\n",
      "Epoch 100: Loss=0.5206\n",
      "Epoch 110: Loss=0.6411\n",
      "Epoch 120: Loss=0.3558\n",
      "Epoch 130: Loss=0.4349\n",
      "Epoch 140: Loss=1.2076\n",
      "Epoch 150: Loss=0.4455\n",
      "Epoch 160: Loss=0.1659\n",
      "Epoch 170: Loss=0.3284\n",
      "Epoch 180: Loss=0.2379\n",
      "Epoch 190: Loss=0.4001\n",
      "Epoch 200: Loss=0.0923\n",
      "✅ MSE: 0.0772 | R²: 0.9985\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=166.0710\n",
      "Epoch 20: Loss=167.4858\n",
      "Epoch 30: Loss=166.9441\n",
      "Epoch 40: Loss=165.5761\n",
      "Epoch 50: Loss=165.7910\n",
      "Epoch 60: Loss=164.2878\n",
      "Epoch 70: Loss=164.5267\n",
      "Epoch 80: Loss=164.1878\n",
      "Epoch 90: Loss=163.7903\n",
      "Epoch 100: Loss=164.6986\n",
      "✅ MSE: 47.3159 | R²: 0.0753\n",
      "▶ Training: lr=0.001, batch=16, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=191.4641\n",
      "Epoch 20: Loss=168.3251\n",
      "Epoch 30: Loss=166.7139\n",
      "Epoch 40: Loss=166.7072\n",
      "Epoch 50: Loss=163.0237\n",
      "Epoch 60: Loss=163.6688\n",
      "Epoch 70: Loss=17.6066\n",
      "Epoch 80: Loss=4.0497\n",
      "Epoch 90: Loss=2.5227\n",
      "Epoch 100: Loss=1.3727\n",
      "Epoch 110: Loss=2.4183\n",
      "Epoch 120: Loss=0.7930\n",
      "Epoch 130: Loss=0.6293\n",
      "Epoch 140: Loss=0.4582\n",
      "Epoch 150: Loss=3.4150\n",
      "Epoch 160: Loss=0.3838\n",
      "Epoch 170: Loss=0.2224\n",
      "Epoch 180: Loss=0.6082\n",
      "Epoch 190: Loss=0.1900\n",
      "Epoch 200: Loss=0.1797\n",
      "✅ MSE: 0.3004 | R²: 0.9941\n",
      "▶ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=157.4817\n",
      "Epoch 20: Loss=139.9274\n",
      "Epoch 30: Loss=107.0870\n",
      "Epoch 40: Loss=64.7836\n",
      "Epoch 50: Loss=6.4531\n",
      "Epoch 60: Loss=3.1588\n",
      "Epoch 70: Loss=1.8375\n",
      "Epoch 80: Loss=1.2666\n",
      "Epoch 90: Loss=1.1430\n",
      "Epoch 100: Loss=0.9587\n",
      "✅ MSE: 1.3864 | R²: 0.9729\n",
      "▶ Training: lr=0.001, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=150.5995\n",
      "Epoch 20: Loss=167.2342\n",
      "Epoch 30: Loss=89.5843\n",
      "Epoch 40: Loss=29.6269\n",
      "Epoch 50: Loss=10.3971\n",
      "Epoch 60: Loss=7.4864\n",
      "Epoch 70: Loss=9.8615\n",
      "Epoch 80: Loss=2.9588\n",
      "Epoch 90: Loss=1.3688\n",
      "Epoch 100: Loss=1.1219\n",
      "Epoch 110: Loss=1.0462\n",
      "Epoch 120: Loss=1.3677\n",
      "Epoch 130: Loss=0.5300\n",
      "Epoch 140: Loss=1.3435\n",
      "Epoch 150: Loss=0.5568\n",
      "Epoch 160: Loss=0.4956\n",
      "Epoch 170: Loss=0.3285\n",
      "Epoch 180: Loss=0.2487\n",
      "Epoch 190: Loss=1.0465\n",
      "Epoch 200: Loss=0.8827\n",
      "✅ MSE: 0.5592 | R²: 0.9891\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=169.8884\n",
      "Epoch 20: Loss=149.4896\n",
      "Epoch 30: Loss=158.3779\n",
      "Epoch 40: Loss=121.9346\n",
      "Epoch 50: Loss=83.1851\n",
      "Epoch 60: Loss=10.3624\n",
      "Epoch 70: Loss=3.7052\n",
      "Epoch 80: Loss=3.0698\n",
      "Epoch 90: Loss=3.2562\n",
      "Epoch 100: Loss=2.9480\n",
      "✅ MSE: 3.0748 | R²: 0.9399\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=154.4063\n",
      "Epoch 20: Loss=132.7387\n",
      "Epoch 30: Loss=105.0685\n",
      "Epoch 40: Loss=61.0632\n",
      "Epoch 50: Loss=12.8177\n",
      "Epoch 60: Loss=2.9952\n",
      "Epoch 70: Loss=3.5442\n",
      "Epoch 80: Loss=1.1466\n",
      "Epoch 90: Loss=1.9318\n",
      "Epoch 100: Loss=1.1236\n",
      "Epoch 110: Loss=0.6916\n",
      "Epoch 120: Loss=1.0434\n",
      "Epoch 130: Loss=0.6299\n",
      "Epoch 140: Loss=1.0129\n",
      "Epoch 150: Loss=0.9820\n",
      "Epoch 160: Loss=1.9047\n",
      "Epoch 170: Loss=1.9963\n",
      "Epoch 180: Loss=0.5924\n",
      "Epoch 190: Loss=0.9788\n",
      "Epoch 200: Loss=1.1349\n",
      "✅ MSE: 1.1499 | R²: 0.9775\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=158.5140\n",
      "Epoch 20: Loss=154.6578\n",
      "Epoch 30: Loss=145.4863\n",
      "Epoch 40: Loss=131.6301\n",
      "Epoch 50: Loss=49.0673\n",
      "Epoch 60: Loss=4.8596\n",
      "Epoch 70: Loss=4.2141\n",
      "Epoch 80: Loss=1.2226\n",
      "Epoch 90: Loss=1.3245\n",
      "Epoch 100: Loss=0.9563\n",
      "✅ MSE: 2.2496 | R²: 0.9560\n",
      "▶ Training: lr=0.001, batch=32, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=155.8753\n",
      "Epoch 20: Loss=153.1594\n",
      "Epoch 30: Loss=94.8637\n",
      "Epoch 40: Loss=17.6520\n",
      "Epoch 50: Loss=7.1789\n",
      "Epoch 60: Loss=5.3504\n",
      "Epoch 70: Loss=3.4717\n",
      "Epoch 80: Loss=1.9051\n",
      "Epoch 90: Loss=1.2186\n",
      "Epoch 100: Loss=0.7492\n",
      "Epoch 110: Loss=1.1424\n",
      "Epoch 120: Loss=0.5574\n",
      "Epoch 130: Loss=0.9568\n",
      "Epoch 140: Loss=0.4860\n",
      "Epoch 150: Loss=0.3044\n",
      "Epoch 160: Loss=0.6141\n",
      "Epoch 170: Loss=0.5658\n",
      "Epoch 180: Loss=0.3091\n",
      "Epoch 190: Loss=0.7400\n",
      "Epoch 200: Loss=0.2540\n",
      "✅ MSE: 0.3674 | R²: 0.9928\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=158.6703\n",
      "Epoch 20: Loss=134.2523\n",
      "Epoch 30: Loss=99.3279\n",
      "Epoch 40: Loss=29.5542\n",
      "Epoch 50: Loss=7.0783\n",
      "Epoch 60: Loss=3.0076\n",
      "Epoch 70: Loss=1.4423\n",
      "Epoch 80: Loss=1.8238\n",
      "Epoch 90: Loss=0.8036\n",
      "Epoch 100: Loss=0.3926\n",
      "✅ MSE: 1.2675 | R²: 0.9752\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=167.7487\n",
      "Epoch 20: Loss=156.4182\n",
      "Epoch 30: Loss=132.2625\n",
      "Epoch 40: Loss=41.4850\n",
      "Epoch 50: Loss=12.5272\n",
      "Epoch 60: Loss=4.4718\n",
      "Epoch 70: Loss=3.3757\n",
      "Epoch 80: Loss=2.1248\n",
      "Epoch 90: Loss=1.2712\n",
      "Epoch 100: Loss=0.5846\n",
      "Epoch 110: Loss=0.3650\n",
      "Epoch 120: Loss=0.2596\n",
      "Epoch 130: Loss=0.3076\n",
      "Epoch 140: Loss=0.5543\n",
      "Epoch 150: Loss=0.8093\n",
      "Epoch 160: Loss=3.3448\n",
      "Epoch 170: Loss=0.3404\n",
      "Epoch 180: Loss=0.2302\n",
      "Epoch 190: Loss=0.8588\n",
      "Epoch 200: Loss=0.1006\n",
      "✅ MSE: 0.1968 | R²: 0.9962\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=156.4093\n",
      "Epoch 20: Loss=148.1643\n",
      "Epoch 30: Loss=132.0834\n",
      "Epoch 40: Loss=124.0523\n",
      "Epoch 50: Loss=48.9032\n",
      "Epoch 60: Loss=12.1632\n",
      "Epoch 70: Loss=5.2629\n",
      "Epoch 80: Loss=1.7247\n",
      "Epoch 90: Loss=1.1122\n",
      "Epoch 100: Loss=0.8966\n",
      "✅ MSE: 2.0905 | R²: 0.9591\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=169.2988\n",
      "Epoch 20: Loss=151.8202\n",
      "Epoch 30: Loss=143.1966\n",
      "Epoch 40: Loss=129.6178\n",
      "Epoch 50: Loss=90.8996\n",
      "Epoch 60: Loss=40.0465\n",
      "Epoch 70: Loss=6.1353\n",
      "Epoch 80: Loss=3.9959\n",
      "Epoch 90: Loss=2.4091\n",
      "Epoch 100: Loss=2.0063\n",
      "Epoch 110: Loss=2.6094\n",
      "Epoch 120: Loss=0.8112\n",
      "Epoch 130: Loss=0.7080\n",
      "Epoch 140: Loss=0.7360\n",
      "Epoch 150: Loss=0.7323\n",
      "Epoch 160: Loss=0.6840\n",
      "Epoch 170: Loss=0.2587\n",
      "Epoch 180: Loss=0.5577\n",
      "Epoch 190: Loss=0.7169\n",
      "Epoch 200: Loss=0.1815\n",
      "✅ MSE: 0.4200 | R²: 0.9918\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=165.4052\n",
      "Epoch 20: Loss=162.5876\n",
      "Epoch 30: Loss=158.6020\n",
      "Epoch 40: Loss=152.6106\n",
      "Epoch 50: Loss=147.1528\n",
      "Epoch 60: Loss=156.2378\n",
      "Epoch 70: Loss=80.4752\n",
      "Epoch 80: Loss=22.2534\n",
      "Epoch 90: Loss=4.2141\n",
      "Epoch 100: Loss=3.1709\n",
      "✅ MSE: 2.8745 | R²: 0.9438\n",
      "▶ Training: lr=0.0005, batch=16, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=162.6340\n",
      "Epoch 20: Loss=159.4744\n",
      "Epoch 30: Loss=144.5117\n",
      "Epoch 40: Loss=73.6735\n",
      "Epoch 50: Loss=15.5340\n",
      "Epoch 60: Loss=4.6526\n",
      "Epoch 70: Loss=3.3178\n",
      "Epoch 80: Loss=2.5903\n",
      "Epoch 90: Loss=1.2410\n",
      "Epoch 100: Loss=1.1609\n",
      "Epoch 110: Loss=9.9559\n",
      "Epoch 120: Loss=1.3392\n",
      "Epoch 130: Loss=1.9246\n",
      "Epoch 140: Loss=0.7976\n",
      "Epoch 150: Loss=1.7287\n",
      "Epoch 160: Loss=0.5907\n",
      "Epoch 170: Loss=0.6154\n",
      "Epoch 180: Loss=0.3974\n",
      "Epoch 190: Loss=1.2995\n",
      "Epoch 200: Loss=0.6629\n",
      "✅ MSE: 1.0231 | R²: 0.9800\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=150.0649\n",
      "Epoch 20: Loss=133.0536\n",
      "Epoch 30: Loss=123.5573\n",
      "Epoch 40: Loss=66.5496\n",
      "Epoch 50: Loss=48.5700\n",
      "Epoch 60: Loss=11.1604\n",
      "Epoch 70: Loss=5.3117\n",
      "Epoch 80: Loss=3.3015\n",
      "Epoch 90: Loss=3.1850\n",
      "Epoch 100: Loss=2.1533\n",
      "✅ MSE: 3.7440 | R²: 0.9268\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=194.1527\n",
      "Epoch 20: Loss=151.5292\n",
      "Epoch 30: Loss=132.9509\n",
      "Epoch 40: Loss=114.3261\n",
      "Epoch 50: Loss=83.5130\n",
      "Epoch 60: Loss=40.6297\n",
      "Epoch 70: Loss=13.3784\n",
      "Epoch 80: Loss=6.2954\n",
      "Epoch 90: Loss=5.2454\n",
      "Epoch 100: Loss=2.9255\n",
      "Epoch 110: Loss=2.4408\n",
      "Epoch 120: Loss=1.2634\n",
      "Epoch 130: Loss=1.0835\n",
      "Epoch 140: Loss=1.2591\n",
      "Epoch 150: Loss=0.5221\n",
      "Epoch 160: Loss=0.2569\n",
      "Epoch 170: Loss=0.4572\n",
      "Epoch 180: Loss=0.2532\n",
      "Epoch 190: Loss=0.1672\n",
      "Epoch 200: Loss=0.2180\n",
      "✅ MSE: 0.4316 | R²: 0.9916\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=160.6622\n",
      "Epoch 20: Loss=153.6092\n",
      "Epoch 30: Loss=147.4114\n",
      "Epoch 40: Loss=142.5615\n",
      "Epoch 50: Loss=144.8507\n",
      "Epoch 60: Loss=117.1307\n",
      "Epoch 70: Loss=114.1653\n",
      "Epoch 80: Loss=55.2069\n",
      "Epoch 90: Loss=15.2150\n",
      "Epoch 100: Loss=2.2358\n",
      "✅ MSE: 2.6547 | R²: 0.9481\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=156.7848\n",
      "Epoch 20: Loss=192.8143\n",
      "Epoch 30: Loss=143.2783\n",
      "Epoch 40: Loss=132.1655\n",
      "Epoch 50: Loss=114.4786\n",
      "Epoch 60: Loss=82.4550\n",
      "Epoch 70: Loss=56.9114\n",
      "Epoch 80: Loss=16.0094\n",
      "Epoch 90: Loss=3.4420\n",
      "Epoch 100: Loss=1.5734\n",
      "Epoch 110: Loss=0.8620\n",
      "Epoch 120: Loss=0.8913\n",
      "Epoch 130: Loss=1.0422\n",
      "Epoch 140: Loss=0.8622\n",
      "Epoch 150: Loss=0.8173\n",
      "Epoch 160: Loss=0.4478\n",
      "Epoch 170: Loss=0.7617\n",
      "Epoch 180: Loss=0.5877\n",
      "Epoch 190: Loss=0.3667\n",
      "Epoch 200: Loss=0.7109\n",
      "✅ MSE: 0.9816 | R²: 0.9808\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(64, 32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=160.5927\n",
      "Epoch 20: Loss=161.3164\n",
      "Epoch 30: Loss=311.6631\n",
      "Epoch 40: Loss=161.9165\n",
      "Epoch 50: Loss=159.5706\n",
      "Epoch 60: Loss=159.4635\n",
      "Epoch 70: Loss=159.6310\n",
      "Epoch 80: Loss=162.4523\n",
      "Epoch 90: Loss=159.4124\n",
      "Epoch 100: Loss=158.7915\n",
      "✅ MSE: 47.6452 | R²: 0.0689\n",
      "▶ Training: lr=0.0005, batch=32, hidden=(64, 32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=158.7296\n",
      "Epoch 20: Loss=155.8939\n",
      "Epoch 30: Loss=151.4450\n",
      "Epoch 40: Loss=205.5106\n",
      "Epoch 50: Loss=226.1824\n",
      "Epoch 60: Loss=114.4293\n",
      "Epoch 70: Loss=86.8352\n",
      "Epoch 80: Loss=50.5126\n",
      "Epoch 90: Loss=13.2573\n",
      "Epoch 100: Loss=2.4202\n",
      "Epoch 110: Loss=1.2187\n",
      "Epoch 120: Loss=0.9560\n",
      "Epoch 130: Loss=0.9329\n",
      "Epoch 140: Loss=1.4379\n",
      "Epoch 150: Loss=1.0395\n",
      "Epoch 160: Loss=0.6794\n",
      "Epoch 170: Loss=0.5316\n",
      "Epoch 180: Loss=1.0987\n",
      "Epoch 190: Loss=0.7642\n",
      "Epoch 200: Loss=0.4765\n",
      "✅ MSE: 2.0496 | R²: 0.9599\n",
      "🏆 Best config: {'lr': 0.001, 'batch_size': 16, 'hidden_sizes': (64, 32, 16), 'epochs': 200}, R²=0.9985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIG ---\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median/Reg_Analysis_FNN_PCA\"\n",
    "os.makedirs(f\"{output_root}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/plots\", exist_ok=True)\n",
    "\n",
    "# --- FNN Model ---\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"✅ MSE: {mse:.4f} | R²: {r2:.4f}\")\n",
    "    return mse, r2, y_true, y_pred\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_results(y_true, y_pred, train_losses=None):\n",
    "    residuals = y_true - y_pred\n",
    "    if train_losses:\n",
    "        plt.figure(); plt.plot(train_losses); plt.title(\"Training Loss\"); plt.tight_layout()\n",
    "        plt.savefig(f\"{output_root}/plots/loss_curve.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_pred, y=residuals); plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(\"Residuals vs Predicted\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_vs_predicted.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=y_pred); plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(\"Predicted vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/predicted_vs_actual.png\"); plt.close()\n",
    "    plt.figure(); sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Residuals\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_distribution.png\"); plt.close()\n",
    "    plt.figure(); plt.plot(np.sort(np.abs(residuals)), np.linspace(0, 1, len(residuals)))\n",
    "    plt.title(\"Cumulative Absolute Errors\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/cumulative_absolute_errors.png\"); plt.close()\n",
    "    plt.figure(); stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/qq_plot_residuals.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=np.abs(residuals))\n",
    "    plt.title(\"Absolute Error vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/absolute_error_vs_actual.png\"); plt.close()\n",
    "\n",
    "# --- Run FNN ---\n",
    "def run_fnn(X_train, X_val, X_test, y_train, y_val, y_test, input_size, hidden_sizes, lr, batch_size, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FNN(input_size=input_size, hidden_sizes=hidden_sizes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
    "    X_val_tensor = torch.tensor(X_val).float().to(device)\n",
    "    y_val_tensor = torch.tensor(y_val).float().to(device)\n",
    "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, X_test_tensor, y_test_tensor\n",
    "\n",
    "# --- Grid Search ---\n",
    "def grid_search_fnn(csv_path, param_grid):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    shape_features = ['Area_MA', 'Perimeter_MA', 'Extent_MA', 'Solidity_MA', 'Compactness_MA', 'Elongation_MA', 'Circularity_MA', 'Convexity_MA']\n",
    "    spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "    target = 'X_Centroid_Velocity_MA'\n",
    "    df = df.dropna(subset=shape_features + spatial_features + [target])\n",
    "\n",
    "    X_shape = df[shape_features].values\n",
    "    X_spatial = df[spatial_features].values\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    X_shape_scaled = StandardScaler().fit_transform(X_shape)\n",
    "    X_train_s, X_temp_s, X_train_spatial, X_temp_spatial, y_train, y_temp = train_test_split(\n",
    "        X_shape_scaled, X_spatial, y, test_size=0.4, random_state=42)\n",
    "    X_val_s, X_test_s, X_val_spatial, X_test_spatial, y_val, y_test = train_test_split(\n",
    "        X_temp_s, X_temp_spatial, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_train_pca = pca.fit_transform(X_train_s)\n",
    "    X_val_pca = pca.transform(X_val_s)\n",
    "    X_test_pca = pca.transform(X_test_s)\n",
    "\n",
    "    # Combine PCA + spatial\n",
    "    X_train_final = np.hstack([X_train_pca, X_train_spatial])\n",
    "    X_val_final = np.hstack([X_val_pca, X_val_spatial])\n",
    "    X_test_final = np.hstack([X_test_pca, X_test_spatial])\n",
    "\n",
    "    results = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_losses = None\n",
    "    best_test_tensors = None\n",
    "\n",
    "    for lr, batch_size, hidden_sizes, epochs in product(param_grid[\"lr\"], param_grid[\"batch_size\"],\n",
    "                                                        param_grid[\"hidden_sizes\"], param_grid[\"epochs\"]):\n",
    "        print(f\"▶ Training: lr={lr}, batch={batch_size}, hidden={hidden_sizes}, epochs={epochs}\")\n",
    "        model, train_losses, X_test_tensor, y_test_tensor = run_fnn(\n",
    "            X_train_final, X_val_final, X_test_final,\n",
    "            y_train, y_val, y_test,\n",
    "            input_size=X_train_final.shape[1],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "        results.append({\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"hidden_sizes\": str(hidden_sizes),\n",
    "            \"epochs\": epochs, \"mse\": mse, \"r2\": r2\n",
    "        })\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_config = {\n",
    "                \"lr\": lr, \"batch_size\": batch_size,\n",
    "                \"hidden_sizes\": hidden_sizes, \"epochs\": epochs\n",
    "            }\n",
    "            best_losses = train_losses\n",
    "            best_test_tensors = (X_test_tensor, y_test_tensor, y_true, y_pred)\n",
    "\n",
    "    # Save best model\n",
    "    model_path = f\"{output_root}/models/fnn_best_model.pt\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(f\"{output_root}/results/grid_search_results.csv\", index=False)\n",
    "    print(f\"🏆 Best config: {best_config}, R²={best_r2:.4f}\")\n",
    "    return best_model, best_config, best_losses, best_test_tensors\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        \"lr\": [0.001, 0.0005],\n",
    "        \"batch_size\": [16, 32],\n",
    "        \"hidden_sizes\": [(128, 64, 32), (64, 32, 16),(64,32,16,8)],\n",
    "        \"epochs\": [100, 200]\n",
    "    }\n",
    "    csv_path = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Median/Cells_Centroid_Velocity_TrueLabel_MA_Median_5.csv\"\n",
    "    best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = grid_search_fnn(csv_path, param_grid)\n",
    "    plot_results(y_true, y_pred, train_losses=best_losses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shp_chc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
