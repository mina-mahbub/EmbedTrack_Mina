{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122fb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phate\n",
    "import scprep\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from scipy.stats import stats\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "import csv\n",
    "import shutil\n",
    "from skimage.morphology import dilation, erosion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from skimage import measure\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 3D Plotting\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from skimage.morphology import dilation, erosion\n",
    "from skimage import measure\n",
    "from scipy.ndimage import center_of_mass\n",
    "from glob import glob\n",
    "import random\n",
    "from skimage.measure import regionprops, label\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from tifffile import imread\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712b629",
   "metadata": {},
   "source": [
    "# Final Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adfdec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: XGBoost\n",
      "        Split  R-squared        MSE\n",
      "0       Train   0.772814  39.112254\n",
      "1  Validation   0.837445  16.810190\n",
      "2        Test   0.813750  12.103973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2888493/1708242322.py:261: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_path = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Mean\"\n",
    "plot_dir = f\"{output_root}/Ridge_Plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# === LOAD AND CLEAN DATA ===\n",
    "df = pd.read_csv(input_path)\n",
    "collinear_features = ['Area_MA', 'Perimeter_MA', 'Solidity_MA', 'Extent_MA',\n",
    "                      'Circularity_MA', 'Convexity_MA', 'Elongation_MA', 'Compactness_MA']\n",
    "spatial_features = ['Centroid_X_MA', 'Centroid_Y_MA', 'X_Centroid_Distance_MA', 'Y_Centroid_Distance_MA']\n",
    "target_col = 'X_Centroid_Velocity_MA'\n",
    "\n",
    "df_clean = df[collinear_features + spatial_features + [target_col]].dropna()\n",
    "X_collinear = df_clean[collinear_features].values\n",
    "X_spatial = df_clean[spatial_features].values\n",
    "y = df_clean[target_col].values\n",
    "\n",
    "# === SPLIT DATA ===\n",
    "X_col_train, X_col_temp, X_spatial_train, X_spatial_temp, y_train, y_temp = train_test_split(\n",
    "    X_collinear, X_spatial, y, test_size=0.4, random_state=42)\n",
    "X_col_val, X_col_test, X_spatial_val, X_spatial_test, y_val, y_test = train_test_split(\n",
    "    X_col_temp, X_spatial_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# === PCA ON TRAIN ===\n",
    "scaler = StandardScaler()\n",
    "X_col_train_scaled = scaler.fit_transform(X_col_train)\n",
    "X_col_val_scaled = scaler.transform(X_col_val)\n",
    "X_col_test_scaled = scaler.transform(X_col_test)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca_train = pca.fit_transform(X_col_train_scaled)\n",
    "X_pca_val = pca.transform(X_col_val_scaled)\n",
    "X_pca_test = pca.transform(X_col_test_scaled)\n",
    "\n",
    "X_train_final = np.hstack([X_pca_train, X_spatial_train])\n",
    "X_val_final = np.hstack([X_pca_val, X_spatial_val])\n",
    "X_test_final = np.hstack([X_pca_test, X_spatial_test])\n",
    "\n",
    "# === DEFINE MODELS ===\n",
    "models = {\n",
    "    # \"LinearRegression\": LinearRegression(),\n",
    "    # \"Ridge\": RidgeCV(alphas=np.logspace(-3, 3, 50), cv=5),\n",
    "    # \"Lasso\": LassoCV(cv=5, random_state=42, max_iter=10000),\n",
    "    # \"ElasticNet\": ElasticNetCV(cv=5, random_state=42, max_iter=10000),\n",
    "    # \"RandomForest\": RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.01, random_state=42, n_jobs=-1, verbosity=0)\n",
    "}\n",
    "\n",
    "# === EVALUATE MODELS ON VALIDATION SET ===\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_val_pred = model.predict(X_val_final)\n",
    "        r2 = r2_score(y_val, y_val_pred)\n",
    "        results[name] = r2\n",
    "    except Exception as e:\n",
    "        results[name] = f\"Failed: {str(e)}\"\n",
    "\n",
    "best_model_name = max(results, key=lambda k: results[k] if isinstance(results[k], float) else -np.inf)\n",
    "best_model = models[best_model_name]\n",
    "best_model.fit(X_train_final, y_train)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# === SAVE COEFFICIENTS AND PERFORMANCE ===\n",
    "coeff_labels = [f'PC{i+1}' for i in range(3)] + spatial_features\n",
    "coef_df = pd.DataFrame({'Feature': coeff_labels, 'Coefficient': best_model.coef_}) if hasattr(best_model, 'coef_') else pd.DataFrame()\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['R-squared', 'MSE'],\n",
    "    'Value': [r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred)]\n",
    "})\n",
    "coef_df.to_csv(f\"{plot_dir}/best_model_coefficients.csv\", index=False)\n",
    "metrics_df.to_csv(f\"{plot_dir}/best_model_metrics.csv\", index=False)\n",
    "\n",
    "# === PLOTTING ===\n",
    "def plot_results(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    absolute_errors = np.abs(residuals)\n",
    "\n",
    "    def save_plot(fig, name):\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plot_dir}/{name}.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_pred, y=residuals, ax=ax)\n",
    "    ax.axhline(0, linestyle='--', color='red')\n",
    "    ax.set_title(\"Residuals vs Predicted\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_vs_predicted\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, ax=ax)\n",
    "    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], linestyle='--', color='red')\n",
    "    ax.set_title(\"Predicted vs Actual\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    save_plot(fig, \"predicted_vs_actual\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.histplot(residuals, bins=30, kde=True, ax=ax)\n",
    "    ax.set_title(\"Distribution of Residuals\")\n",
    "    ax.set_xlabel(\"Residuals\")\n",
    "    save_plot(fig, \"residuals_distribution\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sorted_errors = np.sort(absolute_errors)\n",
    "    cum_dist = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "    ax.plot(sorted_errors, cum_dist)\n",
    "    ax.set_title(\"Cumulative Distribution of Absolute Errors\")\n",
    "    ax.set_xlabel(\"Absolute Error\")\n",
    "    ax.set_ylabel(\"Cumulative Proportion\")\n",
    "    save_plot(fig, \"cumulative_absolute_errors\")\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot of Residuals\")\n",
    "    save_plot(fig, \"qq_plot_residuals\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(x=y_true, y=absolute_errors, ax=ax)\n",
    "    ax.set_title(\"Absolute Error vs Actual Value\")\n",
    "    ax.set_xlabel(\"Actual Values\")\n",
    "    ax.set_ylabel(\"Absolute Error\")\n",
    "    save_plot(fig, \"absolute_error_vs_actual\")\n",
    "\n",
    "plot_results(y_test, y_test_pred)\n",
    "\n",
    "# === REPORT ON SPLITS ===\n",
    "y_train_pred = best_model.predict(X_train_final)\n",
    "y_val_pred = best_model.predict(X_val_final)\n",
    "y_test_pred = best_model.predict(X_test_final)\n",
    "\n",
    "performance = pd.DataFrame({\n",
    "    \"Split\": [\"Train\", \"Validation\", \"Test\"],\n",
    "    \"R-squared\": [\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_val, y_val_pred),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ],\n",
    "    \"MSE\": [\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_val, y_val_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)\n",
    "    ]\n",
    "})\n",
    "\n",
    "performance.to_csv(f\"{plot_dir}/model_performance_by_split.csv\", index=False)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(performance)\n",
    "\n",
    "\n",
    "# Generate the code block for PCA + Clustering using validation data and applying to test data\n",
    "\n",
    "\n",
    "# === PCA + KMeans CLUSTERING (Fit on Validation, Apply to Test) ===\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "output_directory = plot_dir\n",
    "window_size = 5\n",
    "\n",
    "# --- Fit PCA on Validation Set ---\n",
    "X_scaled_val = X_col_val_scaled\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled_val)\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# --- Plot PCA Cumulative Variance (Elbow Plot) ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='s', linestyle='-', color='red', label=\"Cumulative Variance\")\n",
    "plt.axvline(x=optimal_components, color='green', linestyle='--', label=f\"Optimal Components ({optimal_components})\")\n",
    "plt.scatter(optimal_components, cumulative_variance[optimal_components-1], color='black', s=50, label=\"Chosen Point\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"PCA Elbow Plot with Cumulative Variance\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_Elbow_Plot_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Fit 2D PCA on Validation ---\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_val_2d = pca_2d.fit_transform(X_scaled_val)\n",
    "\n",
    "# --- KMeans on 2D PCA space (Validation) ---\n",
    "silhouette_scores = []\n",
    "k_range = range(2, 15)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "    score = silhouette_score(pca_val_2d, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "optimal_score = max(silhouette_scores)\n",
    "\n",
    "# --- Plot Silhouette Scores ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_range, silhouette_scores, marker='o', linestyle='-', color='purple', label=\"Silhouette Score\")\n",
    "plt.axvline(x=optimal_k, color='green', linestyle='--', label=f\"Optimal k = {optimal_k}\")\n",
    "plt.scatter(optimal_k, optimal_score, color='red', s=100, zorder=5)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score vs Number of Clusters (Validation)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Silhouette_Score_vs_K_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Apply to Test Data ---\n",
    "X_scaled_test = X_col_test_scaled\n",
    "pca_test_2d = pca_2d.transform(X_scaled_test)\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=0, n_init='auto').fit(pca_val_2d)\n",
    "test_clusters = kmeans_final.predict(pca_test_2d)\n",
    "\n",
    "# --- Create Test DataFrame with PCA + Clusters ---\n",
    "pca_test_df = pd.DataFrame(pca_test_2d, columns=[\"PCA1\", \"PCA2\"])\n",
    "pca_test_df[\"X_Centroid_Velocity_MA\"] = X_col_test[:, 0]  # Area_MA is first collinear feature\n",
    "pca_test_df[\"Cluster\"] = test_clusters\n",
    "\n",
    "# --- Clustered 2D PCA (Colored by Area) ---\n",
    "vmin = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 5)\n",
    "vmax = np.percentile(pca_test_df[\"X_Centroid_Velocity_MA\"], 95)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sc = plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"X_Centroid_Velocity_MA\"], cmap=\"plasma\", alpha=0.8, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(sc, label=\"X_Centroid Velocity (MA)\")\n",
    "plt.title(\"2D PCA Test Data Colored by X_Centroid Velocity(MA)\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Cluster Plot with Centers ---\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(pca_test_df[\"PCA1\"], pca_test_df[\"PCA2\"], c=pca_test_df[\"Cluster\"], cmap=\"tab10\", alpha=0.8)\n",
    "centers = kmeans_final.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, marker='o', label='Centers')\n",
    "for idx, (x, y) in enumerate(centers):\n",
    "    plt.text(x, y, str(idx), fontsize=12, fontweight='bold', ha='center', va='center', color='white',\n",
    "             bbox=dict(facecolor='black', boxstyle='circle,pad=0.2'))\n",
    "plt.title(f\"Test Data PCA with KMeans Clusters (k={optimal_k})\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"PCA_2D_Clusters_X_Centroid_Velocity_MA_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Boxplot: Area by Cluster (Test Data) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", palette=\"Set2\")\n",
    "sns.stripplot(data=pca_test_df, x=\"Cluster\", y=\"X_Centroid_Velocity_MA\", color='gray', alpha=0.5, jitter=0.2, size=3)\n",
    "plt.title(\"X_Centroid Velocity (MA) by Cluster (Test Data)\", fontsize=14)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"X_Centroid Velocity (MA)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_directory, f\"Boxplot_X_Centroid_Velocity_MA_By_Cluster_{window_size}.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- Save clustered PCA test data and summary ---\n",
    "pca_test_df.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Results_Test_{window_size}.csv\"), index=False)\n",
    "\n",
    "summary = pca_test_df.groupby(\"Cluster\").agg({\n",
    "    \"PCA1\": [\"mean\", \"std\"],\n",
    "    \"PCA2\": [\"mean\", \"std\"],\n",
    "    \"X_Centroid_Velocity_MA\": [\"mean\", \"std\"]\n",
    "}).reset_index()\n",
    "summary.columns = [\"_\".join(col).strip(\"_\") for col in summary.columns.values]\n",
    "summary.to_csv(os.path.join(output_directory, f\"PCA_Cluster_Summary_Test_{window_size}.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae6cb0",
   "metadata": {},
   "source": [
    "# Create the updated FNN pipeline script content with PCA + spatial features + grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d207188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Training: lr=0.001, batch=8, hidden=(256, 128, 64), epochs=100\n",
      "Epoch 10: Loss=102.5135\n",
      "Epoch 20: Loss=55.1129\n",
      "Epoch 30: Loss=47.3460\n",
      "Epoch 40: Loss=43.2630\n",
      "Epoch 50: Loss=43.8071\n",
      "Epoch 60: Loss=41.0464\n",
      "Epoch 70: Loss=37.8257\n",
      "Epoch 80: Loss=39.9355\n",
      "Epoch 90: Loss=35.4207\n",
      "Epoch 100: Loss=34.7596\n",
      "✅ MSE: 78.1590 | R²: 0.3875\n",
      "▶ Training: lr=0.001, batch=8, hidden=(256, 128, 64), epochs=200\n",
      "Epoch 10: Loss=87.8384\n",
      "Epoch 20: Loss=55.1688\n",
      "Epoch 30: Loss=46.7420\n",
      "Epoch 40: Loss=45.6505\n",
      "Epoch 50: Loss=41.0517\n",
      "Epoch 60: Loss=40.1480\n",
      "Epoch 70: Loss=37.7734\n",
      "Epoch 80: Loss=37.9541\n",
      "Epoch 90: Loss=34.0012\n",
      "Epoch 100: Loss=33.3370\n",
      "Epoch 110: Loss=30.9191\n",
      "Epoch 120: Loss=27.6809\n",
      "Epoch 130: Loss=26.4067\n",
      "Epoch 140: Loss=23.6007\n",
      "Epoch 150: Loss=21.7523\n",
      "Epoch 160: Loss=18.9267\n",
      "Epoch 170: Loss=20.7311\n",
      "Epoch 180: Loss=18.3697\n",
      "Epoch 190: Loss=15.8303\n",
      "Epoch 200: Loss=15.0589\n",
      "✅ MSE: 79.8135 | R²: 0.3746\n",
      "▶ Training: lr=0.001, batch=8, hidden=(256, 128, 64), epochs=300\n",
      "Epoch 10: Loss=85.9162\n",
      "Epoch 20: Loss=50.8449\n",
      "Epoch 30: Loss=47.1457\n",
      "Epoch 40: Loss=41.3173\n",
      "Epoch 50: Loss=39.9676\n",
      "Epoch 60: Loss=39.3570\n",
      "Epoch 70: Loss=37.7042\n",
      "Epoch 80: Loss=37.7483\n",
      "Epoch 90: Loss=30.8435\n",
      "Epoch 100: Loss=33.3285\n",
      "Epoch 110: Loss=27.2035\n",
      "Epoch 120: Loss=27.0245\n",
      "Epoch 130: Loss=24.1615\n",
      "Epoch 140: Loss=21.2797\n",
      "Epoch 150: Loss=20.2113\n",
      "Epoch 160: Loss=19.5814\n",
      "Epoch 170: Loss=20.1579\n",
      "Epoch 180: Loss=15.6622\n",
      "Epoch 190: Loss=14.3991\n",
      "Epoch 200: Loss=15.3047\n",
      "Epoch 210: Loss=13.4041\n",
      "Epoch 220: Loss=12.4553\n",
      "Epoch 230: Loss=14.1495\n",
      "Epoch 240: Loss=11.9980\n",
      "Epoch 250: Loss=10.1831\n",
      "Epoch 260: Loss=9.4862\n",
      "Epoch 270: Loss=9.7890\n",
      "Epoch 280: Loss=11.1950\n",
      "Epoch 290: Loss=8.8613\n",
      "Epoch 300: Loss=9.9630\n",
      "✅ MSE: 101.3501 | R²: 0.2058\n",
      "▶ Training: lr=0.001, batch=8, hidden=(128, 64, 32), epochs=100\n",
      "Epoch 10: Loss=105.9644\n",
      "Epoch 20: Loss=56.7158\n",
      "Epoch 30: Loss=49.2554\n",
      "Epoch 40: Loss=45.9553\n",
      "Epoch 50: Loss=45.1901\n",
      "Epoch 60: Loss=43.6445\n",
      "Epoch 70: Loss=43.7071\n",
      "Epoch 80: Loss=41.7773\n",
      "Epoch 90: Loss=40.2083\n",
      "Epoch 100: Loss=41.5747\n",
      "✅ MSE: 79.8766 | R²: 0.3741\n",
      "▶ Training: lr=0.001, batch=8, hidden=(128, 64, 32), epochs=200\n",
      "Epoch 10: Loss=94.8570\n",
      "Epoch 20: Loss=58.0211\n",
      "Epoch 30: Loss=48.9741\n",
      "Epoch 40: Loss=46.2615\n",
      "Epoch 50: Loss=46.0072\n",
      "Epoch 60: Loss=43.3882\n",
      "Epoch 70: Loss=41.0993\n",
      "Epoch 80: Loss=39.3463\n",
      "Epoch 90: Loss=38.9095\n",
      "Epoch 100: Loss=39.9240\n",
      "Epoch 110: Loss=37.2452\n",
      "Epoch 120: Loss=34.7420\n",
      "Epoch 130: Loss=33.9110\n",
      "Epoch 140: Loss=33.0564\n",
      "Epoch 150: Loss=33.2084\n",
      "Epoch 160: Loss=31.7252\n",
      "Epoch 170: Loss=30.5460\n",
      "Epoch 180: Loss=29.0559\n",
      "Epoch 190: Loss=28.7183\n",
      "Epoch 200: Loss=27.8849\n",
      "✅ MSE: 80.4560 | R²: 0.3695\n",
      "▶ Training: lr=0.001, batch=8, hidden=(128, 64, 32), epochs=300\n",
      "Epoch 10: Loss=102.9280\n",
      "Epoch 20: Loss=54.3533\n",
      "Epoch 30: Loss=48.1206\n",
      "Epoch 40: Loss=44.2254\n",
      "Epoch 50: Loss=43.1671\n",
      "Epoch 60: Loss=43.6666\n",
      "Epoch 70: Loss=40.7622\n",
      "Epoch 80: Loss=40.3804\n",
      "Epoch 90: Loss=38.2815\n",
      "Epoch 100: Loss=39.5608\n",
      "Epoch 110: Loss=38.4073\n",
      "Epoch 120: Loss=34.8126\n",
      "Epoch 130: Loss=34.2758\n",
      "Epoch 140: Loss=32.5346\n",
      "Epoch 150: Loss=30.2330\n",
      "Epoch 160: Loss=28.7872\n",
      "Epoch 170: Loss=30.1008\n",
      "Epoch 180: Loss=27.5444\n",
      "Epoch 190: Loss=27.5850\n",
      "Epoch 200: Loss=25.5767\n",
      "Epoch 210: Loss=25.4646\n",
      "Epoch 220: Loss=25.1978\n",
      "Epoch 230: Loss=25.1475\n",
      "Epoch 240: Loss=24.3948\n",
      "Epoch 250: Loss=22.6796\n",
      "Epoch 260: Loss=21.5865\n",
      "Epoch 270: Loss=21.6428\n",
      "Epoch 280: Loss=19.7522\n",
      "Epoch 290: Loss=19.5160\n",
      "Epoch 300: Loss=19.0288\n",
      "✅ MSE: 83.5717 | R²: 0.3451\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16), epochs=100\n",
      "Epoch 10: Loss=130.3325\n",
      "Epoch 20: Loss=91.9006\n",
      "Epoch 30: Loss=59.7629\n",
      "Epoch 40: Loss=55.1404\n",
      "Epoch 50: Loss=49.9921\n",
      "Epoch 60: Loss=47.2704\n",
      "Epoch 70: Loss=45.7846\n",
      "Epoch 80: Loss=46.6258\n",
      "Epoch 90: Loss=43.5791\n",
      "Epoch 100: Loss=40.7899\n",
      "✅ MSE: 83.5715 | R²: 0.3451\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16), epochs=200\n",
      "Epoch 10: Loss=127.5352\n",
      "Epoch 20: Loss=94.9000\n",
      "Epoch 30: Loss=61.5988\n",
      "Epoch 40: Loss=51.1191\n",
      "Epoch 50: Loss=47.4577\n",
      "Epoch 60: Loss=46.2543\n",
      "Epoch 70: Loss=44.7044\n",
      "Epoch 80: Loss=42.3205\n",
      "Epoch 90: Loss=42.3189\n",
      "Epoch 100: Loss=41.8793\n",
      "Epoch 110: Loss=40.2942\n",
      "Epoch 120: Loss=40.5587\n",
      "Epoch 130: Loss=39.4655\n",
      "Epoch 140: Loss=37.9581\n",
      "Epoch 150: Loss=37.4082\n",
      "Epoch 160: Loss=36.7770\n",
      "Epoch 170: Loss=35.7296\n",
      "Epoch 180: Loss=35.2166\n",
      "Epoch 190: Loss=34.2073\n",
      "Epoch 200: Loss=32.6735\n",
      "✅ MSE: 85.1297 | R²: 0.3329\n",
      "▶ Training: lr=0.001, batch=8, hidden=(64, 32, 16), epochs=300\n",
      "Epoch 10: Loss=128.2399\n",
      "Epoch 20: Loss=82.4222\n",
      "Epoch 30: Loss=57.1394\n",
      "Epoch 40: Loss=50.6608\n",
      "Epoch 50: Loss=46.9226\n",
      "Epoch 60: Loss=47.8349\n",
      "Epoch 70: Loss=44.7166\n",
      "Epoch 80: Loss=44.3076\n",
      "Epoch 90: Loss=43.3905\n",
      "Epoch 100: Loss=41.8505\n",
      "Epoch 110: Loss=42.2566\n",
      "Epoch 120: Loss=41.1428\n",
      "Epoch 130: Loss=39.8668\n",
      "Epoch 140: Loss=38.2977\n",
      "Epoch 150: Loss=38.2427\n",
      "Epoch 160: Loss=38.0672\n",
      "Epoch 170: Loss=36.8588\n",
      "Epoch 180: Loss=35.7627\n",
      "Epoch 190: Loss=35.4968\n",
      "Epoch 200: Loss=36.8290\n",
      "Epoch 210: Loss=35.0040\n",
      "Epoch 220: Loss=32.7981\n",
      "Epoch 230: Loss=31.3905\n",
      "Epoch 240: Loss=31.3774\n",
      "Epoch 250: Loss=30.3963\n",
      "Epoch 260: Loss=29.8271\n",
      "Epoch 270: Loss=29.2677\n",
      "Epoch 280: Loss=28.8174\n",
      "Epoch 290: Loss=28.1306\n",
      "Epoch 300: Loss=27.6123\n",
      "✅ MSE: 89.9972 | R²: 0.2948\n",
      "▶ Training: lr=0.001, batch=8, hidden=(32, 16, 8), epochs=100\n",
      "Epoch 10: Loss=136.6087\n",
      "Epoch 20: Loss=122.2849\n",
      "Epoch 30: Loss=106.6257\n",
      "Epoch 40: Loss=88.4336\n",
      "Epoch 50: Loss=67.3527\n",
      "Epoch 60: Loss=55.8360\n",
      "Epoch 70: Loss=51.6581\n",
      "Epoch 80: Loss=47.6094\n",
      "Epoch 90: Loss=47.7236\n",
      "Epoch 100: Loss=45.1200\n",
      "✅ MSE: 87.9254 | R²: 0.3110\n",
      "▶ Training: lr=0.001, batch=8, hidden=(32, 16, 8), epochs=200\n",
      "Epoch 10: Loss=138.5574\n",
      "Epoch 20: Loss=126.4164\n",
      "Epoch 30: Loss=100.7035\n",
      "Epoch 40: Loss=71.3775\n",
      "Epoch 50: Loss=56.8866\n",
      "Epoch 60: Loss=50.7842\n",
      "Epoch 70: Loss=47.5788\n",
      "Epoch 80: Loss=46.8523\n",
      "Epoch 90: Loss=45.0560\n",
      "Epoch 100: Loss=43.3568\n",
      "Epoch 110: Loss=42.6967\n",
      "Epoch 120: Loss=43.8689\n",
      "Epoch 130: Loss=41.2882\n",
      "Epoch 140: Loss=41.0693\n",
      "Epoch 150: Loss=40.1856\n",
      "Epoch 160: Loss=40.2648\n",
      "Epoch 170: Loss=41.2763\n",
      "Epoch 180: Loss=38.4591\n",
      "Epoch 190: Loss=38.3409\n",
      "Epoch 200: Loss=38.6233\n",
      "✅ MSE: 82.6447 | R²: 0.3524\n",
      "▶ Training: lr=0.001, batch=8, hidden=(32, 16, 8), epochs=300\n",
      "Epoch 10: Loss=135.7239\n",
      "Epoch 20: Loss=113.8417\n",
      "Epoch 30: Loss=79.1465\n",
      "Epoch 40: Loss=59.7377\n",
      "Epoch 50: Loss=53.4069\n",
      "Epoch 60: Loss=51.0122\n",
      "Epoch 70: Loss=48.8826\n",
      "Epoch 80: Loss=47.0672\n",
      "Epoch 90: Loss=46.3132\n",
      "Epoch 100: Loss=46.5203\n",
      "Epoch 110: Loss=45.8659\n",
      "Epoch 120: Loss=45.0857\n",
      "Epoch 130: Loss=44.1591\n",
      "Epoch 140: Loss=45.3952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 199\u001b[39m\n\u001b[32m    192\u001b[39m param_grid = {\n\u001b[32m    193\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m0.001\u001b[39m,\u001b[32m0.0001\u001b[39m, \u001b[32m0.00001\u001b[39m,\u001b[32m0.000001\u001b[39m],\n\u001b[32m    194\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m8\u001b[39m,\u001b[32m16\u001b[39m, \u001b[32m32\u001b[39m],\n\u001b[32m    195\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhidden_sizes\u001b[39m\u001b[33m\"\u001b[39m: [(\u001b[32m256\u001b[39m,\u001b[32m128\u001b[39m,\u001b[32m64\u001b[39m),(\u001b[32m128\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m32\u001b[39m), (\u001b[32m64\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m16\u001b[39m),(\u001b[32m32\u001b[39m,\u001b[32m16\u001b[39m,\u001b[32m8\u001b[39m),(\u001b[32m64\u001b[39m,\u001b[32m32\u001b[39m,\u001b[32m16\u001b[39m,\u001b[32m8\u001b[39m)],\n\u001b[32m    196\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m,\u001b[32m300\u001b[39m]\n\u001b[32m    197\u001b[39m }\n\u001b[32m    198\u001b[39m csv_path = \u001b[33m\"\u001b[39m\u001b[33m/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = \u001b[43mgrid_search_fnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m plot_results(y_true, y_pred, train_losses=best_losses)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mgrid_search_fnn\u001b[39m\u001b[34m(csv_path, param_grid)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lr, batch_size, hidden_sizes, epochs \u001b[38;5;129;01min\u001b[39;00m product(param_grid[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m], param_grid[\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    154\u001b[39m                                                     param_grid[\u001b[33m\"\u001b[39m\u001b[33mhidden_sizes\u001b[39m\u001b[33m\"\u001b[39m], param_grid[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m▶ Training: lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, batch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, hidden=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     model, train_losses, X_test_tensor, y_test_tensor = \u001b[43mrun_fnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_train_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mX_test_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n\u001b[32m    167\u001b[39m     results.append({\n\u001b[32m    168\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr, \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: batch_size, \u001b[33m\"\u001b[39m\u001b[33mhidden_sizes\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(hidden_sizes),\n\u001b[32m    169\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m: epochs, \u001b[33m\"\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m\"\u001b[39m: mse, \u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m: r2\n\u001b[32m    170\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mrun_fnn\u001b[39m\u001b[34m(X_train, X_test, y_train, y_test, input_size, hidden_sizes, lr, batch_size, epochs)\u001b[39m\n\u001b[32m     98\u001b[39m pred = model(xb)\n\u001b[32m     99\u001b[39m loss = criterion(pred, yb)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m optimizer.step()\n\u001b[32m    102\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/Shp_chc/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIG ---\n",
    "output_root = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Mean/Reg_Analysis_FNN_PCA\"\n",
    "os.makedirs(f\"{output_root}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{output_root}/plots\", exist_ok=True)\n",
    "\n",
    "# --- FNN Model ---\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.cpu().numpy().flatten()\n",
    "    y_true = y_test_tensor.cpu().numpy().flatten()\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"✅ MSE: {mse:.4f} | R²: {r2:.4f}\")\n",
    "    return mse, r2, y_true, y_pred\n",
    "\n",
    "# --- Plotting ---\n",
    "def plot_results(y_true, y_pred, train_losses=None):\n",
    "    residuals = y_true - y_pred\n",
    "    if train_losses:\n",
    "        plt.figure(); plt.plot(train_losses); plt.title(\"Training Loss\"); plt.tight_layout()\n",
    "        plt.savefig(f\"{output_root}/plots/loss_curve.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_pred, y=residuals); plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.title(\"Residuals vs Predicted\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_vs_predicted.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=y_pred); plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(\"Predicted vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/predicted_vs_actual.png\"); plt.close()\n",
    "    plt.figure(); sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribution of Residuals\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/residuals_distribution.png\"); plt.close()\n",
    "    plt.figure(); plt.plot(np.sort(np.abs(residuals)), np.linspace(0, 1, len(residuals)))\n",
    "    plt.title(\"Cumulative Absolute Errors\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/cumulative_absolute_errors.png\"); plt.close()\n",
    "    plt.figure(); stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"Q-Q Plot\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/qq_plot_residuals.png\"); plt.close()\n",
    "    plt.figure(); sns.scatterplot(x=y_true, y=np.abs(residuals))\n",
    "    plt.title(\"Absolute Error vs Actual\"); plt.tight_layout()\n",
    "    plt.savefig(f\"{output_root}/plots/absolute_error_vs_actual.png\"); plt.close()\n",
    "\n",
    "# --- Run FNN ---\n",
    "def run_fnn(X_train,  X_test, y_train, y_test, input_size, hidden_sizes, lr, batch_size, epochs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FNN(input_size=input_size, hidden_sizes=hidden_sizes).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train).float().to(device)\n",
    "    y_train_tensor = torch.tensor(y_train).float().to(device)\n",
    "   \n",
    "    X_test_tensor = torch.tensor(X_test).float().to(device)\n",
    "    y_test_tensor = torch.tensor(y_test).float().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f}\")\n",
    "\n",
    "    return model, train_losses, X_test_tensor, y_test_tensor\n",
    "\n",
    "# --- Grid Search ---\n",
    "def grid_search_fnn(csv_path, param_grid):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    shape_features = ['Area_MA', 'Perimeter_MA', 'Extent_MA', 'Solidity_MA', 'Compactness_MA', 'Elongation_MA', 'Circularity_MA', 'Convexity_MA']\n",
    "    \n",
    "    target = 'X_Centroid_Velocity_MA'\n",
    "    df = df.dropna(subset=shape_features + [target])\n",
    "\n",
    "    X_shape = df[shape_features].values\n",
    "    \n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_shape, y, test_size=0.2, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=5)\n",
    "    X_train_final = pca.fit_transform(X_train_scaled)\n",
    "    X_test_final = pca.transform(X_test_scaled)\n",
    "\n",
    "         \n",
    "\n",
    "    results = []\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_config = None\n",
    "    best_losses = None\n",
    "    best_test_tensors = None\n",
    "\n",
    "    for lr, batch_size, hidden_sizes, epochs in product(param_grid[\"lr\"], param_grid[\"batch_size\"],\n",
    "                                                        param_grid[\"hidden_sizes\"], param_grid[\"epochs\"]):\n",
    "        print(f\"▶ Training: lr={lr}, batch={batch_size}, hidden={hidden_sizes}, epochs={epochs}\")\n",
    "        model, train_losses, X_test_tensor, y_test_tensor = run_fnn(\n",
    "            X_train_final,  X_test_final,\n",
    "            y_train,  y_test,\n",
    "            input_size=X_train_final.shape[1],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        mse, r2, y_true, y_pred = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "        results.append({\n",
    "            \"lr\": lr, \"batch_size\": batch_size, \"hidden_sizes\": str(hidden_sizes),\n",
    "            \"epochs\": epochs, \"mse\": mse, \"r2\": r2\n",
    "        })\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_config = {\n",
    "                \"lr\": lr, \"batch_size\": batch_size,\n",
    "                \"hidden_sizes\": hidden_sizes, \"epochs\": epochs\n",
    "            }\n",
    "            best_losses = train_losses\n",
    "            best_test_tensors = (X_test_tensor, y_test_tensor, y_true, y_pred)\n",
    "\n",
    "    # Save best model\n",
    "    model_path = f\"{output_root}/models/fnn_best_model.pt\"\n",
    "    torch.save(best_model.state_dict(), model_path)\n",
    "\n",
    "    pd.DataFrame(results).to_csv(f\"{output_root}/results/grid_search_results.csv\", index=False)\n",
    "    print(f\"🏆 Best config: {best_config}, R²={best_r2:.4f}\")\n",
    "    return best_model, best_config, best_losses, best_test_tensors\n",
    "\n",
    "# --- Entry Point ---\n",
    "if __name__ == \"__main__\":\n",
    "    param_grid = {\n",
    "        \"lr\": [0.001,0.0001, 0.00001,0.000001],\n",
    "        \"batch_size\": [8,16, 32],\n",
    "        \"hidden_sizes\": [(256,128,64),(128, 64, 32), (64, 32, 16),(32,16,8),(64,32,16,8)],\n",
    "        \"epochs\": [100, 200,300]\n",
    "    }\n",
    "    csv_path = \"/home/MinaHossain/EmbedTrack/Track_HP4_Result_Shape/Mean/Cells_Centroid_Velocity_TrueLabel_MA_Mean_5.csv\"\n",
    "    best_model, best_config, best_losses, (X_test_tensor, y_test_tensor, y_true, y_pred) = grid_search_fnn(csv_path, param_grid)\n",
    "    plot_results(y_true, y_pred, train_losses=best_losses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Shp_chc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
